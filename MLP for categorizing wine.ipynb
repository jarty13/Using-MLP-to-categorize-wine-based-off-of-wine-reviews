{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferarty/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/jenniferarty/Desktop/projects/wine-reviews/final.py:197: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  data['Gen_Label'] = category_labels\n"
     ]
    }
   ],
   "source": [
    "from final import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "*The raw code for this Jupyter notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "*The raw code for this Jupyter notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['variety'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine= df[['description','category']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine.index= range(len(df_wine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine= df_wine.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description    0\n",
       "category       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take random sample of 10000\n",
    "wine = df_wine.sample(n=10000, random_state=0)\n",
    "wine.index =range(10000)\n",
    "category = wine[\"category\"]\n",
    "reviews = wine[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def filter_stop_words(train_sentences, stop_words):\n",
    "# #     update_review=[]\n",
    "# #     for i, sentence in enumerate(train_sentences):\n",
    "# #         new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "# #         train_sentences[i] = ' '.join(new_sent)\n",
    "# #         update_review.append(train_sentences)\n",
    "# #     return update_review\n",
    "\n",
    "# # stop =stopwords.words(\"english\")\n",
    "# # stop.append('A')\n",
    "# # stop.append('As')\n",
    "# # stop.append(\"It's\")\n",
    "# # stop.append(\"This\")\n",
    "# # stop.append(\"The\")\n",
    "# stop=set(stop)\n",
    "# # test2= filter_stop_words(test, stop_words)\n",
    "# reviews= wine[\"description\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert our text data from text to a vectorized matrix.\n",
    "one_hot_results= tokenizer.texts_to_matrix(reviews, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding of wine categories \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5, 15, ...,  3, 15,  3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform cateogry into a numeric vector\n",
    "category_cat = le.transform(category)\n",
    "category_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to one hote encoding wiht keras \n",
    "category_onehot =to_categorical(category_cat)\n",
    "category_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 19)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(category_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - test split. create training set and remove from test set \n",
    "#reviews\n",
    "test_index = random.sample(range(1,10000), 1500)\n",
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split categories and remove test set from train set \n",
    "label_test = category_onehot[test_index]\n",
    "label_train = np.delete(category_onehot,test_index,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label test (1500, 19)\n",
      "label train (8500, 19)\n",
      "test (1500, 2000)\n",
      "train (8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print('label test {}'.format(label_test.shape)) # Expected Output: (1500, 7)\n",
    "print('label train {}'.format(label_train.shape)) # Expected Output: (8500, 7)\n",
    "print('test {}'.format(test.shape)) # Expected Output: (1500, 2000)\n",
    "print('train {}'.format(train.shape)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running models and creating validation set \n",
    "random.seed(0)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2000)\n",
      "(1000, 19)\n"
     ]
    }
   ],
   "source": [
    "print(val.shape)\n",
    "print(label_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 1.7598 - acc: 0.4679 - val_loss: 1.2568 - val_acc: 0.5890\n",
      "Epoch 2/100\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.9562 - acc: 0.7004 - val_loss: 1.1142 - val_acc: 0.6510\n",
      "Epoch 3/100\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.6318 - acc: 0.8048 - val_loss: 1.1488 - val_acc: 0.6350\n",
      "Epoch 4/100\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.4204 - acc: 0.8765 - val_loss: 1.2550 - val_acc: 0.6370\n",
      "Epoch 5/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.2670 - acc: 0.9303 - val_loss: 1.4200 - val_acc: 0.6170\n",
      "Epoch 6/100\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.1603 - acc: 0.9665 - val_loss: 1.5561 - val_acc: 0.6060\n",
      "Epoch 7/100\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.0899 - acc: 0.9869 - val_loss: 1.7441 - val_acc: 0.6080\n",
      "Epoch 8/100\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.0483 - acc: 0.9960 - val_loss: 1.9017 - val_acc: 0.6110\n",
      "Epoch 9/100\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.0258 - acc: 0.9988 - val_loss: 2.0430 - val_acc: 0.6140\n",
      "Epoch 10/100\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.0150 - acc: 0.9999 - val_loss: 2.1432 - val_acc: 0.6130\n",
      "Epoch 11/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.0097 - acc: 0.9999 - val_loss: 2.2426 - val_acc: 0.6080\n",
      "Epoch 12/100\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 2.3279 - val_acc: 0.6170\n",
      "Epoch 13/100\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 2.4041 - val_acc: 0.6090\n",
      "Epoch 14/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.4793 - val_acc: 0.6090\n",
      "Epoch 15/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.5479 - val_acc: 0.6060\n",
      "Epoch 16/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 2.6011 - val_acc: 0.6100\n",
      "Epoch 17/100\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 2.6625 - val_acc: 0.6060\n",
      "Epoch 18/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 2.7138 - val_acc: 0.6030\n",
      "Epoch 19/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.7675 - val_acc: 0.6050\n",
      "Epoch 20/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 9.9997e-04 - acc: 1.0000 - val_loss: 2.8117 - val_acc: 0.6030\n",
      "Epoch 21/100\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 8.2883e-04 - acc: 1.0000 - val_loss: 2.8598 - val_acc: 0.6010\n",
      "Epoch 22/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 6.9312e-04 - acc: 1.0000 - val_loss: 2.9068 - val_acc: 0.6020\n",
      "Epoch 23/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 5.8180e-04 - acc: 1.0000 - val_loss: 2.9478 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 4.9038e-04 - acc: 1.0000 - val_loss: 2.9904 - val_acc: 0.6020\n",
      "Epoch 25/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 4.1444e-04 - acc: 1.0000 - val_loss: 3.0301 - val_acc: 0.5980\n",
      "Epoch 26/100\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 3.5180e-04 - acc: 1.0000 - val_loss: 3.0734 - val_acc: 0.5980\n",
      "Epoch 27/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 2.9882e-04 - acc: 1.0000 - val_loss: 3.1120 - val_acc: 0.5980\n",
      "Epoch 28/100\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 2.5597e-04 - acc: 1.0000 - val_loss: 3.1471 - val_acc: 0.5980\n",
      "Epoch 29/100\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 2.1827e-04 - acc: 1.0000 - val_loss: 3.1822 - val_acc: 0.5990\n",
      "Epoch 30/100\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.8695e-04 - acc: 1.0000 - val_loss: 3.2207 - val_acc: 0.5960\n",
      "Epoch 31/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.6018e-04 - acc: 1.0000 - val_loss: 3.2582 - val_acc: 0.5970\n",
      "Epoch 32/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.3794e-04 - acc: 1.0000 - val_loss: 3.2923 - val_acc: 0.5960\n",
      "Epoch 33/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.1868e-04 - acc: 1.0000 - val_loss: 3.3287 - val_acc: 0.5960\n",
      "Epoch 34/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.0249e-04 - acc: 1.0000 - val_loss: 3.3605 - val_acc: 0.5970\n",
      "Epoch 35/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 8.8468e-05 - acc: 1.0000 - val_loss: 3.3902 - val_acc: 0.5960\n",
      "Epoch 36/100\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 7.6377e-05 - acc: 1.0000 - val_loss: 3.4235 - val_acc: 0.5960\n",
      "Epoch 37/100\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 6.5994e-05 - acc: 1.0000 - val_loss: 3.4514 - val_acc: 0.5960\n",
      "Epoch 38/100\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 5.7255e-05 - acc: 1.0000 - val_loss: 3.4831 - val_acc: 0.5960\n",
      "Epoch 39/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 4.9658e-05 - acc: 1.0000 - val_loss: 3.5121 - val_acc: 0.5950\n",
      "Epoch 40/100\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 4.3050e-05 - acc: 1.0000 - val_loss: 3.5450 - val_acc: 0.5950\n",
      "Epoch 41/100\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 3.7367e-05 - acc: 1.0000 - val_loss: 3.5736 - val_acc: 0.5950\n",
      "Epoch 42/100\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 3.2439e-05 - acc: 1.0000 - val_loss: 3.6030 - val_acc: 0.5940\n",
      "Epoch 43/100\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 2.8216e-05 - acc: 1.0000 - val_loss: 3.6316 - val_acc: 0.5940\n",
      "Epoch 44/100\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 2.4504e-05 - acc: 1.0000 - val_loss: 3.6573 - val_acc: 0.5950\n",
      "Epoch 45/100\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 2.1374e-05 - acc: 1.0000 - val_loss: 3.6852 - val_acc: 0.5900\n",
      "Epoch 46/100\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 1.8606e-05 - acc: 1.0000 - val_loss: 3.7119 - val_acc: 0.5900\n",
      "Epoch 47/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.6169e-05 - acc: 1.0000 - val_loss: 3.7368 - val_acc: 0.5920\n",
      "Epoch 48/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.4105e-05 - acc: 1.0000 - val_loss: 3.7604 - val_acc: 0.5920\n",
      "Epoch 49/100\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.2305e-05 - acc: 1.0000 - val_loss: 3.7846 - val_acc: 0.5920\n",
      "Epoch 50/100\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 1.0740e-05 - acc: 1.0000 - val_loss: 3.8097 - val_acc: 0.5900\n",
      "Epoch 51/100\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 9.3672e-06 - acc: 1.0000 - val_loss: 3.8356 - val_acc: 0.5910\n",
      "Epoch 52/100\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 8.1756e-06 - acc: 1.0000 - val_loss: 3.8606 - val_acc: 0.5900\n",
      "Epoch 53/100\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 7.1496e-06 - acc: 1.0000 - val_loss: 3.8796 - val_acc: 0.5900\n",
      "Epoch 54/100\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 6.2518e-06 - acc: 1.0000 - val_loss: 3.9042 - val_acc: 0.5890\n",
      "Epoch 55/100\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 5.4734e-06 - acc: 1.0000 - val_loss: 3.9272 - val_acc: 0.5910\n",
      "Epoch 56/100\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 4.7875e-06 - acc: 1.0000 - val_loss: 3.9484 - val_acc: 0.5880\n",
      "Epoch 57/100\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 4.1896e-06 - acc: 1.0000 - val_loss: 3.9683 - val_acc: 0.5890\n",
      "Epoch 58/100\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 3.6748e-06 - acc: 1.0000 - val_loss: 3.9895 - val_acc: 0.5890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 3.2147e-06 - acc: 1.0000 - val_loss: 4.0115 - val_acc: 0.5880\n",
      "Epoch 60/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 2.8201e-06 - acc: 1.0000 - val_loss: 4.0300 - val_acc: 0.5880\n",
      "Epoch 61/100\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 2.4719e-06 - acc: 1.0000 - val_loss: 4.0541 - val_acc: 0.5900\n",
      "Epoch 62/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 2.1739e-06 - acc: 1.0000 - val_loss: 4.0743 - val_acc: 0.5890\n",
      "Epoch 63/100\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 1.9087e-06 - acc: 1.0000 - val_loss: 4.0920 - val_acc: 0.5890\n",
      "Epoch 64/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.6815e-06 - acc: 1.0000 - val_loss: 4.1085 - val_acc: 0.5890\n",
      "Epoch 65/100\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 1.4810e-06 - acc: 1.0000 - val_loss: 4.1298 - val_acc: 0.5900\n",
      "Epoch 66/100\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.3045e-06 - acc: 1.0000 - val_loss: 4.1480 - val_acc: 0.5900\n",
      "Epoch 67/100\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.1509e-06 - acc: 1.0000 - val_loss: 4.1650 - val_acc: 0.5900\n",
      "Epoch 68/100\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.0155e-06 - acc: 1.0000 - val_loss: 4.1820 - val_acc: 0.5890\n",
      "Epoch 69/100\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 8.9756e-07 - acc: 1.0000 - val_loss: 4.1972 - val_acc: 0.5870\n",
      "Epoch 70/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 7.9477e-07 - acc: 1.0000 - val_loss: 4.2162 - val_acc: 0.5900\n",
      "Epoch 71/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 7.0485e-07 - acc: 1.0000 - val_loss: 4.2330 - val_acc: 0.5890\n",
      "Epoch 72/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 6.2702e-07 - acc: 1.0000 - val_loss: 4.2464 - val_acc: 0.5870\n",
      "Epoch 73/100\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 5.5737e-07 - acc: 1.0000 - val_loss: 4.2625 - val_acc: 0.5880\n",
      "Epoch 74/100\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 4.9761e-07 - acc: 1.0000 - val_loss: 4.2771 - val_acc: 0.5880\n",
      "Epoch 75/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 4.4521e-07 - acc: 1.0000 - val_loss: 4.2902 - val_acc: 0.5880\n",
      "Epoch 76/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 3.9949e-07 - acc: 1.0000 - val_loss: 4.3051 - val_acc: 0.5880\n",
      "Epoch 77/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 3.5889e-07 - acc: 1.0000 - val_loss: 4.3198 - val_acc: 0.5870\n",
      "Epoch 78/100\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 3.2467e-07 - acc: 1.0000 - val_loss: 4.3337 - val_acc: 0.5870\n",
      "Epoch 79/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 2.9384e-07 - acc: 1.0000 - val_loss: 4.3466 - val_acc: 0.5880\n",
      "Epoch 80/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 2.6772e-07 - acc: 1.0000 - val_loss: 4.3645 - val_acc: 0.5880\n",
      "Epoch 81/100\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 2.4521e-07 - acc: 1.0000 - val_loss: 4.3767 - val_acc: 0.5870\n",
      "Epoch 82/100\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 2.2542e-07 - acc: 1.0000 - val_loss: 4.3875 - val_acc: 0.5880\n",
      "Epoch 83/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 2.0878e-07 - acc: 1.0000 - val_loss: 4.3962 - val_acc: 0.5870\n",
      "Epoch 84/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.9381e-07 - acc: 1.0000 - val_loss: 4.4124 - val_acc: 0.5890\n",
      "Epoch 85/100\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.8161e-07 - acc: 1.0000 - val_loss: 4.4223 - val_acc: 0.5880\n",
      "Epoch 86/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.7101e-07 - acc: 1.0000 - val_loss: 4.4385 - val_acc: 0.5870\n",
      "Epoch 87/100\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 1.6259e-07 - acc: 1.0000 - val_loss: 4.4461 - val_acc: 0.5860\n",
      "Epoch 88/100\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.5481e-07 - acc: 1.0000 - val_loss: 4.4573 - val_acc: 0.5860\n",
      "Epoch 89/100\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.4847e-07 - acc: 1.0000 - val_loss: 4.4680 - val_acc: 0.5890\n",
      "Epoch 90/100\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.4321e-07 - acc: 1.0000 - val_loss: 4.4757 - val_acc: 0.5850\n",
      "Epoch 91/100\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.3886e-07 - acc: 1.0000 - val_loss: 4.4848 - val_acc: 0.5850\n",
      "Epoch 92/100\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.3521e-07 - acc: 1.0000 - val_loss: 4.4956 - val_acc: 0.5870\n",
      "Epoch 93/100\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.3218e-07 - acc: 1.0000 - val_loss: 4.5009 - val_acc: 0.5860\n",
      "Epoch 94/100\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.2947e-07 - acc: 1.0000 - val_loss: 4.5074 - val_acc: 0.5860\n",
      "Epoch 95/100\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.2770e-07 - acc: 1.0000 - val_loss: 4.5170 - val_acc: 0.5880\n",
      "Epoch 96/100\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.2607e-07 - acc: 1.0000 - val_loss: 4.5228 - val_acc: 0.5850\n",
      "Epoch 97/100\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.2465e-07 - acc: 1.0000 - val_loss: 4.5297 - val_acc: 0.5860\n",
      "Epoch 98/100\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.2361e-07 - acc: 1.0000 - val_loss: 4.5354 - val_acc: 0.5850\n",
      "Epoch 99/100\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.2277e-07 - acc: 1.0000 - val_loss: 4.5414 - val_acc: 0.5840\n",
      "Epoch 100/100\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.2206e-07 - acc: 1.0000 - val_loss: 4.5446 - val_acc: 0.5850\n"
     ]
    }
   ],
   "source": [
    "#Creating model Sequential with reul and no regularization, epoachs 200 batch 32 \n",
    "model = Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the final results on the training and testing sets \n",
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "#final test results \n",
    "results_test = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results without regularization: \n",
      " Loss:1.2140274053156948e-07 \n",
      " Accuracy :1.0\n",
      "Test results without regularization:\n",
      " Loss:4.30288648223877 \n",
      " Accuracy:0.6086666665077209\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results without regularization: \\n Loss:{} \\n Accuracy :{}'.format(results_train[0],results_train[1],) )\n",
    "print('Test results without regularization:\\n Loss:{} \\n Accuracy:{}'.format(results_test[0] ,results_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val_dict =model_val.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 2s 221us/step - loss: 1.7411 - acc: 0.4636 - val_loss: 1.2196 - val_acc: 0.6020\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.9437 - acc: 0.7005 - val_loss: 1.0931 - val_acc: 0.6280\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.6237 - acc: 0.8032 - val_loss: 1.1450 - val_acc: 0.6290\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 0.4146 - acc: 0.8784 - val_loss: 1.2600 - val_acc: 0.6260\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.2639 - acc: 0.9307 - val_loss: 1.3820 - val_acc: 0.6300\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.1609 - acc: 0.9657 - val_loss: 1.5785 - val_acc: 0.6160\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.0896 - acc: 0.9867 - val_loss: 1.7078 - val_acc: 0.6200\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.0476 - acc: 0.9964 - val_loss: 1.8574 - val_acc: 0.6130\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.0259 - acc: 0.9993 - val_loss: 2.0001 - val_acc: 0.6090\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.0148 - acc: 0.9997 - val_loss: 2.1054 - val_acc: 0.6110\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 2.1921 - val_acc: 0.6190\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 2.2790 - val_acc: 0.6140\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 2.3485 - val_acc: 0.6210\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.4287 - val_acc: 0.6200\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.4892 - val_acc: 0.6140\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.5466 - val_acc: 0.6170\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 2.5954 - val_acc: 0.6170\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 2.6461 - val_acc: 0.6170\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 2.6914 - val_acc: 0.6170\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 2.7423 - val_acc: 0.6170\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 8.9614e-04 - acc: 1.0000 - val_loss: 2.7859 - val_acc: 0.6190\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 7.5256e-04 - acc: 1.0000 - val_loss: 2.8247 - val_acc: 0.6140\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 6.3456e-04 - acc: 1.0000 - val_loss: 2.8656 - val_acc: 0.6200\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 5.3772e-04 - acc: 1.0000 - val_loss: 2.9093 - val_acc: 0.6190\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 4.5621e-04 - acc: 1.0000 - val_loss: 2.9424 - val_acc: 0.6190\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 3.8710e-04 - acc: 1.0000 - val_loss: 2.9777 - val_acc: 0.6130\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 3.3242e-04 - acc: 1.0000 - val_loss: 3.0147 - val_acc: 0.6150\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 2.8407e-04 - acc: 1.0000 - val_loss: 3.0508 - val_acc: 0.6170\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 2.4333e-04 - acc: 1.0000 - val_loss: 3.0882 - val_acc: 0.6170\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 2.0907e-04 - acc: 1.0000 - val_loss: 3.1169 - val_acc: 0.6200\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.7987e-04 - acc: 1.0000 - val_loss: 3.1534 - val_acc: 0.6180\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.5510e-04 - acc: 1.0000 - val_loss: 3.1861 - val_acc: 0.6190\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.3398e-04 - acc: 1.0000 - val_loss: 3.2198 - val_acc: 0.6140\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.1580e-04 - acc: 1.0000 - val_loss: 3.2509 - val_acc: 0.6170\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.0017e-04 - acc: 1.0000 - val_loss: 3.2820 - val_acc: 0.6140\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 8.6734e-05 - acc: 1.0000 - val_loss: 3.3074 - val_acc: 0.6180\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 7.5177e-05 - acc: 1.0000 - val_loss: 3.3385 - val_acc: 0.6150\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 6.5118e-05 - acc: 1.0000 - val_loss: 3.3638 - val_acc: 0.6140\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 5.6528e-05 - acc: 1.0000 - val_loss: 3.3947 - val_acc: 0.6170\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 4.9126e-05 - acc: 1.0000 - val_loss: 3.4170 - val_acc: 0.6110\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 4.2796e-05 - acc: 1.0000 - val_loss: 3.4446 - val_acc: 0.6110\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 3.7209e-05 - acc: 1.0000 - val_loss: 3.4682 - val_acc: 0.6130\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 3.2346e-05 - acc: 1.0000 - val_loss: 3.4949 - val_acc: 0.6100\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 2.8164e-05 - acc: 1.0000 - val_loss: 3.5224 - val_acc: 0.6080\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 2.4552e-05 - acc: 1.0000 - val_loss: 3.5451 - val_acc: 0.6110\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 2.1351e-05 - acc: 1.0000 - val_loss: 3.5680 - val_acc: 0.6090\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 1.8643e-05 - acc: 1.0000 - val_loss: 3.5936 - val_acc: 0.6080\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.6238e-05 - acc: 1.0000 - val_loss: 3.6145 - val_acc: 0.6080\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.4202e-05 - acc: 1.0000 - val_loss: 3.6375 - val_acc: 0.6100\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.2401e-05 - acc: 1.0000 - val_loss: 3.6604 - val_acc: 0.6080\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.0829e-05 - acc: 1.0000 - val_loss: 3.6808 - val_acc: 0.6090\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 9.4287e-06 - acc: 1.0000 - val_loss: 3.6998 - val_acc: 0.6100\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 8.2865e-06 - acc: 1.0000 - val_loss: 3.7239 - val_acc: 0.6090\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 7.2265e-06 - acc: 1.0000 - val_loss: 3.7451 - val_acc: 0.6060\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 6.3278e-06 - acc: 1.0000 - val_loss: 3.7620 - val_acc: 0.6050\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 5.5453e-06 - acc: 1.0000 - val_loss: 3.7853 - val_acc: 0.6050\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 4.8503e-06 - acc: 1.0000 - val_loss: 3.8035 - val_acc: 0.6040\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 4.2504e-06 - acc: 1.0000 - val_loss: 3.8185 - val_acc: 0.6050\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 3.7285e-06 - acc: 1.0000 - val_loss: 3.8367 - val_acc: 0.6070\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 3.2700e-06 - acc: 1.0000 - val_loss: 3.8579 - val_acc: 0.6040\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping at 50 epochs\n",
    "model = Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "#Early Stoping final results on the training and testing sets \n",
    "results_train_early = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping final test results \n",
    "results_test_early = model.evaluate(test,label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results- Early Stopping: \n",
      " Loss:2.8317204816630692e-06 \n",
      " Accuracy :1.0\n",
      "Test results -Early Stopping:\n",
      " Loss:3.7990979766845703 \n",
      " Accuracy:0.603333333492279\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results- Early Stopping: \\n Loss:{} \\n Accuracy :{}'.format(results_train_early[0],results_train_early[1],) )\n",
    "print('Test results -Early Stopping:\\n Loss:{} \\n Accuracy:{}'.format(results_test_early[0] ,results_test_early[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 2s 218us/step - loss: 1.9893 - acc: 0.3951 - val_loss: 1.4058 - val_acc: 0.5370\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.2742 - acc: 0.5944 - val_loss: 1.1736 - val_acc: 0.6330\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.9976 - acc: 0.6868 - val_loss: 1.1084 - val_acc: 0.6320\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.8036 - acc: 0.7468 - val_loss: 1.1163 - val_acc: 0.6330\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.6759 - acc: 0.7899 - val_loss: 1.1558 - val_acc: 0.6430\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.5501 - acc: 0.8251 - val_loss: 1.2326 - val_acc: 0.6320\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.4704 - acc: 0.8547 - val_loss: 1.2768 - val_acc: 0.6280\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.3936 - acc: 0.8773 - val_loss: 1.3617 - val_acc: 0.6230\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.3348 - acc: 0.8985 - val_loss: 1.4249 - val_acc: 0.6280\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.2833 - acc: 0.9157 - val_loss: 1.5058 - val_acc: 0.6240\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.2385 - acc: 0.9285 - val_loss: 1.5968 - val_acc: 0.6270\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.2161 - acc: 0.9365 - val_loss: 1.6457 - val_acc: 0.6360\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.1813 - acc: 0.9488 - val_loss: 1.7502 - val_acc: 0.6330\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.1650 - acc: 0.9497 - val_loss: 1.8078 - val_acc: 0.6260\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 0.1464 - acc: 0.9583 - val_loss: 1.9002 - val_acc: 0.6270\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.1336 - acc: 0.9589 - val_loss: 1.9847 - val_acc: 0.6120\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.1213 - acc: 0.9621 - val_loss: 1.9804 - val_acc: 0.6270\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.1120 - acc: 0.9665 - val_loss: 2.0861 - val_acc: 0.6150\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.0938 - acc: 0.9727 - val_loss: 2.1342 - val_acc: 0.6180\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.0836 - acc: 0.9752 - val_loss: 2.1785 - val_acc: 0.6130\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.0887 - acc: 0.9755 - val_loss: 2.2445 - val_acc: 0.6140\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.0825 - acc: 0.9747 - val_loss: 2.2883 - val_acc: 0.6090\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.0775 - acc: 0.9773 - val_loss: 2.3071 - val_acc: 0.6040\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.0806 - acc: 0.9767 - val_loss: 2.3014 - val_acc: 0.6140\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0681 - acc: 0.9805 - val_loss: 2.4072 - val_acc: 0.6220\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.0697 - acc: 0.9783 - val_loss: 2.4778 - val_acc: 0.6070\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.0582 - acc: 0.9843 - val_loss: 2.5264 - val_acc: 0.6110\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.0601 - acc: 0.9824 - val_loss: 2.4845 - val_acc: 0.6020\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.0537 - acc: 0.9825 - val_loss: 2.5374 - val_acc: 0.6080\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.0562 - acc: 0.9825 - val_loss: 2.5631 - val_acc: 0.6150\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0546 - acc: 0.9820 - val_loss: 2.6174 - val_acc: 0.6110\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.0469 - acc: 0.9863 - val_loss: 2.7391 - val_acc: 0.5960\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.0489 - acc: 0.9865 - val_loss: 2.7032 - val_acc: 0.5940\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.0506 - acc: 0.9827 - val_loss: 2.7522 - val_acc: 0.6070\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.0413 - acc: 0.9872 - val_loss: 2.7241 - val_acc: 0.6050\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.0454 - acc: 0.9859 - val_loss: 2.7836 - val_acc: 0.6140\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0437 - acc: 0.9864 - val_loss: 2.8066 - val_acc: 0.6030\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.0364 - acc: 0.9887 - val_loss: 2.8590 - val_acc: 0.6080\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.0396 - acc: 0.9873 - val_loss: 2.8518 - val_acc: 0.6050\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.0396 - acc: 0.9879 - val_loss: 2.8633 - val_acc: 0.6150\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.0389 - acc: 0.9884 - val_loss: 2.9331 - val_acc: 0.6100\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.0394 - acc: 0.9880 - val_loss: 2.9000 - val_acc: 0.6080\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.0350 - acc: 0.9891 - val_loss: 2.9488 - val_acc: 0.6070\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.0334 - acc: 0.9913 - val_loss: 2.9727 - val_acc: 0.6150\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.0353 - acc: 0.9891 - val_loss: 2.9446 - val_acc: 0.6080\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.0351 - acc: 0.9885 - val_loss: 3.0474 - val_acc: 0.6170\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.0325 - acc: 0.9897 - val_loss: 3.0328 - val_acc: 0.6050\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.0345 - acc: 0.9880 - val_loss: 3.0870 - val_acc: 0.6060\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.0329 - acc: 0.9899 - val_loss: 3.1159 - val_acc: 0.6030\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 3.1623 - val_acc: 0.6090\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.0266 - acc: 0.9913 - val_loss: 3.1934 - val_acc: 0.6040\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.0295 - acc: 0.9903 - val_loss: 3.2251 - val_acc: 0.6050\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.0362 - acc: 0.9884 - val_loss: 3.1685 - val_acc: 0.6070\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.0320 - acc: 0.9905 - val_loss: 3.2544 - val_acc: 0.6100\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.0288 - acc: 0.9897 - val_loss: 3.3098 - val_acc: 0.6030\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.0263 - acc: 0.9923 - val_loss: 3.3264 - val_acc: 0.6090\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.0273 - acc: 0.9909 - val_loss: 3.3282 - val_acc: 0.6130\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0310 - acc: 0.9908 - val_loss: 3.2885 - val_acc: 0.6160\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.0310 - acc: 0.9904 - val_loss: 3.2526 - val_acc: 0.6190\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.0261 - acc: 0.9913 - val_loss: 3.2796 - val_acc: 0.6080\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.0258 - acc: 0.9924 - val_loss: 3.2779 - val_acc: 0.6080\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0277 - acc: 0.9913 - val_loss: 3.3044 - val_acc: 0.6100\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 0.0292 - acc: 0.9909 - val_loss: 3.3628 - val_acc: 0.6080\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.0251 - acc: 0.9924 - val_loss: 3.4098 - val_acc: 0.6120\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0268 - acc: 0.9929 - val_loss: 3.3607 - val_acc: 0.6040\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.0217 - acc: 0.9929 - val_loss: 3.3876 - val_acc: 0.6050\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.0233 - acc: 0.9917 - val_loss: 3.3663 - val_acc: 0.6170\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 3.4421 - val_acc: 0.6080\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.0257 - acc: 0.9917 - val_loss: 3.4143 - val_acc: 0.6090\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.0231 - acc: 0.9925 - val_loss: 3.4385 - val_acc: 0.6140\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 3.3656 - val_acc: 0.6170\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.0266 - acc: 0.9923 - val_loss: 3.4407 - val_acc: 0.6140\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.0242 - acc: 0.9921 - val_loss: 3.4769 - val_acc: 0.6130\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.0256 - acc: 0.9909 - val_loss: 3.4363 - val_acc: 0.6160\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.0236 - acc: 0.9920 - val_loss: 3.5144 - val_acc: 0.6140\n"
     ]
    }
   ],
   "source": [
    "#Drop out method\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "drop_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step\n",
      "1500/1500 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_drop= model.evaluate(train_final, label_train_final)\n",
    "results_test_drop = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results Drop Model: \n",
      " Loss:0.0002415136418567272 \n",
      " Accuracy :1.0\n",
      "Test results Drop Model:\n",
      " Loss:3.186903839747111 \n",
      " Accuracy:0.6379999998410543\n"
     ]
    }
   ],
   "source": [
    "print('Training results Drop Model: \\n Loss:{} \\n Accuracy :{}'.format(results_train_drop[0],results_train_drop[1],) )\n",
    "print('Test results Drop Model:\\n Loss:{} \\n Accuracy:{}'.format(results_test_drop[0] ,results_test_drop[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 2s 217us/step - loss: 2.2891 - acc: 0.4557 - val_loss: 1.7213 - val_acc: 0.5850\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.5783 - acc: 0.6289 - val_loss: 1.6034 - val_acc: 0.6250\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.4690 - acc: 0.6677 - val_loss: 1.5806 - val_acc: 0.6170\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 1.4022 - acc: 0.6928 - val_loss: 1.5547 - val_acc: 0.6270\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.3603 - acc: 0.7041 - val_loss: 1.5337 - val_acc: 0.6300\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.3249 - acc: 0.7104 - val_loss: 1.5388 - val_acc: 0.6250\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 1.2967 - acc: 0.7193 - val_loss: 1.5462 - val_acc: 0.6230\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.2725 - acc: 0.7256 - val_loss: 1.5328 - val_acc: 0.6180\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.2544 - acc: 0.7303 - val_loss: 1.5279 - val_acc: 0.6280\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 1.2287 - acc: 0.7397 - val_loss: 1.5290 - val_acc: 0.6240\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.2038 - acc: 0.7455 - val_loss: 1.5461 - val_acc: 0.6230\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.1867 - acc: 0.7561 - val_loss: 1.5247 - val_acc: 0.6380\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.1625 - acc: 0.7631 - val_loss: 1.5383 - val_acc: 0.6390\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.1521 - acc: 0.7651 - val_loss: 1.5358 - val_acc: 0.6210\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.1306 - acc: 0.7757 - val_loss: 1.5504 - val_acc: 0.6220\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.1072 - acc: 0.7839 - val_loss: 1.5400 - val_acc: 0.6370\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.0953 - acc: 0.7900 - val_loss: 1.5693 - val_acc: 0.6300\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.0725 - acc: 0.7968 - val_loss: 1.5663 - val_acc: 0.6330\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.0530 - acc: 0.8023 - val_loss: 1.5782 - val_acc: 0.6340\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.0364 - acc: 0.8096 - val_loss: 1.5921 - val_acc: 0.6210\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.0202 - acc: 0.8215 - val_loss: 1.6029 - val_acc: 0.6370\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.0072 - acc: 0.8213 - val_loss: 1.6161 - val_acc: 0.6290\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 0.9813 - acc: 0.8321 - val_loss: 1.6308 - val_acc: 0.6070\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 0.9671 - acc: 0.8384 - val_loss: 1.6306 - val_acc: 0.6440\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.9561 - acc: 0.8485 - val_loss: 1.6197 - val_acc: 0.6280\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 0.9366 - acc: 0.8548 - val_loss: 1.6843 - val_acc: 0.6030\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.9208 - acc: 0.8593 - val_loss: 1.6457 - val_acc: 0.6160\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.9026 - acc: 0.8660 - val_loss: 1.6950 - val_acc: 0.6120\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.8890 - acc: 0.8709 - val_loss: 1.6815 - val_acc: 0.6280\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.8808 - acc: 0.8768 - val_loss: 1.7216 - val_acc: 0.6210\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.8619 - acc: 0.8831 - val_loss: 1.7093 - val_acc: 0.6280\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.8509 - acc: 0.8869 - val_loss: 1.7389 - val_acc: 0.6280\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.8401 - acc: 0.8905 - val_loss: 1.7214 - val_acc: 0.6300\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.8288 - acc: 0.8949 - val_loss: 1.7381 - val_acc: 0.6170\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.8123 - acc: 0.9032 - val_loss: 1.7312 - val_acc: 0.6250\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.8019 - acc: 0.9069 - val_loss: 1.7815 - val_acc: 0.6150\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.7938 - acc: 0.9071 - val_loss: 1.7671 - val_acc: 0.6270\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.7781 - acc: 0.9099 - val_loss: 1.7653 - val_acc: 0.6370\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.7744 - acc: 0.9125 - val_loss: 1.7724 - val_acc: 0.6170\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.7583 - acc: 0.9207 - val_loss: 1.7999 - val_acc: 0.6100\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.7473 - acc: 0.9216 - val_loss: 1.7866 - val_acc: 0.6180\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.7453 - acc: 0.9216 - val_loss: 1.8119 - val_acc: 0.6140\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.7351 - acc: 0.9275 - val_loss: 1.8158 - val_acc: 0.6130\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.7196 - acc: 0.9307 - val_loss: 1.7940 - val_acc: 0.6100\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.7147 - acc: 0.9317 - val_loss: 1.8296 - val_acc: 0.6190\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.7060 - acc: 0.9353 - val_loss: 1.8280 - val_acc: 0.6170\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.7032 - acc: 0.9327 - val_loss: 1.8164 - val_acc: 0.6260\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.7027 - acc: 0.9317 - val_loss: 1.8382 - val_acc: 0.6000\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.6849 - acc: 0.9408 - val_loss: 1.8146 - val_acc: 0.6010\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.6789 - acc: 0.9413 - val_loss: 1.8350 - val_acc: 0.6080\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.6784 - acc: 0.9404 - val_loss: 1.8263 - val_acc: 0.6320\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.6637 - acc: 0.9447 - val_loss: 1.8274 - val_acc: 0.6120\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.6553 - acc: 0.9460 - val_loss: 1.8563 - val_acc: 0.6070\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.6583 - acc: 0.9439 - val_loss: 1.8803 - val_acc: 0.6000\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.6449 - acc: 0.9488 - val_loss: 1.8347 - val_acc: 0.6250\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.6372 - acc: 0.9520 - val_loss: 1.8396 - val_acc: 0.6140\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.6366 - acc: 0.9497 - val_loss: 1.8566 - val_acc: 0.6060\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.6400 - acc: 0.9500 - val_loss: 1.8936 - val_acc: 0.6070\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.6292 - acc: 0.9525 - val_loss: 1.8886 - val_acc: 0.6010\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.6143 - acc: 0.9588 - val_loss: 1.8538 - val_acc: 0.5990\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.6055 - acc: 0.9584 - val_loss: 1.8768 - val_acc: 0.5960\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.6080 - acc: 0.9549 - val_loss: 1.9441 - val_acc: 0.5980\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.6224 - acc: 0.9481 - val_loss: 1.8730 - val_acc: 0.6030\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.6229 - acc: 0.9499 - val_loss: 1.9176 - val_acc: 0.6190\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.5931 - acc: 0.9605 - val_loss: 1.9055 - val_acc: 0.5780\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.5882 - acc: 0.9601 - val_loss: 1.9131 - val_acc: 0.5980\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.5769 - acc: 0.9655 - val_loss: 1.9021 - val_acc: 0.6040\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.5857 - acc: 0.9547 - val_loss: 1.9132 - val_acc: 0.5990\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.5887 - acc: 0.9553 - val_loss: 1.9878 - val_acc: 0.5730\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.5776 - acc: 0.9615 - val_loss: 1.9496 - val_acc: 0.5980\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.5717 - acc: 0.9625 - val_loss: 1.9719 - val_acc: 0.5900\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.5718 - acc: 0.9609 - val_loss: 1.9401 - val_acc: 0.5890\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.5595 - acc: 0.9656 - val_loss: 1.9459 - val_acc: 0.5830\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.5667 - acc: 0.9587 - val_loss: 1.9420 - val_acc: 0.6000\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.5699 - acc: 0.9564 - val_loss: 1.9766 - val_acc: 0.5970\n"
     ]
    }
   ],
   "source": [
    "#L2\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,)))\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(19,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_reg = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 62us/step\n",
      "1500/1500 [==============================] - 0s 51us/step\n"
     ]
    }
   ],
   "source": [
    "#l2 training test \n",
    "results_train_L2= model.evaluate(train_final, label_train_final)\n",
    "results_test_L2 = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results L2 Model: \n",
      " Loss:0.5064801580905914 \n",
      " Accuracy :0.9810666666666666\n",
      "Test results L2 Model:\n",
      " Loss:1.8648234329223632 \n",
      " Accuracy:0.6219999996821085\n"
     ]
    }
   ],
   "source": [
    "#print l2\n",
    "print('Training results L2 Model: \\n Loss:{} \\n Accuracy :{}'.format(results_train_L2[0],results_train_L2[1],) )\n",
    "print('Test results L2 Model:\\n Loss:{} \\n Accuracy:{}'.format(results_test_L2[0] ,results_test_L2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 2.2287 - acc: 0.3241 - val_loss: 1.6528 - val_acc: 0.4750\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.5925 - acc: 0.5005 - val_loss: 1.3321 - val_acc: 0.5720\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.3098 - acc: 0.5819 - val_loss: 1.2207 - val_acc: 0.5920\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.1467 - acc: 0.6339 - val_loss: 1.1686 - val_acc: 0.6220\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.0034 - acc: 0.6763 - val_loss: 1.1547 - val_acc: 0.6320\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.9291 - acc: 0.6992 - val_loss: 1.1438 - val_acc: 0.6360\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.8089 - acc: 0.7405 - val_loss: 1.1585 - val_acc: 0.6380\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.7367 - acc: 0.7617 - val_loss: 1.1978 - val_acc: 0.6420\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.6711 - acc: 0.7869 - val_loss: 1.2152 - val_acc: 0.6410\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.6333 - acc: 0.7997 - val_loss: 1.2196 - val_acc: 0.6390\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.5633 - acc: 0.8180 - val_loss: 1.2778 - val_acc: 0.6300\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.5352 - acc: 0.8328 - val_loss: 1.3159 - val_acc: 0.6400\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.4948 - acc: 0.8391 - val_loss: 1.3494 - val_acc: 0.6300\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.4530 - acc: 0.8511 - val_loss: 1.4168 - val_acc: 0.6310\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.4215 - acc: 0.8657 - val_loss: 1.4521 - val_acc: 0.6310\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.4036 - acc: 0.8701 - val_loss: 1.5056 - val_acc: 0.6330\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.3824 - acc: 0.8795 - val_loss: 1.5442 - val_acc: 0.6290\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.3494 - acc: 0.8876 - val_loss: 1.6094 - val_acc: 0.6230\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.3302 - acc: 0.8949 - val_loss: 1.6317 - val_acc: 0.6220\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.3191 - acc: 0.8984 - val_loss: 1.6963 - val_acc: 0.6290\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.3034 - acc: 0.9011 - val_loss: 1.7473 - val_acc: 0.6170\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.2880 - acc: 0.9060 - val_loss: 1.7651 - val_acc: 0.6260\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.2927 - acc: 0.9087 - val_loss: 1.7792 - val_acc: 0.6190\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.2676 - acc: 0.9108 - val_loss: 1.8554 - val_acc: 0.6190\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.2525 - acc: 0.9180 - val_loss: 1.8701 - val_acc: 0.6200\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.2437 - acc: 0.9199 - val_loss: 1.9737 - val_acc: 0.6230\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.2363 - acc: 0.9228 - val_loss: 2.0501 - val_acc: 0.6110\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.2237 - acc: 0.9296 - val_loss: 2.0469 - val_acc: 0.6160\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.2172 - acc: 0.9285 - val_loss: 2.0874 - val_acc: 0.6190\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.2162 - acc: 0.9303 - val_loss: 2.1378 - val_acc: 0.6200\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.2153 - acc: 0.9309 - val_loss: 2.1955 - val_acc: 0.6330\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1996 - acc: 0.9359 - val_loss: 2.2223 - val_acc: 0.6170\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.1905 - acc: 0.9368 - val_loss: 2.2446 - val_acc: 0.6040\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.1811 - acc: 0.9419 - val_loss: 2.2718 - val_acc: 0.6040\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 0.1925 - acc: 0.9372 - val_loss: 2.3324 - val_acc: 0.6180\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.1872 - acc: 0.9412 - val_loss: 2.4045 - val_acc: 0.6080\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.1787 - acc: 0.9415 - val_loss: 2.4157 - val_acc: 0.6150\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.1708 - acc: 0.9452 - val_loss: 2.4622 - val_acc: 0.6120\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 0.1763 - acc: 0.9453 - val_loss: 2.4194 - val_acc: 0.6120\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 0.1755 - acc: 0.9445 - val_loss: 2.4991 - val_acc: 0.6110\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.1625 - acc: 0.9464 - val_loss: 2.5937 - val_acc: 0.6220\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 0.1702 - acc: 0.9464 - val_loss: 2.5997 - val_acc: 0.6210\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.1693 - acc: 0.9451 - val_loss: 2.5810 - val_acc: 0.6240\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.1708 - acc: 0.9445 - val_loss: 2.5554 - val_acc: 0.6150\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.1621 - acc: 0.9481 - val_loss: 2.6258 - val_acc: 0.6210\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 0.1515 - acc: 0.9519 - val_loss: 2.6301 - val_acc: 0.6140\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 0.1461 - acc: 0.9529 - val_loss: 2.6444 - val_acc: 0.6150\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.1467 - acc: 0.9524 - val_loss: 2.6562 - val_acc: 0.6150\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 0.1414 - acc: 0.9544 - val_loss: 2.7401 - val_acc: 0.6160\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.1387 - acc: 0.9548 - val_loss: 2.7688 - val_acc: 0.6150\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.1556 - acc: 0.9524 - val_loss: 2.7866 - val_acc: 0.6080\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.1440 - acc: 0.9528 - val_loss: 2.7548 - val_acc: 0.6200\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.1313 - acc: 0.9580 - val_loss: 2.8167 - val_acc: 0.6140\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.1347 - acc: 0.9556 - val_loss: 2.8433 - val_acc: 0.6110\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.1312 - acc: 0.9596 - val_loss: 2.8708 - val_acc: 0.6120\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1365 - acc: 0.9559 - val_loss: 2.8938 - val_acc: 0.6080\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 0.1283 - acc: 0.9565 - val_loss: 2.9486 - val_acc: 0.6100\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.1264 - acc: 0.9573 - val_loss: 2.9671 - val_acc: 0.6170\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.1223 - acc: 0.9603 - val_loss: 2.9808 - val_acc: 0.6130\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.1329 - acc: 0.9577 - val_loss: 2.9719 - val_acc: 0.6130\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.1246 - acc: 0.9604 - val_loss: 2.9748 - val_acc: 0.6210\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 0.1155 - acc: 0.9617 - val_loss: 3.0265 - val_acc: 0.6140\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 0.1261 - acc: 0.9605 - val_loss: 2.9752 - val_acc: 0.6160\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.1199 - acc: 0.9633 - val_loss: 3.0237 - val_acc: 0.6130\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.1165 - acc: 0.9657 - val_loss: 3.1014 - val_acc: 0.6190\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1182 - acc: 0.9641 - val_loss: 3.0813 - val_acc: 0.6200\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.1208 - acc: 0.9619 - val_loss: 3.1102 - val_acc: 0.6230\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.1243 - acc: 0.9568 - val_loss: 3.0276 - val_acc: 0.6140\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.1247 - acc: 0.9604 - val_loss: 3.0527 - val_acc: 0.6120\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.1237 - acc: 0.9611 - val_loss: 3.1048 - val_acc: 0.6170\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1175 - acc: 0.9607 - val_loss: 3.1403 - val_acc: 0.6110\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.1211 - acc: 0.9641 - val_loss: 3.1861 - val_acc: 0.6130\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.1208 - acc: 0.9621 - val_loss: 3.1532 - val_acc: 0.6160\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.1218 - acc: 0.9637 - val_loss: 3.1564 - val_acc: 0.6170\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.1154 - acc: 0.9637 - val_loss: 3.2365 - val_acc: 0.6080\n"
     ]
    }
   ],
   "source": [
    "#drop .5\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "drop_model_5 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 54us/step\n",
      "1500/1500 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "# Drop 50 \n",
    "results_train_drop_5= model.evaluate(train_final, label_train_final)\n",
    "results_test_drop_5= model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results Drop Model 50%: \n",
      " Loss:0.007141579612173761 \n",
      " Accuracy :0.9985333333333334\n",
      "Test results  Drop Model 50%:\n",
      " Loss:2.9891765251159668 \n",
      " Accuracy:0.6460000004768371\n"
     ]
    }
   ],
   "source": [
    "#print l1\n",
    "print('Training results Drop Model 50%: \\n Loss:{} \\n Accuracy :{}'.format(results_train_drop_5[0],results_train_drop_5[1],) )\n",
    "print('Test results  Drop Model 50%:\\n Loss:{} \\n Accuracy:{}'.format(results_test_drop_5[0] ,results_test_drop_5[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 2s 256us/step - loss: 5.7067 - acc: 0.2887 - val_loss: 2.4692 - val_acc: 0.3790\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 2.2647 - acc: 0.4193 - val_loss: 2.1781 - val_acc: 0.4080\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 2.0910 - acc: 0.4541 - val_loss: 2.0732 - val_acc: 0.4360\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 2.0052 - acc: 0.4685 - val_loss: 2.0105 - val_acc: 0.4540\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 1.9586 - acc: 0.4751 - val_loss: 1.9734 - val_acc: 0.4640\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.9322 - acc: 0.4797 - val_loss: 1.9470 - val_acc: 0.4740\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.9070 - acc: 0.4825 - val_loss: 1.9223 - val_acc: 0.4810\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.8848 - acc: 0.4869 - val_loss: 1.9102 - val_acc: 0.4770\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.8604 - acc: 0.5056 - val_loss: 1.8795 - val_acc: 0.5050\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.8436 - acc: 0.5184 - val_loss: 1.8684 - val_acc: 0.5130\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.8210 - acc: 0.5315 - val_loss: 1.8534 - val_acc: 0.5200\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.8024 - acc: 0.5388 - val_loss: 1.8354 - val_acc: 0.5370\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.7884 - acc: 0.5509 - val_loss: 1.8240 - val_acc: 0.5340\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.7730 - acc: 0.5613 - val_loss: 1.8099 - val_acc: 0.5480\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.7583 - acc: 0.5671 - val_loss: 1.7976 - val_acc: 0.5590\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.7502 - acc: 0.5687 - val_loss: 1.7890 - val_acc: 0.5610\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.7369 - acc: 0.5749 - val_loss: 1.7838 - val_acc: 0.5660\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.7281 - acc: 0.5793 - val_loss: 1.7723 - val_acc: 0.5670\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.7194 - acc: 0.5817 - val_loss: 1.7738 - val_acc: 0.5760\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.7130 - acc: 0.5837 - val_loss: 1.7605 - val_acc: 0.5740\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.6988 - acc: 0.5851 - val_loss: 1.7538 - val_acc: 0.5760\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 1.6934 - acc: 0.5916 - val_loss: 1.7560 - val_acc: 0.5720\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.6867 - acc: 0.5948 - val_loss: 1.7410 - val_acc: 0.5830\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.6804 - acc: 0.5991 - val_loss: 1.7408 - val_acc: 0.5870\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.6755 - acc: 0.6047 - val_loss: 1.7426 - val_acc: 0.5830\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.6711 - acc: 0.6071 - val_loss: 1.7570 - val_acc: 0.5960\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.6625 - acc: 0.6116 - val_loss: 1.7255 - val_acc: 0.5930\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.6586 - acc: 0.6125 - val_loss: 1.7294 - val_acc: 0.6080\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.6542 - acc: 0.6203 - val_loss: 1.7304 - val_acc: 0.6000\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.6529 - acc: 0.6231 - val_loss: 1.7275 - val_acc: 0.6100\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.6521 - acc: 0.6247 - val_loss: 1.7233 - val_acc: 0.6080\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.6402 - acc: 0.6295 - val_loss: 1.7149 - val_acc: 0.6120\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.6378 - acc: 0.6351 - val_loss: 1.7100 - val_acc: 0.6090\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.6397 - acc: 0.6313 - val_loss: 1.7214 - val_acc: 0.6100\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.6410 - acc: 0.6349 - val_loss: 1.7181 - val_acc: 0.6170\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.6317 - acc: 0.6429 - val_loss: 1.7149 - val_acc: 0.6180\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.6297 - acc: 0.6375 - val_loss: 1.7146 - val_acc: 0.6180\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.6304 - acc: 0.6415 - val_loss: 1.7074 - val_acc: 0.6170\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.6217 - acc: 0.6435 - val_loss: 1.7035 - val_acc: 0.6120\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.6207 - acc: 0.6459 - val_loss: 1.7042 - val_acc: 0.6230\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 1.6245 - acc: 0.6435 - val_loss: 1.7071 - val_acc: 0.6290\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.6170 - acc: 0.6480 - val_loss: 1.6972 - val_acc: 0.6270\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.6186 - acc: 0.6445 - val_loss: 1.7157 - val_acc: 0.6170\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 1.6151 - acc: 0.6497 - val_loss: 1.6974 - val_acc: 0.6260\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.6113 - acc: 0.6493 - val_loss: 1.6950 - val_acc: 0.6220\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.6082 - acc: 0.6451 - val_loss: 1.6977 - val_acc: 0.6250\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.6076 - acc: 0.6477 - val_loss: 1.6973 - val_acc: 0.6100\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.6070 - acc: 0.6500 - val_loss: 1.6996 - val_acc: 0.6260\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.6066 - acc: 0.6476 - val_loss: 1.7010 - val_acc: 0.6230\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.6048 - acc: 0.6509 - val_loss: 1.7022 - val_acc: 0.6280\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.6033 - acc: 0.6505 - val_loss: 1.6889 - val_acc: 0.6180\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.5988 - acc: 0.6525 - val_loss: 1.6913 - val_acc: 0.6190\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 1.5991 - acc: 0.6549 - val_loss: 1.7161 - val_acc: 0.6140\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.5987 - acc: 0.6539 - val_loss: 1.6922 - val_acc: 0.6240\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.5977 - acc: 0.6507 - val_loss: 1.6987 - val_acc: 0.6350\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.5957 - acc: 0.6552 - val_loss: 1.7003 - val_acc: 0.6190\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.5922 - acc: 0.6544 - val_loss: 1.7155 - val_acc: 0.6250\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.5943 - acc: 0.6549 - val_loss: 1.6857 - val_acc: 0.6310\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.5898 - acc: 0.6597 - val_loss: 1.6870 - val_acc: 0.6240\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.5903 - acc: 0.6605 - val_loss: 1.6994 - val_acc: 0.6180\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.5890 - acc: 0.6584 - val_loss: 1.6943 - val_acc: 0.6260\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.5843 - acc: 0.6575 - val_loss: 1.6974 - val_acc: 0.6280\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 1.5852 - acc: 0.6547 - val_loss: 1.6994 - val_acc: 0.6290\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 1.5833 - acc: 0.6595 - val_loss: 1.6953 - val_acc: 0.6270\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.5849 - acc: 0.6603 - val_loss: 1.6961 - val_acc: 0.6270\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 1.5803 - acc: 0.6621 - val_loss: 1.6972 - val_acc: 0.6170\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.5797 - acc: 0.6629 - val_loss: 1.7112 - val_acc: 0.6270\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 1.5811 - acc: 0.6616 - val_loss: 1.7085 - val_acc: 0.6120\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.5766 - acc: 0.6605 - val_loss: 1.6871 - val_acc: 0.6290\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.5767 - acc: 0.6643 - val_loss: 1.6826 - val_acc: 0.6350\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.5736 - acc: 0.6600 - val_loss: 1.6985 - val_acc: 0.6160\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.5695 - acc: 0.6641 - val_loss: 1.6948 - val_acc: 0.6240\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.5767 - acc: 0.6613 - val_loss: 1.6972 - val_acc: 0.6300\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.5755 - acc: 0.6620 - val_loss: 1.6875 - val_acc: 0.6260\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.5678 - acc: 0.6655 - val_loss: 1.6989 - val_acc: 0.6310\n"
     ]
    }
   ],
   "source": [
    "#L1 with Dropout \n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(19,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L1_reg = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step\n",
      "1500/1500 [==============================] - 0s 68us/step\n"
     ]
    }
   ],
   "source": [
    "#l1\n",
    "#l1 training test \n",
    "results_train_L1= model.evaluate(train_final, label_train_final)\n",
    "results_test_L1= model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results L1 Drop Model: \n",
      " Loss:1.4909427564620972 \n",
      " Accuracy :0.6938666666666666\n",
      "Test results  L1 Drop Model:\n",
      " Loss:1.6464287048975628 \n",
      " Accuracy:0.6366666661898295\n"
     ]
    }
   ],
   "source": [
    "#l1\n",
    "#print l1\n",
    "print('Training results L1 Drop Model: \\n Loss:{} \\n Accuracy :{}'.format(results_train_L1[0],results_train_L1[1],) )\n",
    "print('Test results  L1 Drop Model:\\n Loss:{} \\n Accuracy:{}'.format(results_test_L1[0] ,results_test_L1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take random sample of 40000\n",
    "wine2= df_wine.dropna()\n",
    "wine2 = df_wine.sample(40000)\n",
    "wine2.index = range(40000)\n",
    "product = wine2[\"category\"]\n",
    "complaints =wine2[\"description\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/200\n",
      "33000/33000 [==============================] - 6s 188us/step - loss: 1.6842 - acc: 0.4679 - val_loss: 1.1527 - val_acc: 0.6263\n",
      "Epoch 2/200\n",
      "33000/33000 [==============================] - 5s 147us/step - loss: 1.2188 - acc: 0.6062 - val_loss: 1.0083 - val_acc: 0.6733\n",
      "Epoch 3/200\n",
      "33000/33000 [==============================] - 5s 158us/step - loss: 1.0810 - acc: 0.6514 - val_loss: 0.9671 - val_acc: 0.6810\n",
      "Epoch 4/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.9989 - acc: 0.6776 - val_loss: 0.9626 - val_acc: 0.6877\n",
      "Epoch 5/200\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.9391 - acc: 0.6971 - val_loss: 0.9578 - val_acc: 0.6857\n",
      "Epoch 6/200\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.8853 - acc: 0.7124 - val_loss: 0.9575 - val_acc: 0.6880\n",
      "Epoch 7/200\n",
      "33000/33000 [==============================] - 5s 145us/step - loss: 0.8403 - acc: 0.7295 - val_loss: 0.9592 - val_acc: 0.6883\n",
      "Epoch 8/200\n",
      "33000/33000 [==============================] - 5s 161us/step - loss: 0.8104 - acc: 0.7392 - val_loss: 0.9811 - val_acc: 0.6887\n",
      "Epoch 9/200\n",
      "33000/33000 [==============================] - 5s 159us/step - loss: 0.7702 - acc: 0.7511 - val_loss: 0.9820 - val_acc: 0.6910\n",
      "Epoch 10/200\n",
      "33000/33000 [==============================] - 5s 160us/step - loss: 0.7450 - acc: 0.7583 - val_loss: 1.0015 - val_acc: 0.6900\n",
      "Epoch 11/200\n",
      "33000/33000 [==============================] - 5s 160us/step - loss: 0.7094 - acc: 0.7697 - val_loss: 1.0135 - val_acc: 0.6863\n",
      "Epoch 12/200\n",
      "33000/33000 [==============================] - 5s 161us/step - loss: 0.6924 - acc: 0.7749 - val_loss: 1.0672 - val_acc: 0.6853\n",
      "Epoch 13/200\n",
      "33000/33000 [==============================] - 5s 160us/step - loss: 0.6712 - acc: 0.7823 - val_loss: 1.0557 - val_acc: 0.6837\n",
      "Epoch 14/200\n",
      "33000/33000 [==============================] - 5s 163us/step - loss: 0.6515 - acc: 0.7867 - val_loss: 1.0992 - val_acc: 0.6767\n",
      "Epoch 15/200\n",
      "33000/33000 [==============================] - 5s 161us/step - loss: 0.6299 - acc: 0.7960 - val_loss: 1.0862 - val_acc: 0.6793\n",
      "Epoch 16/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.6103 - acc: 0.7994 - val_loss: 1.1098 - val_acc: 0.6747\n",
      "Epoch 17/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.5933 - acc: 0.8058 - val_loss: 1.1397 - val_acc: 0.6793\n",
      "Epoch 18/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.5842 - acc: 0.8058 - val_loss: 1.1692 - val_acc: 0.6803\n",
      "Epoch 19/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.5708 - acc: 0.8134 - val_loss: 1.1852 - val_acc: 0.6793\n",
      "Epoch 20/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.5558 - acc: 0.8165 - val_loss: 1.2131 - val_acc: 0.6853\n",
      "Epoch 21/200\n",
      "33000/33000 [==============================] - 5s 158us/step - loss: 0.5454 - acc: 0.8208 - val_loss: 1.2343 - val_acc: 0.6807\n",
      "Epoch 22/200\n",
      "33000/33000 [==============================] - 5s 160us/step - loss: 0.5299 - acc: 0.8258 - val_loss: 1.2954 - val_acc: 0.6823\n",
      "Epoch 23/200\n",
      "33000/33000 [==============================] - 5s 161us/step - loss: 0.5214 - acc: 0.8293 - val_loss: 1.3228 - val_acc: 0.6750\n",
      "Epoch 24/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.5154 - acc: 0.8311 - val_loss: 1.3351 - val_acc: 0.6810\n",
      "Epoch 25/200\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.5053 - acc: 0.8326 - val_loss: 1.3881 - val_acc: 0.6750\n",
      "Epoch 26/200\n",
      "33000/33000 [==============================] - 5s 155us/step - loss: 0.4994 - acc: 0.8349 - val_loss: 1.3498 - val_acc: 0.6720\n",
      "Epoch 27/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.4961 - acc: 0.8373 - val_loss: 1.3886 - val_acc: 0.6800\n",
      "Epoch 28/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.4876 - acc: 0.8365 - val_loss: 1.4029 - val_acc: 0.6750\n",
      "Epoch 29/200\n",
      "33000/33000 [==============================] - 5s 164us/step - loss: 0.4755 - acc: 0.8408 - val_loss: 1.4663 - val_acc: 0.6763\n",
      "Epoch 30/200\n",
      "33000/33000 [==============================] - 6s 167us/step - loss: 0.4745 - acc: 0.8430 - val_loss: 1.4491 - val_acc: 0.6773\n",
      "Epoch 31/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.4644 - acc: 0.8459 - val_loss: 1.5178 - val_acc: 0.6787\n",
      "Epoch 32/200\n",
      "33000/33000 [==============================] - 5s 163us/step - loss: 0.4637 - acc: 0.8461 - val_loss: 1.5060 - val_acc: 0.6763\n",
      "Epoch 33/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.4401 - acc: 0.8507 - val_loss: 1.5572 - val_acc: 0.6810\n",
      "Epoch 34/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.4400 - acc: 0.8525 - val_loss: 1.5382 - val_acc: 0.6770\n",
      "Epoch 35/200\n",
      "33000/33000 [==============================] - 5s 159us/step - loss: 0.4444 - acc: 0.8523 - val_loss: 1.5852 - val_acc: 0.6830\n",
      "Epoch 36/200\n",
      "33000/33000 [==============================] - 5s 164us/step - loss: 0.4346 - acc: 0.8534 - val_loss: 1.5930 - val_acc: 0.6793\n",
      "Epoch 37/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.4372 - acc: 0.8536 - val_loss: 1.6232 - val_acc: 0.6827\n",
      "Epoch 38/200\n",
      "33000/33000 [==============================] - 6s 167us/step - loss: 0.4289 - acc: 0.8562 - val_loss: 1.6171 - val_acc: 0.6813\n",
      "Epoch 39/200\n",
      "33000/33000 [==============================] - 6s 171us/step - loss: 0.4277 - acc: 0.8583 - val_loss: 1.6617 - val_acc: 0.6790\n",
      "Epoch 40/200\n",
      "33000/33000 [==============================] - 6s 167us/step - loss: 0.4223 - acc: 0.8595 - val_loss: 1.7060 - val_acc: 0.6777\n",
      "Epoch 41/200\n",
      "33000/33000 [==============================] - 6s 174us/step - loss: 0.4205 - acc: 0.8590 - val_loss: 1.6965 - val_acc: 0.6747\n",
      "Epoch 42/200\n",
      "33000/33000 [==============================] - 6s 172us/step - loss: 0.4116 - acc: 0.8625 - val_loss: 1.7299 - val_acc: 0.6757\n",
      "Epoch 43/200\n",
      "33000/33000 [==============================] - 5s 163us/step - loss: 0.4164 - acc: 0.8601 - val_loss: 1.7600 - val_acc: 0.6740\n",
      "Epoch 44/200\n",
      "33000/33000 [==============================] - 6s 172us/step - loss: 0.4016 - acc: 0.8636 - val_loss: 1.7573 - val_acc: 0.6753\n",
      "Epoch 45/200\n",
      "33000/33000 [==============================] - 6s 172us/step - loss: 0.4007 - acc: 0.8645 - val_loss: 1.7995 - val_acc: 0.6693\n",
      "Epoch 46/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.3992 - acc: 0.8635 - val_loss: 1.8096 - val_acc: 0.6750\n",
      "Epoch 47/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3927 - acc: 0.8689 - val_loss: 1.8399 - val_acc: 0.6763\n",
      "Epoch 48/200\n",
      "33000/33000 [==============================] - 6s 167us/step - loss: 0.4014 - acc: 0.8650 - val_loss: 1.8260 - val_acc: 0.6737\n",
      "Epoch 49/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3929 - acc: 0.8679 - val_loss: 1.8699 - val_acc: 0.6830\n",
      "Epoch 50/200\n",
      "33000/33000 [==============================] - 5s 164us/step - loss: 0.3831 - acc: 0.8703 - val_loss: 1.9096 - val_acc: 0.6797\n",
      "Epoch 51/200\n",
      "33000/33000 [==============================] - 5s 150us/step - loss: 0.3868 - acc: 0.8703 - val_loss: 1.8697 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "33000/33000 [==============================] - 6s 169us/step - loss: 0.3897 - acc: 0.8691 - val_loss: 1.9014 - val_acc: 0.6787\n",
      "Epoch 53/200\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 0.3836 - acc: 0.8714 - val_loss: 1.9320 - val_acc: 0.6807\n",
      "Epoch 54/200\n",
      "33000/33000 [==============================] - 5s 157us/step - loss: 0.3823 - acc: 0.8706 - val_loss: 1.9382 - val_acc: 0.6833\n",
      "Epoch 55/200\n",
      "33000/33000 [==============================] - 5s 145us/step - loss: 0.3789 - acc: 0.8726 - val_loss: 1.9557 - val_acc: 0.6803\n",
      "Epoch 56/200\n",
      "33000/33000 [==============================] - 6s 170us/step - loss: 0.3660 - acc: 0.8753 - val_loss: 1.9748 - val_acc: 0.6810\n",
      "Epoch 57/200\n",
      "33000/33000 [==============================] - 5s 159us/step - loss: 0.3800 - acc: 0.8738 - val_loss: 1.9317 - val_acc: 0.6803\n",
      "Epoch 58/200\n",
      "33000/33000 [==============================] - 6s 168us/step - loss: 0.3743 - acc: 0.8756 - val_loss: 1.9797 - val_acc: 0.6777\n",
      "Epoch 59/200\n",
      "33000/33000 [==============================] - 5s 153us/step - loss: 0.3777 - acc: 0.8735 - val_loss: 1.9658 - val_acc: 0.6777\n",
      "Epoch 60/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.3693 - acc: 0.8756 - val_loss: 2.0021 - val_acc: 0.6737\n",
      "Epoch 61/200\n",
      "33000/33000 [==============================] - 4s 131us/step - loss: 0.3695 - acc: 0.8768 - val_loss: 1.9907 - val_acc: 0.6727\n",
      "Epoch 62/200\n",
      "33000/33000 [==============================] - 4s 131us/step - loss: 0.3607 - acc: 0.8789 - val_loss: 2.0608 - val_acc: 0.6747\n",
      "Epoch 63/200\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.3626 - acc: 0.8779 - val_loss: 2.0517 - val_acc: 0.6757\n",
      "Epoch 64/200\n",
      "33000/33000 [==============================] - 5s 156us/step - loss: 0.3627 - acc: 0.8762 - val_loss: 2.0981 - val_acc: 0.6747\n",
      "Epoch 65/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.3607 - acc: 0.8788 - val_loss: 2.0960 - val_acc: 0.6763\n",
      "Epoch 66/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3620 - acc: 0.8777 - val_loss: 2.1189 - val_acc: 0.6717\n",
      "Epoch 67/200\n",
      "33000/33000 [==============================] - 6s 169us/step - loss: 0.3580 - acc: 0.8781 - val_loss: 2.1242 - val_acc: 0.6727\n",
      "Epoch 68/200\n",
      "33000/33000 [==============================] - 5s 161us/step - loss: 0.3513 - acc: 0.8802 - val_loss: 2.1233 - val_acc: 0.6767\n",
      "Epoch 69/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.3456 - acc: 0.8832 - val_loss: 2.1550 - val_acc: 0.6723\n",
      "Epoch 70/200\n",
      "33000/33000 [==============================] - 6s 171us/step - loss: 0.3550 - acc: 0.8815 - val_loss: 2.1521 - val_acc: 0.6780\n",
      "Epoch 71/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3525 - acc: 0.8820 - val_loss: 2.1660 - val_acc: 0.6737\n",
      "Epoch 72/200\n",
      "33000/33000 [==============================] - 6s 170us/step - loss: 0.3583 - acc: 0.8794 - val_loss: 2.1665 - val_acc: 0.6807\n",
      "Epoch 73/200\n",
      "33000/33000 [==============================] - 6s 175us/step - loss: 0.3567 - acc: 0.8784 - val_loss: 2.1785 - val_acc: 0.6770\n",
      "Epoch 74/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3528 - acc: 0.8820 - val_loss: 2.1683 - val_acc: 0.6810\n",
      "Epoch 75/200\n",
      "33000/33000 [==============================] - 5s 162us/step - loss: 0.3447 - acc: 0.8823 - val_loss: 2.2031 - val_acc: 0.6790\n",
      "Epoch 76/200\n",
      "33000/33000 [==============================] - 5s 165us/step - loss: 0.3451 - acc: 0.8855 - val_loss: 2.1923 - val_acc: 0.6780\n",
      "Epoch 77/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3490 - acc: 0.8807 - val_loss: 2.1482 - val_acc: 0.6797\n",
      "Epoch 78/200\n",
      "33000/33000 [==============================] - 6s 170us/step - loss: 0.3470 - acc: 0.8842 - val_loss: 2.2040 - val_acc: 0.6813\n",
      "Epoch 79/200\n",
      "33000/33000 [==============================] - 5s 154us/step - loss: 0.3459 - acc: 0.8843 - val_loss: 2.2074 - val_acc: 0.6797\n",
      "Epoch 80/200\n",
      "33000/33000 [==============================] - 5s 166us/step - loss: 0.3457 - acc: 0.8823 - val_loss: 2.1970 - val_acc: 0.6833\n",
      "Epoch 81/200\n",
      "33000/33000 [==============================] - 5s 163us/step - loss: 0.3453 - acc: 0.8824 - val_loss: 2.2319 - val_acc: 0.6817\n",
      "Epoch 82/200\n",
      "33000/33000 [==============================] - 6s 171us/step - loss: 0.3332 - acc: 0.8884 - val_loss: 2.2772 - val_acc: 0.6760\n",
      "Epoch 83/200\n",
      "33000/33000 [==============================] - 6s 167us/step - loss: 0.3483 - acc: 0.8811 - val_loss: 2.2765 - val_acc: 0.6790\n",
      "Epoch 84/200\n",
      "33000/33000 [==============================] - 5s 164us/step - loss: 0.3330 - acc: 0.8862 - val_loss: 2.2656 - val_acc: 0.6793\n",
      "Epoch 85/200\n",
      "33000/33000 [==============================] - 6s 176us/step - loss: 0.3473 - acc: 0.8858 - val_loss: 2.2558 - val_acc: 0.6783\n",
      "Epoch 86/200\n",
      "33000/33000 [==============================] - 6s 171us/step - loss: 0.3433 - acc: 0.8849 - val_loss: 2.2797 - val_acc: 0.6793\n",
      "Epoch 87/200\n",
      "33000/33000 [==============================] - 6s 175us/step - loss: 0.3407 - acc: 0.8839 - val_loss: 2.2621 - val_acc: 0.6717\n",
      "Epoch 88/200\n",
      "33000/33000 [==============================] - 6s 171us/step - loss: 0.3356 - acc: 0.8870 - val_loss: 2.3027 - val_acc: 0.6783\n",
      "Epoch 89/200\n",
      "33000/33000 [==============================] - 6s 168us/step - loss: 0.3424 - acc: 0.8875 - val_loss: 2.3114 - val_acc: 0.6770\n",
      "Epoch 90/200\n",
      "33000/33000 [==============================] - 6s 173us/step - loss: 0.3343 - acc: 0.8851 - val_loss: 2.2964 - val_acc: 0.6850\n",
      "Epoch 91/200\n",
      "33000/33000 [==============================] - 6s 177us/step - loss: 0.3362 - acc: 0.8859 - val_loss: 2.3015 - val_acc: 0.6783\n",
      "Epoch 92/200\n",
      "33000/33000 [==============================] - 6s 170us/step - loss: 0.3459 - acc: 0.8838 - val_loss: 2.3114 - val_acc: 0.6733\n",
      "Epoch 93/200\n",
      "19264/33000 [================>.............] - ETA: 2s - loss: 0.3372 - acc: 0.8854"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-f86d37bf87ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     validation_data=(val, label_val))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m                 array_vals.append(\n\u001b[1;32m   2618\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2619\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Drop out method with bigger database\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "drop_model2 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 58us/step\n",
      "4000/4000 [==============================] - 0s 84us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_drop_extended= model.evaluate(train_final, label_train_final)\n",
    "results_test_drop_extended = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results- Drop Model Extended: \n",
      " Loss:0.06753153615309433 \n",
      " Accuracy :0.9854848484848485\n",
      "Test results -Drop Model Extended:\n",
      " Loss:2.246895628929138 \n",
      " Accuracy:0.6865\n"
     ]
    }
   ],
   "source": [
    "print('Training results- Drop Model Extended: \\n Loss:{} \\n Accuracy :{}'.format(results_train_drop_extended[0],results_train_drop_extended[1],) )\n",
    "print('Test results -Drop Model Extended:\\n Loss:{} \\n Accuracy:{}'.format(results_test_drop_extended[0] ,results_test_drop_extended[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 1.7243 - acc: 0.5795 - val_loss: 1.4868 - val_acc: 0.6527\n",
      "Epoch 2/75\n",
      "33000/33000 [==============================] - 10s 312us/step - loss: 1.4645 - acc: 0.6522 - val_loss: 1.4184 - val_acc: 0.6750\n",
      "Epoch 3/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.4086 - acc: 0.6603 - val_loss: 1.3748 - val_acc: 0.6740\n",
      "Epoch 4/75\n",
      "33000/33000 [==============================] - 9s 278us/step - loss: 1.3691 - acc: 0.6688 - val_loss: 1.3448 - val_acc: 0.6807\n",
      "Epoch 5/75\n",
      "33000/33000 [==============================] - 9s 285us/step - loss: 1.3389 - acc: 0.6754 - val_loss: 1.3194 - val_acc: 0.6843\n",
      "Epoch 6/75\n",
      "33000/33000 [==============================] - 9s 281us/step - loss: 1.3062 - acc: 0.6815 - val_loss: 1.3098 - val_acc: 0.6850\n",
      "Epoch 7/75\n",
      "33000/33000 [==============================] - 9s 277us/step - loss: 1.2783 - acc: 0.6855 - val_loss: 1.2936 - val_acc: 0.6820\n",
      "Epoch 8/75\n",
      "33000/33000 [==============================] - 9s 267us/step - loss: 1.2511 - acc: 0.6900 - val_loss: 1.2550 - val_acc: 0.6997\n",
      "Epoch 9/75\n",
      "33000/33000 [==============================] - 9s 269us/step - loss: 1.2268 - acc: 0.6994 - val_loss: 1.2428 - val_acc: 0.6953\n",
      "Epoch 10/75\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 1.2081 - acc: 0.7006 - val_loss: 1.2578 - val_acc: 0.6800\n",
      "Epoch 11/75\n",
      "33000/33000 [==============================] - 9s 284us/step - loss: 1.1905 - acc: 0.7060 - val_loss: 1.2184 - val_acc: 0.7017\n",
      "Epoch 12/75\n",
      "33000/33000 [==============================] - 10s 290us/step - loss: 1.1709 - acc: 0.7102 - val_loss: 1.2126 - val_acc: 0.6987\n",
      "Epoch 13/75\n",
      "33000/33000 [==============================] - 9s 286us/step - loss: 1.1577 - acc: 0.7129 - val_loss: 1.2010 - val_acc: 0.6977\n",
      "Epoch 14/75\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 1.1446 - acc: 0.7175 - val_loss: 1.2124 - val_acc: 0.6933\n",
      "Epoch 15/75\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 1.1348 - acc: 0.7190 - val_loss: 1.1909 - val_acc: 0.7000\n",
      "Epoch 16/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 1.1273 - acc: 0.7217 - val_loss: 1.1882 - val_acc: 0.7080\n",
      "Epoch 17/75\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 1.1186 - acc: 0.7228 - val_loss: 1.2048 - val_acc: 0.6997\n",
      "Epoch 18/75\n",
      "33000/33000 [==============================] - 10s 294us/step - loss: 1.1114 - acc: 0.7241 - val_loss: 1.1663 - val_acc: 0.7057\n",
      "Epoch 19/75\n",
      "33000/33000 [==============================] - 10s 295us/step - loss: 1.1024 - acc: 0.7257 - val_loss: 1.1798 - val_acc: 0.7047\n",
      "Epoch 20/75\n",
      "33000/33000 [==============================] - 9s 275us/step - loss: 1.0980 - acc: 0.7295 - val_loss: 1.1924 - val_acc: 0.6970\n",
      "Epoch 21/75\n",
      "33000/33000 [==============================] - 10s 296us/step - loss: 1.0907 - acc: 0.7298 - val_loss: 1.1711 - val_acc: 0.7110\n",
      "Epoch 22/75\n",
      "33000/33000 [==============================] - 10s 295us/step - loss: 1.0849 - acc: 0.7339 - val_loss: 1.1834 - val_acc: 0.7013\n",
      "Epoch 23/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0781 - acc: 0.7351 - val_loss: 1.1778 - val_acc: 0.6993\n",
      "Epoch 24/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0745 - acc: 0.7348 - val_loss: 1.1769 - val_acc: 0.7063\n",
      "Epoch 25/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0701 - acc: 0.7384 - val_loss: 1.1915 - val_acc: 0.7023\n",
      "Epoch 26/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.0657 - acc: 0.7395 - val_loss: 1.1888 - val_acc: 0.6943\n",
      "Epoch 27/75\n",
      "33000/33000 [==============================] - 10s 291us/step - loss: 1.0628 - acc: 0.7358 - val_loss: 1.1695 - val_acc: 0.7020\n",
      "Epoch 28/75\n",
      "33000/33000 [==============================] - 10s 296us/step - loss: 1.0577 - acc: 0.7417 - val_loss: 1.1669 - val_acc: 0.7120\n",
      "Epoch 29/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.0535 - acc: 0.7436 - val_loss: 1.1865 - val_acc: 0.6947\n",
      "Epoch 30/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0536 - acc: 0.7412 - val_loss: 1.1842 - val_acc: 0.7023\n",
      "Epoch 31/75\n",
      "33000/33000 [==============================] - 10s 304us/step - loss: 1.0487 - acc: 0.7436 - val_loss: 1.1926 - val_acc: 0.6960\n",
      "Epoch 32/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 1.0490 - acc: 0.7447 - val_loss: 1.1810 - val_acc: 0.7000\n",
      "Epoch 33/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0424 - acc: 0.7445 - val_loss: 1.1701 - val_acc: 0.7030\n",
      "Epoch 34/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 1.0423 - acc: 0.7454 - val_loss: 1.1826 - val_acc: 0.6983\n",
      "Epoch 35/75\n",
      "33000/33000 [==============================] - 10s 312us/step - loss: 1.0418 - acc: 0.7462 - val_loss: 1.1785 - val_acc: 0.7030\n",
      "Epoch 36/75\n",
      "33000/33000 [==============================] - 10s 301us/step - loss: 1.0381 - acc: 0.7468 - val_loss: 1.1890 - val_acc: 0.6947\n",
      "Epoch 37/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.0374 - acc: 0.7454 - val_loss: 1.1787 - val_acc: 0.7007\n",
      "Epoch 38/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 1.0333 - acc: 0.7518 - val_loss: 1.1997 - val_acc: 0.6930\n",
      "Epoch 39/75\n",
      "33000/33000 [==============================] - 9s 276us/step - loss: 1.0319 - acc: 0.7498 - val_loss: 1.2063 - val_acc: 0.7017\n",
      "Epoch 40/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.0292 - acc: 0.7484 - val_loss: 1.1954 - val_acc: 0.6977\n",
      "Epoch 41/75\n",
      "33000/33000 [==============================] - 10s 303us/step - loss: 1.0279 - acc: 0.7507 - val_loss: 1.2157 - val_acc: 0.6930\n",
      "Epoch 42/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.0273 - acc: 0.7510 - val_loss: 1.1800 - val_acc: 0.7063\n",
      "Epoch 43/75\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 1.0257 - acc: 0.7508 - val_loss: 1.1639 - val_acc: 0.7083\n",
      "Epoch 44/75\n",
      "33000/33000 [==============================] - 10s 305us/step - loss: 1.0220 - acc: 0.7532 - val_loss: 1.1817 - val_acc: 0.7103\n",
      "Epoch 45/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.0254 - acc: 0.7525 - val_loss: 1.1936 - val_acc: 0.6970\n",
      "Epoch 46/75\n",
      "33000/33000 [==============================] - 10s 306us/step - loss: 1.0209 - acc: 0.7533 - val_loss: 1.2019 - val_acc: 0.6997\n",
      "Epoch 47/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.0197 - acc: 0.7562 - val_loss: 1.2010 - val_acc: 0.6910\n",
      "Epoch 48/75\n",
      "33000/33000 [==============================] - 10s 317us/step - loss: 1.0211 - acc: 0.7533 - val_loss: 1.2037 - val_acc: 0.7030\n",
      "Epoch 49/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 1.0163 - acc: 0.7559 - val_loss: 1.1785 - val_acc: 0.7003\n",
      "Epoch 50/75\n",
      "33000/33000 [==============================] - 11s 345us/step - loss: 1.0153 - acc: 0.7557 - val_loss: 1.1946 - val_acc: 0.7013\n",
      "Epoch 51/75\n",
      "33000/33000 [==============================] - 9s 286us/step - loss: 1.0150 - acc: 0.7558 - val_loss: 1.1841 - val_acc: 0.7090\n",
      "Epoch 52/75\n",
      "33000/33000 [==============================] - 10s 298us/step - loss: 1.0135 - acc: 0.7570 - val_loss: 1.1966 - val_acc: 0.6973\n",
      "Epoch 53/75\n",
      "33000/33000 [==============================] - 9s 274us/step - loss: 1.0119 - acc: 0.7557 - val_loss: 1.1986 - val_acc: 0.6953\n",
      "Epoch 54/75\n",
      "33000/33000 [==============================] - 9s 263us/step - loss: 1.0106 - acc: 0.7576 - val_loss: 1.2018 - val_acc: 0.6990\n",
      "Epoch 55/75\n",
      "33000/33000 [==============================] - 8s 257us/step - loss: 1.0105 - acc: 0.7559 - val_loss: 1.2489 - val_acc: 0.6857\n",
      "Epoch 56/75\n",
      "33000/33000 [==============================] - 9s 274us/step - loss: 1.0085 - acc: 0.7590 - val_loss: 1.1980 - val_acc: 0.6970\n",
      "Epoch 57/75\n",
      "33000/33000 [==============================] - 11s 333us/step - loss: 1.0065 - acc: 0.7583 - val_loss: 1.2218 - val_acc: 0.6973\n",
      "Epoch 58/75\n",
      "33000/33000 [==============================] - 11s 331us/step - loss: 1.0069 - acc: 0.7594 - val_loss: 1.1918 - val_acc: 0.7010\n",
      "Epoch 59/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.0059 - acc: 0.7589 - val_loss: 1.2443 - val_acc: 0.6873\n",
      "Epoch 60/75\n",
      "33000/33000 [==============================] - 11s 323us/step - loss: 1.0024 - acc: 0.7607 - val_loss: 1.2104 - val_acc: 0.6930\n",
      "Epoch 61/75\n",
      "33000/33000 [==============================] - 10s 316us/step - loss: 0.9999 - acc: 0.7631 - val_loss: 1.2198 - val_acc: 0.6983\n",
      "Epoch 62/75\n",
      "33000/33000 [==============================] - 10s 315us/step - loss: 1.0052 - acc: 0.7602 - val_loss: 1.2115 - val_acc: 0.7060\n",
      "Epoch 63/75\n",
      "33000/33000 [==============================] - 11s 320us/step - loss: 1.0017 - acc: 0.7607 - val_loss: 1.2113 - val_acc: 0.6993\n",
      "Epoch 64/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 0.9974 - acc: 0.7632 - val_loss: 1.2116 - val_acc: 0.6967\n",
      "Epoch 65/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 0.9967 - acc: 0.7635 - val_loss: 1.2023 - val_acc: 0.6993\n",
      "Epoch 66/75\n",
      "33000/33000 [==============================] - 10s 303us/step - loss: 0.9966 - acc: 0.7624 - val_loss: 1.2163 - val_acc: 0.6913\n",
      "Epoch 67/75\n",
      "33000/33000 [==============================] - 10s 294us/step - loss: 0.9971 - acc: 0.7613 - val_loss: 1.2229 - val_acc: 0.6993\n",
      "Epoch 68/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 0.9927 - acc: 0.7639 - val_loss: 1.2305 - val_acc: 0.7007\n",
      "Epoch 69/75\n",
      "33000/33000 [==============================] - 9s 277us/step - loss: 0.9944 - acc: 0.7647 - val_loss: 1.2242 - val_acc: 0.6970\n",
      "Epoch 70/75\n",
      "33000/33000 [==============================] - 9s 275us/step - loss: 0.9929 - acc: 0.7620 - val_loss: 1.2121 - val_acc: 0.6910\n",
      "Epoch 71/75\n",
      "33000/33000 [==============================] - 10s 297us/step - loss: 0.9901 - acc: 0.7642 - val_loss: 1.2042 - val_acc: 0.7013\n",
      "Epoch 72/75\n",
      "33000/33000 [==============================] - 10s 301us/step - loss: 0.9897 - acc: 0.7661 - val_loss: 1.2354 - val_acc: 0.6817\n",
      "Epoch 73/75\n",
      "33000/33000 [==============================] - 10s 294us/step - loss: 0.9880 - acc: 0.7662 - val_loss: 1.2321 - val_acc: 0.6950\n",
      "Epoch 74/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 0.9922 - acc: 0.7654 - val_loss: 1.2074 - val_acc: 0.6997\n",
      "Epoch 75/75\n",
      "33000/33000 [==============================] - 10s 304us/step - loss: 0.9897 - acc: 0.7680 - val_loss: 1.2221 - val_acc: 0.6990\n"
     ]
    }
   ],
   "source": [
    "#Drop out method with bigger database\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(19,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 3s 92us/step\n",
      "4000/4000 [==============================] - 1s 141us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_l2_extended= model.evaluate(train_final, label_train_final)\n",
    "results_test_l2_extended = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results- Drop Model Extended: \n",
      " Loss:0.8451337846264695 \n",
      " Accuracy :0.823060606060606\n",
      "Test results -Drop Model Extended:\n",
      " Loss:1.2341540246009826 \n",
      " Accuracy:0.68975\n"
     ]
    }
   ],
   "source": [
    "print('Training results- Drop Model Extended: \\n Loss:{} \\n Accuracy :{}'.format(results_train_l2_extended[0],results_train_l2_extended[1])) \n",
    "print('Test results -Drop Model Extended:\\n Loss:{} \\n Accuracy:{}'.format(results_test_l2_extended[0] ,results_test_l2_extended[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/75\n",
      "33000/33000 [==============================] - 7s 207us/step - loss: 13.1016 - acc: 0.2629 - val_loss: 3.1957 - val_acc: 0.2793\n",
      "Epoch 2/75\n",
      "33000/33000 [==============================] - 7s 199us/step - loss: 2.9729 - acc: 0.3252 - val_loss: 2.6824 - val_acc: 0.3850\n",
      "Epoch 3/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 2.5253 - acc: 0.4042 - val_loss: 2.2847 - val_acc: 0.4483\n",
      "Epoch 4/75\n",
      "33000/33000 [==============================] - 7s 204us/step - loss: 2.2044 - acc: 0.4336 - val_loss: 2.0385 - val_acc: 0.4717\n",
      "Epoch 5/75\n",
      "33000/33000 [==============================] - 7s 205us/step - loss: 2.0364 - acc: 0.4505 - val_loss: 1.9353 - val_acc: 0.4917\n",
      "Epoch 6/75\n",
      "33000/33000 [==============================] - 7s 204us/step - loss: 1.9601 - acc: 0.4626 - val_loss: 1.8768 - val_acc: 0.4977\n",
      "Epoch 7/75\n",
      "33000/33000 [==============================] - 7s 226us/step - loss: 1.9119 - acc: 0.4699 - val_loss: 1.8472 - val_acc: 0.5013\n",
      "Epoch 8/75\n",
      "33000/33000 [==============================] - 8s 246us/step - loss: 1.8750 - acc: 0.4762 - val_loss: 1.8059 - val_acc: 0.5110\n",
      "Epoch 9/75\n",
      "33000/33000 [==============================] - 7s 208us/step - loss: 1.8456 - acc: 0.4864 - val_loss: 1.7819 - val_acc: 0.5253\n",
      "Epoch 10/75\n",
      "33000/33000 [==============================] - 7s 206us/step - loss: 1.8207 - acc: 0.4952 - val_loss: 1.7573 - val_acc: 0.5223\n",
      "Epoch 11/75\n",
      "33000/33000 [==============================] - 7s 201us/step - loss: 1.8002 - acc: 0.4988 - val_loss: 1.7484 - val_acc: 0.5217\n",
      "Epoch 12/75\n",
      "33000/33000 [==============================] - 7s 205us/step - loss: 1.7828 - acc: 0.5033 - val_loss: 1.7277 - val_acc: 0.5377\n",
      "Epoch 13/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.7686 - acc: 0.5117 - val_loss: 1.7258 - val_acc: 0.5333\n",
      "Epoch 14/75\n",
      "33000/33000 [==============================] - 7s 208us/step - loss: 1.7545 - acc: 0.5168 - val_loss: 1.7356 - val_acc: 0.5317\n",
      "Epoch 15/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.7425 - acc: 0.5223 - val_loss: 1.7967 - val_acc: 0.5127\n",
      "Epoch 16/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 1.7307 - acc: 0.5258 - val_loss: 1.7254 - val_acc: 0.5410\n",
      "Epoch 17/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.7196 - acc: 0.5309 - val_loss: 1.6781 - val_acc: 0.5510\n",
      "Epoch 18/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 1.7084 - acc: 0.5379 - val_loss: 1.6927 - val_acc: 0.5450\n",
      "Epoch 19/75\n",
      "33000/33000 [==============================] - 7s 211us/step - loss: 1.6983 - acc: 0.5485 - val_loss: 1.6603 - val_acc: 0.5757\n",
      "Epoch 20/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 1.6870 - acc: 0.5531 - val_loss: 1.6344 - val_acc: 0.5860\n",
      "Epoch 21/75\n",
      "33000/33000 [==============================] - 7s 211us/step - loss: 1.6805 - acc: 0.5570 - val_loss: 1.6353 - val_acc: 0.5930\n",
      "Epoch 22/75\n",
      "33000/33000 [==============================] - 7s 225us/step - loss: 1.6720 - acc: 0.5606 - val_loss: 1.6259 - val_acc: 0.5837\n",
      "Epoch 23/75\n",
      "33000/33000 [==============================] - 7s 212us/step - loss: 1.6660 - acc: 0.5622 - val_loss: 1.6094 - val_acc: 0.5997\n",
      "Epoch 24/75\n",
      "33000/33000 [==============================] - 7s 208us/step - loss: 1.6583 - acc: 0.5654 - val_loss: 1.6240 - val_acc: 0.5917\n",
      "Epoch 25/75\n",
      "33000/33000 [==============================] - 6s 179us/step - loss: 1.6526 - acc: 0.5667 - val_loss: 1.6054 - val_acc: 0.6000\n",
      "Epoch 26/75\n",
      "33000/33000 [==============================] - 6s 193us/step - loss: 1.6465 - acc: 0.5672 - val_loss: 1.6231 - val_acc: 0.5883\n",
      "Epoch 27/75\n",
      "33000/33000 [==============================] - 7s 217us/step - loss: 1.6419 - acc: 0.5705 - val_loss: 1.6140 - val_acc: 0.5970\n",
      "Epoch 28/75\n",
      "33000/33000 [==============================] - 7s 213us/step - loss: 1.6369 - acc: 0.5700 - val_loss: 1.5956 - val_acc: 0.6030\n",
      "Epoch 29/75\n",
      "33000/33000 [==============================] - 7s 206us/step - loss: 1.6332 - acc: 0.5733 - val_loss: 1.6081 - val_acc: 0.6047\n",
      "Epoch 30/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.6310 - acc: 0.5722 - val_loss: 1.5998 - val_acc: 0.5950\n",
      "Epoch 31/75\n",
      "33000/33000 [==============================] - 7s 207us/step - loss: 1.6263 - acc: 0.5728 - val_loss: 1.5884 - val_acc: 0.5977\n",
      "Epoch 32/75\n",
      "33000/33000 [==============================] - 6s 175us/step - loss: 1.6227 - acc: 0.5762 - val_loss: 1.6328 - val_acc: 0.5800\n",
      "Epoch 33/75\n",
      "33000/33000 [==============================] - 7s 204us/step - loss: 1.6203 - acc: 0.5738 - val_loss: 1.6093 - val_acc: 0.5850\n",
      "Epoch 34/75\n",
      "33000/33000 [==============================] - 7s 212us/step - loss: 1.6170 - acc: 0.5767 - val_loss: 1.5866 - val_acc: 0.5963\n",
      "Epoch 35/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 1.6139 - acc: 0.5766 - val_loss: 1.5907 - val_acc: 0.5960\n",
      "Epoch 36/75\n",
      "33000/33000 [==============================] - 7s 210us/step - loss: 1.6110 - acc: 0.5772 - val_loss: 1.5611 - val_acc: 0.6097\n",
      "Epoch 37/75\n",
      "33000/33000 [==============================] - 7s 213us/step - loss: 1.6110 - acc: 0.5778 - val_loss: 1.5732 - val_acc: 0.6090\n",
      "Epoch 38/75\n",
      "33000/33000 [==============================] - 7s 214us/step - loss: 1.6087 - acc: 0.5772 - val_loss: 1.5675 - val_acc: 0.6043\n",
      "Epoch 39/75\n",
      "33000/33000 [==============================] - 7s 221us/step - loss: 1.6052 - acc: 0.5798 - val_loss: 1.5829 - val_acc: 0.6000\n",
      "Epoch 40/75\n",
      "33000/33000 [==============================] - 7s 219us/step - loss: 1.6027 - acc: 0.5799 - val_loss: 1.5686 - val_acc: 0.6023\n",
      "Epoch 41/75\n",
      "33000/33000 [==============================] - 7s 211us/step - loss: 1.6039 - acc: 0.5795 - val_loss: 1.5884 - val_acc: 0.5940\n",
      "Epoch 42/75\n",
      "33000/33000 [==============================] - 7s 215us/step - loss: 1.6001 - acc: 0.5802 - val_loss: 1.5981 - val_acc: 0.5990\n",
      "Epoch 43/75\n",
      "33000/33000 [==============================] - 7s 211us/step - loss: 1.5975 - acc: 0.5824 - val_loss: 1.5619 - val_acc: 0.6020\n",
      "Epoch 44/75\n",
      "33000/33000 [==============================] - 7s 215us/step - loss: 1.5988 - acc: 0.5801 - val_loss: 1.5956 - val_acc: 0.6017\n",
      "Epoch 45/75\n",
      "33000/33000 [==============================] - 7s 216us/step - loss: 1.5947 - acc: 0.5810 - val_loss: 1.5512 - val_acc: 0.6073\n",
      "Epoch 46/75\n",
      "33000/33000 [==============================] - 7s 217us/step - loss: 1.5929 - acc: 0.5795 - val_loss: 1.5442 - val_acc: 0.6167\n",
      "Epoch 47/75\n",
      "33000/33000 [==============================] - 7s 216us/step - loss: 1.5908 - acc: 0.5824 - val_loss: 1.5645 - val_acc: 0.6080\n",
      "Epoch 48/75\n",
      "33000/33000 [==============================] - 7s 220us/step - loss: 1.5911 - acc: 0.5839 - val_loss: 1.5773 - val_acc: 0.5990\n",
      "Epoch 49/75\n",
      "33000/33000 [==============================] - 7s 215us/step - loss: 1.5877 - acc: 0.5838 - val_loss: 1.6750 - val_acc: 0.5700\n",
      "Epoch 50/75\n",
      "33000/33000 [==============================] - 8s 228us/step - loss: 1.5860 - acc: 0.5855 - val_loss: 1.7674 - val_acc: 0.5160\n",
      "Epoch 51/75\n",
      "33000/33000 [==============================] - 7s 218us/step - loss: 1.5864 - acc: 0.5884 - val_loss: 1.5397 - val_acc: 0.6093\n",
      "Epoch 52/75\n",
      "33000/33000 [==============================] - 7s 216us/step - loss: 1.5851 - acc: 0.5868 - val_loss: 1.5360 - val_acc: 0.6080\n",
      "Epoch 53/75\n",
      "33000/33000 [==============================] - 7s 213us/step - loss: 1.5826 - acc: 0.5884 - val_loss: 1.5470 - val_acc: 0.6120\n",
      "Epoch 54/75\n",
      "33000/33000 [==============================] - 7s 215us/step - loss: 1.5791 - acc: 0.5915 - val_loss: 1.6604 - val_acc: 0.5533\n",
      "Epoch 55/75\n",
      "33000/33000 [==============================] - 7s 217us/step - loss: 1.5764 - acc: 0.5924 - val_loss: 1.5800 - val_acc: 0.5923\n",
      "Epoch 56/75\n",
      "33000/33000 [==============================] - 8s 229us/step - loss: 1.5712 - acc: 0.5987 - val_loss: 1.6607 - val_acc: 0.5883\n",
      "Epoch 57/75\n",
      "33000/33000 [==============================] - 7s 219us/step - loss: 1.5687 - acc: 0.6002 - val_loss: 1.5991 - val_acc: 0.6037\n",
      "Epoch 58/75\n",
      "33000/33000 [==============================] - 7s 215us/step - loss: 1.5624 - acc: 0.6048 - val_loss: 1.5876 - val_acc: 0.6123\n",
      "Epoch 59/75\n",
      "33000/33000 [==============================] - 7s 205us/step - loss: 1.5590 - acc: 0.6080 - val_loss: 1.5106 - val_acc: 0.6297\n",
      "Epoch 60/75\n",
      "33000/33000 [==============================] - 7s 206us/step - loss: 1.5535 - acc: 0.6103 - val_loss: 1.6233 - val_acc: 0.5980\n",
      "Epoch 61/75\n",
      "33000/33000 [==============================] - 7s 207us/step - loss: 1.5474 - acc: 0.6112 - val_loss: 1.6240 - val_acc: 0.5963\n",
      "Epoch 62/75\n",
      "33000/33000 [==============================] - 7s 207us/step - loss: 1.5432 - acc: 0.6143 - val_loss: 1.5885 - val_acc: 0.5990\n",
      "Epoch 63/75\n",
      "33000/33000 [==============================] - 7s 213us/step - loss: 1.5381 - acc: 0.6171 - val_loss: 1.5203 - val_acc: 0.6240\n",
      "Epoch 64/75\n",
      "33000/33000 [==============================] - 7s 208us/step - loss: 1.5333 - acc: 0.6190 - val_loss: 1.5332 - val_acc: 0.6203\n",
      "Epoch 65/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5322 - acc: 0.6197 - val_loss: 1.5839 - val_acc: 0.6110\n",
      "Epoch 66/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5287 - acc: 0.6194 - val_loss: 1.5334 - val_acc: 0.6340\n",
      "Epoch 67/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5284 - acc: 0.6210 - val_loss: 1.4988 - val_acc: 0.6390\n",
      "Epoch 68/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5253 - acc: 0.6206 - val_loss: 1.5189 - val_acc: 0.6277\n",
      "Epoch 69/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5251 - acc: 0.6212 - val_loss: 1.5159 - val_acc: 0.6217\n",
      "Epoch 70/75\n",
      "33000/33000 [==============================] - 7s 206us/step - loss: 1.5254 - acc: 0.6221 - val_loss: 1.4944 - val_acc: 0.6390\n",
      "Epoch 71/75\n",
      "33000/33000 [==============================] - 7s 208us/step - loss: 1.5224 - acc: 0.6226 - val_loss: 1.4772 - val_acc: 0.6450\n",
      "Epoch 72/75\n",
      "33000/33000 [==============================] - 7s 209us/step - loss: 1.5216 - acc: 0.6233 - val_loss: 1.5012 - val_acc: 0.6367\n",
      "Epoch 73/75\n",
      "33000/33000 [==============================] - 7s 218us/step - loss: 1.5187 - acc: 0.6240 - val_loss: 1.5351 - val_acc: 0.6237\n",
      "Epoch 74/75\n",
      "33000/33000 [==============================] - 7s 216us/step - loss: 1.5182 - acc: 0.6247 - val_loss: 1.5176 - val_acc: 0.6283\n",
      "Epoch 75/75\n",
      "33000/33000 [==============================] - 7s 199us/step - loss: 1.5176 - acc: 0.6254 - val_loss: 1.4913 - val_acc: 0.6343\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model=Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l1(0.005),input_shape=(5000,)))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 4s 122us/step\n",
      "4000/4000 [==============================] - 0s 92us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_l1_extended= model.evaluate(train_final, label_train_final)\n",
    "results_test_l1_extended = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results- Drop Model Extended: \n",
      " Loss:1.5055514525211218 \n",
      " Accuracy :0.6286060606060606\n",
      "Test results -Drop Model Extended:\n",
      " Loss:1.5350969915390014 \n",
      " Accuracy:0.62475\n"
     ]
    }
   ],
   "source": [
    "print('Training results- Drop Model Extended: \\n Loss:{} \\n Accuracy :{}'.format(results_train_l1_extended[0],results_train_l1_extended[1])) \n",
    "print('Test results -Drop Model Extended:\\n Loss:{} \\n Accuracy:{}'.format(results_test_l1_extended[0] ,results_test_l1_extended[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 2.0270 - acc: 0.4724 - val_loss: 1.5934 - val_acc: 0.6023\n",
      "Epoch 2/75\n",
      "33000/33000 [==============================] - 10s 293us/step - loss: 1.7393 - acc: 0.5528 - val_loss: 1.5469 - val_acc: 0.6367\n",
      "Epoch 3/75\n",
      "33000/33000 [==============================] - 10s 294us/step - loss: 1.6991 - acc: 0.5722 - val_loss: 1.5197 - val_acc: 0.6370\n",
      "Epoch 4/75\n",
      "33000/33000 [==============================] - 9s 264us/step - loss: 1.6780 - acc: 0.5821 - val_loss: 1.4893 - val_acc: 0.6403\n",
      "Epoch 5/75\n",
      "33000/33000 [==============================] - 9s 272us/step - loss: 1.6576 - acc: 0.5878 - val_loss: 1.4809 - val_acc: 0.6517\n",
      "Epoch 6/75\n",
      "33000/33000 [==============================] - 10s 300us/step - loss: 1.6510 - acc: 0.5932 - val_loss: 1.4809 - val_acc: 0.6463\n",
      "Epoch 7/75\n",
      "33000/33000 [==============================] - 9s 271us/step - loss: 1.6371 - acc: 0.5953 - val_loss: 1.4679 - val_acc: 0.6527\n",
      "Epoch 8/75\n",
      "33000/33000 [==============================] - 9s 263us/step - loss: 1.6319 - acc: 0.5984 - val_loss: 1.4727 - val_acc: 0.6573\n",
      "Epoch 9/75\n",
      "33000/33000 [==============================] - 11s 320us/step - loss: 1.6239 - acc: 0.6015 - val_loss: 1.4665 - val_acc: 0.6570\n",
      "Epoch 10/75\n",
      "33000/33000 [==============================] - 10s 292us/step - loss: 1.6221 - acc: 0.6038 - val_loss: 1.4627 - val_acc: 0.6553\n",
      "Epoch 11/75\n",
      "33000/33000 [==============================] - 10s 289us/step - loss: 1.6167 - acc: 0.6070 - val_loss: 1.4580 - val_acc: 0.6557\n",
      "Epoch 12/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.6108 - acc: 0.6101 - val_loss: 1.4297 - val_acc: 0.6663\n",
      "Epoch 13/75\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 1.6132 - acc: 0.6102 - val_loss: 1.4575 - val_acc: 0.6683\n",
      "Epoch 14/75\n",
      "33000/33000 [==============================] - 10s 304us/step - loss: 1.6063 - acc: 0.6111 - val_loss: 1.4523 - val_acc: 0.6533\n",
      "Epoch 15/75\n",
      "33000/33000 [==============================] - 9s 284us/step - loss: 1.6024 - acc: 0.6143 - val_loss: 1.4363 - val_acc: 0.6687\n",
      "Epoch 16/75\n",
      "33000/33000 [==============================] - 10s 295us/step - loss: 1.6066 - acc: 0.6141 - val_loss: 1.4464 - val_acc: 0.6663\n",
      "Epoch 17/75\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 1.6002 - acc: 0.6176 - val_loss: 1.4549 - val_acc: 0.6760\n",
      "Epoch 18/75\n",
      "33000/33000 [==============================] - 10s 305us/step - loss: 1.6025 - acc: 0.6188 - val_loss: 1.4596 - val_acc: 0.6623\n",
      "Epoch 19/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 1.5993 - acc: 0.6211 - val_loss: 1.4309 - val_acc: 0.6697\n",
      "Epoch 20/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.6049 - acc: 0.6218 - val_loss: 1.4379 - val_acc: 0.6747\n",
      "Epoch 21/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.6008 - acc: 0.6231 - val_loss: 1.4462 - val_acc: 0.6790\n",
      "Epoch 22/75\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 1.5977 - acc: 0.6236 - val_loss: 1.4390 - val_acc: 0.6847\n",
      "Epoch 23/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.5920 - acc: 0.6262 - val_loss: 1.4365 - val_acc: 0.6730\n",
      "Epoch 24/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 1.5952 - acc: 0.6232 - val_loss: 1.4372 - val_acc: 0.6773\n",
      "Epoch 25/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.5894 - acc: 0.6263 - val_loss: 1.4351 - val_acc: 0.6763\n",
      "Epoch 26/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.5927 - acc: 0.6287 - val_loss: 1.4327 - val_acc: 0.6843\n",
      "Epoch 27/75\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 1.5970 - acc: 0.6242 - val_loss: 1.4314 - val_acc: 0.6800\n",
      "Epoch 28/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5917 - acc: 0.6269 - val_loss: 1.4312 - val_acc: 0.6810\n",
      "Epoch 29/75\n",
      "33000/33000 [==============================] - 11s 327us/step - loss: 1.5937 - acc: 0.6282 - val_loss: 1.4316 - val_acc: 0.6877\n",
      "Epoch 30/75\n",
      "33000/33000 [==============================] - 10s 318us/step - loss: 1.5871 - acc: 0.6293 - val_loss: 1.4316 - val_acc: 0.6860\n",
      "Epoch 31/75\n",
      "33000/33000 [==============================] - 11s 321us/step - loss: 1.5980 - acc: 0.6276 - val_loss: 1.4591 - val_acc: 0.6787\n",
      "Epoch 32/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.5907 - acc: 0.6300 - val_loss: 1.4338 - val_acc: 0.6797\n",
      "Epoch 33/75\n",
      "33000/33000 [==============================] - 10s 312us/step - loss: 1.5993 - acc: 0.6266 - val_loss: 1.4377 - val_acc: 0.6770\n",
      "Epoch 34/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.5887 - acc: 0.6323 - val_loss: 1.4455 - val_acc: 0.6753\n",
      "Epoch 35/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.5913 - acc: 0.6297 - val_loss: 1.4547 - val_acc: 0.6773\n",
      "Epoch 36/75\n",
      "33000/33000 [==============================] - 10s 316us/step - loss: 1.5851 - acc: 0.6332 - val_loss: 1.4506 - val_acc: 0.6670\n",
      "Epoch 37/75\n",
      "33000/33000 [==============================] - 10s 312us/step - loss: 1.5945 - acc: 0.6275 - val_loss: 1.4466 - val_acc: 0.6763\n",
      "Epoch 38/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5918 - acc: 0.6298 - val_loss: 1.4333 - val_acc: 0.6817\n",
      "Epoch 39/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5971 - acc: 0.6284 - val_loss: 1.4350 - val_acc: 0.6857\n",
      "Epoch 40/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5890 - acc: 0.6312 - val_loss: 1.4631 - val_acc: 0.6733\n",
      "Epoch 41/75\n",
      "33000/33000 [==============================] - 11s 330us/step - loss: 1.5994 - acc: 0.6303 - val_loss: 1.4404 - val_acc: 0.6767\n",
      "Epoch 42/75\n",
      "33000/33000 [==============================] - 11s 320us/step - loss: 1.5904 - acc: 0.6347 - val_loss: 1.4472 - val_acc: 0.6893\n",
      "Epoch 43/75\n",
      "33000/33000 [==============================] - 10s 288us/step - loss: 1.5886 - acc: 0.6317 - val_loss: 1.4441 - val_acc: 0.6823\n",
      "Epoch 44/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 1.5902 - acc: 0.6347 - val_loss: 1.4239 - val_acc: 0.6853\n",
      "Epoch 45/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.5896 - acc: 0.6360 - val_loss: 1.4462 - val_acc: 0.6840\n",
      "Epoch 46/75\n",
      "33000/33000 [==============================] - 10s 301us/step - loss: 1.5941 - acc: 0.6296 - val_loss: 1.4377 - val_acc: 0.6860\n",
      "Epoch 47/75\n",
      "33000/33000 [==============================] - 10s 317us/step - loss: 1.5978 - acc: 0.6304 - val_loss: 1.4496 - val_acc: 0.6803\n",
      "Epoch 48/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.5889 - acc: 0.6335 - val_loss: 1.4659 - val_acc: 0.6813\n",
      "Epoch 49/75\n",
      "33000/33000 [==============================] - 9s 273us/step - loss: 1.5996 - acc: 0.6314 - val_loss: 1.4403 - val_acc: 0.6800\n",
      "Epoch 50/75\n",
      "33000/33000 [==============================] - 10s 299us/step - loss: 1.5854 - acc: 0.6345 - val_loss: 1.4233 - val_acc: 0.6867\n",
      "Epoch 51/75\n",
      "33000/33000 [==============================] - 11s 322us/step - loss: 1.5901 - acc: 0.6335 - val_loss: 1.4247 - val_acc: 0.6883\n",
      "Epoch 52/75\n",
      "33000/33000 [==============================] - 11s 322us/step - loss: 1.5917 - acc: 0.6367 - val_loss: 1.4386 - val_acc: 0.6847\n",
      "Epoch 53/75\n",
      "33000/33000 [==============================] - 11s 336us/step - loss: 1.5969 - acc: 0.6331 - val_loss: 1.4469 - val_acc: 0.6817\n",
      "Epoch 54/75\n",
      "33000/33000 [==============================] - 10s 316us/step - loss: 1.5902 - acc: 0.6365 - val_loss: 1.4439 - val_acc: 0.6840\n",
      "Epoch 55/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.5905 - acc: 0.6360 - val_loss: 1.4389 - val_acc: 0.6790\n",
      "Epoch 56/75\n",
      "33000/33000 [==============================] - 11s 322us/step - loss: 1.5961 - acc: 0.6312 - val_loss: 1.4370 - val_acc: 0.6833\n",
      "Epoch 57/75\n",
      "33000/33000 [==============================] - 11s 327us/step - loss: 1.5875 - acc: 0.6347 - val_loss: 1.4401 - val_acc: 0.6760\n",
      "Epoch 58/75\n",
      "33000/33000 [==============================] - 11s 324us/step - loss: 1.5918 - acc: 0.6347 - val_loss: 1.4375 - val_acc: 0.6810\n",
      "Epoch 59/75\n",
      "33000/33000 [==============================] - 10s 291us/step - loss: 1.5872 - acc: 0.6347 - val_loss: 1.4376 - val_acc: 0.6887\n",
      "Epoch 60/75\n",
      "33000/33000 [==============================] - 10s 301us/step - loss: 1.5926 - acc: 0.6353 - val_loss: 1.4340 - val_acc: 0.6820\n",
      "Epoch 61/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.5913 - acc: 0.6319 - val_loss: 1.4464 - val_acc: 0.6693\n",
      "Epoch 62/75\n",
      "33000/33000 [==============================] - 10s 304us/step - loss: 1.5928 - acc: 0.6350 - val_loss: 1.4469 - val_acc: 0.6860\n",
      "Epoch 63/75\n",
      "33000/33000 [==============================] - 10s 305us/step - loss: 1.5938 - acc: 0.6325 - val_loss: 1.4251 - val_acc: 0.6827\n",
      "Epoch 64/75\n",
      "33000/33000 [==============================] - 11s 326us/step - loss: 1.5895 - acc: 0.6326 - val_loss: 1.4268 - val_acc: 0.6893\n",
      "Epoch 65/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5953 - acc: 0.6314 - val_loss: 1.4358 - val_acc: 0.6890\n",
      "Epoch 66/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.5953 - acc: 0.6333 - val_loss: 1.4363 - val_acc: 0.6870\n",
      "Epoch 67/75\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 1.5932 - acc: 0.6332 - val_loss: 1.4385 - val_acc: 0.6843\n",
      "Epoch 68/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.5909 - acc: 0.6355 - val_loss: 1.4414 - val_acc: 0.6907\n",
      "Epoch 69/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 1.5948 - acc: 0.6342 - val_loss: 1.4384 - val_acc: 0.6830\n",
      "Epoch 70/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.5915 - acc: 0.6361 - val_loss: 1.4401 - val_acc: 0.6833\n",
      "Epoch 71/75\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 1.5951 - acc: 0.6344 - val_loss: 1.4475 - val_acc: 0.6823\n",
      "Epoch 72/75\n",
      "33000/33000 [==============================] - 10s 314us/step - loss: 1.5964 - acc: 0.6387 - val_loss: 1.4342 - val_acc: 0.6877\n",
      "Epoch 73/75\n",
      "33000/33000 [==============================] - 10s 315us/step - loss: 1.5871 - acc: 0.6345 - val_loss: 1.4181 - val_acc: 0.6900\n",
      "Epoch 74/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 1.5890 - acc: 0.6348 - val_loss: 1.4426 - val_acc: 0.6883\n",
      "Epoch 75/75\n",
      "33000/33000 [==============================] - 11s 336us/step - loss: 1.5881 - acc: 0.6355 - val_loss: 1.4275 - val_acc: 0.6840\n"
     ]
    }
   ],
   "source": [
    "#Drop out method with bigger database\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(19,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_with_drop = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 4s 111us/step\n",
      "4000/4000 [==============================] - 0s 122us/step\n"
     ]
    }
   ],
   "source": [
    "results_train_l2_extended_drop= model.evaluate(train_final, label_train_final)\n",
    "results_test_l2_extended_drop = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results- Drop Model Extended: \n",
      " Loss:1.313795260371584 \n",
      " Accuracy :0.7240909090909091\n",
      "Test results -Drop Model Extended:\n",
      " Loss:1.4547750024795532 \n",
      " Accuracy:0.67625\n"
     ]
    }
   ],
   "source": [
    "print('Training results- Drop Model Extended: \\n Loss:{} \\n Accuracy :{}'.format(results_train_l2_extended_drop[0],results_train_l2_extended_drop[1])) \n",
    "print('Test results -Drop Model Extended:\\n Loss:{} \\n Accuracy:{}'.format(results_test_l2_extended_drop[0] ,results_test_l2_extended_drop[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.8832 - acc: 0.5208 - val_loss: 1.5373 - val_acc: 0.6373\n",
      "Epoch 2/75\n",
      "33000/33000 [==============================] - 9s 267us/step - loss: 1.6249 - acc: 0.5948 - val_loss: 1.4762 - val_acc: 0.6570\n",
      "Epoch 3/75\n",
      "33000/33000 [==============================] - 10s 298us/step - loss: 1.5786 - acc: 0.6132 - val_loss: 1.4421 - val_acc: 0.6567\n",
      "Epoch 4/75\n",
      "33000/33000 [==============================] - 9s 283us/step - loss: 1.5504 - acc: 0.6208 - val_loss: 1.4289 - val_acc: 0.6597\n",
      "Epoch 5/75\n",
      "33000/33000 [==============================] - 10s 301us/step - loss: 1.5320 - acc: 0.6287 - val_loss: 1.4165 - val_acc: 0.6707\n",
      "Epoch 6/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.5211 - acc: 0.6363 - val_loss: 1.4159 - val_acc: 0.6780\n",
      "Epoch 7/75\n",
      "33000/33000 [==============================] - 10s 305us/step - loss: 1.5109 - acc: 0.6404 - val_loss: 1.3917 - val_acc: 0.6910\n",
      "Epoch 8/75\n",
      "33000/33000 [==============================] - 10s 303us/step - loss: 1.4972 - acc: 0.6443 - val_loss: 1.4045 - val_acc: 0.6817\n",
      "Epoch 9/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.4939 - acc: 0.6483 - val_loss: 1.3798 - val_acc: 0.6880\n",
      "Epoch 10/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.4856 - acc: 0.6543 - val_loss: 1.3928 - val_acc: 0.6837\n",
      "Epoch 11/75\n",
      "33000/33000 [==============================] - 10s 296us/step - loss: 1.4800 - acc: 0.6535 - val_loss: 1.3773 - val_acc: 0.6873\n",
      "Epoch 12/75\n",
      "33000/33000 [==============================] - 11s 320us/step - loss: 1.4729 - acc: 0.6572 - val_loss: 1.3886 - val_acc: 0.6867\n",
      "Epoch 13/75\n",
      "33000/33000 [==============================] - 11s 339us/step - loss: 1.4686 - acc: 0.6565 - val_loss: 1.3977 - val_acc: 0.6867\n",
      "Epoch 14/75\n",
      "33000/33000 [==============================] - 10s 307us/step - loss: 1.4671 - acc: 0.6611 - val_loss: 1.3678 - val_acc: 0.6917\n",
      "Epoch 15/75\n",
      "33000/33000 [==============================] - 10s 311us/step - loss: 1.4614 - acc: 0.6605 - val_loss: 1.3700 - val_acc: 0.6890\n",
      "Epoch 16/75\n",
      "33000/33000 [==============================] - 10s 312us/step - loss: 1.4618 - acc: 0.6630 - val_loss: 1.3998 - val_acc: 0.6870\n",
      "Epoch 17/75\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 1.4518 - acc: 0.6668 - val_loss: 1.3831 - val_acc: 0.6910\n",
      "Epoch 18/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.4512 - acc: 0.6679 - val_loss: 1.3886 - val_acc: 0.6903\n",
      "Epoch 19/75\n",
      "33000/33000 [==============================] - 10s 315us/step - loss: 1.4493 - acc: 0.6665 - val_loss: 1.3915 - val_acc: 0.6897\n",
      "Epoch 20/75\n",
      "33000/33000 [==============================] - 10s 306us/step - loss: 1.4492 - acc: 0.6673 - val_loss: 1.3634 - val_acc: 0.6940\n",
      "Epoch 21/75\n",
      "33000/33000 [==============================] - 11s 318us/step - loss: 1.4438 - acc: 0.6700 - val_loss: 1.3814 - val_acc: 0.6893\n",
      "Epoch 22/75\n",
      "33000/33000 [==============================] - 10s 316us/step - loss: 1.4444 - acc: 0.6721 - val_loss: 1.3731 - val_acc: 0.6907\n",
      "Epoch 23/75\n",
      "33000/33000 [==============================] - 10s 308us/step - loss: 1.4390 - acc: 0.6707 - val_loss: 1.3691 - val_acc: 0.6920\n",
      "Epoch 24/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 1.4415 - acc: 0.6716 - val_loss: 1.3817 - val_acc: 0.6957\n",
      "Epoch 25/75\n",
      "33000/33000 [==============================] - 11s 320us/step - loss: 1.4386 - acc: 0.6729 - val_loss: 1.3761 - val_acc: 0.6883\n",
      "Epoch 26/75\n",
      "33000/33000 [==============================] - 10s 309us/step - loss: 1.4405 - acc: 0.6737 - val_loss: 1.3544 - val_acc: 0.6957\n",
      "Epoch 27/75\n",
      "33000/33000 [==============================] - 10s 317us/step - loss: 1.4355 - acc: 0.6736 - val_loss: 1.3589 - val_acc: 0.7050\n",
      "Epoch 28/75\n",
      "33000/33000 [==============================] - 11s 324us/step - loss: 1.4321 - acc: 0.6766 - val_loss: 1.3712 - val_acc: 0.6947\n",
      "Epoch 29/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.4349 - acc: 0.6772 - val_loss: 1.3863 - val_acc: 0.6920\n",
      "Epoch 30/75\n",
      "33000/33000 [==============================] - 11s 322us/step - loss: 1.4352 - acc: 0.6752 - val_loss: 1.3799 - val_acc: 0.6907\n",
      "Epoch 31/75\n",
      "33000/33000 [==============================] - 11s 321us/step - loss: 1.4263 - acc: 0.6772 - val_loss: 1.3727 - val_acc: 0.7003\n",
      "Epoch 32/75\n",
      "33000/33000 [==============================] - 11s 319us/step - loss: 1.4361 - acc: 0.6750 - val_loss: 1.3686 - val_acc: 0.7017\n",
      "Epoch 33/75\n",
      "33000/33000 [==============================] - 9s 271us/step - loss: 1.4222 - acc: 0.6822 - val_loss: 1.3521 - val_acc: 0.7103\n",
      "Epoch 34/75\n",
      "33000/33000 [==============================] - 10s 313us/step - loss: 1.4258 - acc: 0.6799 - val_loss: 1.3612 - val_acc: 0.7010\n",
      "Epoch 35/75\n",
      "33000/33000 [==============================] - 11s 323us/step - loss: 1.4309 - acc: 0.6793 - val_loss: 1.3666 - val_acc: 0.6997\n",
      "Epoch 36/75\n",
      "33000/33000 [==============================] - 11s 323us/step - loss: 1.4234 - acc: 0.6810 - val_loss: 1.3771 - val_acc: 0.6947\n",
      "Epoch 37/75\n",
      "33000/33000 [==============================] - 11s 318us/step - loss: 1.4207 - acc: 0.6802 - val_loss: 1.3800 - val_acc: 0.6917\n",
      "Epoch 38/75\n",
      "33000/33000 [==============================] - 11s 321us/step - loss: 1.4259 - acc: 0.6794 - val_loss: 1.3901 - val_acc: 0.6917\n",
      "Epoch 39/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.4295 - acc: 0.6800 - val_loss: 1.3807 - val_acc: 0.6940\n",
      "Epoch 40/75\n",
      "33000/33000 [==============================] - 11s 333us/step - loss: 1.4230 - acc: 0.6820 - val_loss: 1.3904 - val_acc: 0.6873\n",
      "Epoch 41/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.4247 - acc: 0.6817 - val_loss: 1.3872 - val_acc: 0.6883\n",
      "Epoch 42/75\n",
      "33000/33000 [==============================] - 11s 330us/step - loss: 1.4190 - acc: 0.6843 - val_loss: 1.3942 - val_acc: 0.6900\n",
      "Epoch 43/75\n",
      "33000/33000 [==============================] - 11s 322us/step - loss: 1.4218 - acc: 0.6845 - val_loss: 1.3828 - val_acc: 0.6937\n",
      "Epoch 44/75\n",
      "33000/33000 [==============================] - 11s 328us/step - loss: 1.4226 - acc: 0.6838 - val_loss: 1.3851 - val_acc: 0.6933\n",
      "Epoch 45/75\n",
      "33000/33000 [==============================] - 11s 324us/step - loss: 1.4218 - acc: 0.6856 - val_loss: 1.3881 - val_acc: 0.6987\n",
      "Epoch 46/75\n",
      "33000/33000 [==============================] - 11s 326us/step - loss: 1.4255 - acc: 0.6855 - val_loss: 1.3766 - val_acc: 0.6977\n",
      "Epoch 47/75\n",
      "33000/33000 [==============================] - 11s 323us/step - loss: 1.4190 - acc: 0.6844 - val_loss: 1.3825 - val_acc: 0.7017\n",
      "Epoch 48/75\n",
      "33000/33000 [==============================] - 11s 327us/step - loss: 1.4200 - acc: 0.6859 - val_loss: 1.3799 - val_acc: 0.7023\n",
      "Epoch 49/75\n",
      "33000/33000 [==============================] - 11s 324us/step - loss: 1.4195 - acc: 0.6857 - val_loss: 1.3888 - val_acc: 0.6943\n",
      "Epoch 50/75\n",
      "33000/33000 [==============================] - 11s 327us/step - loss: 1.4188 - acc: 0.6856 - val_loss: 1.3987 - val_acc: 0.6903\n",
      "Epoch 51/75\n",
      "33000/33000 [==============================] - 11s 321us/step - loss: 1.4248 - acc: 0.6844 - val_loss: 1.3951 - val_acc: 0.6937\n",
      "Epoch 52/75\n",
      "33000/33000 [==============================] - 11s 329us/step - loss: 1.4160 - acc: 0.6859 - val_loss: 1.3847 - val_acc: 0.7007\n",
      "Epoch 53/75\n",
      "33000/33000 [==============================] - 11s 331us/step - loss: 1.4227 - acc: 0.6861 - val_loss: 1.3905 - val_acc: 0.6973\n",
      "Epoch 54/75\n",
      "33000/33000 [==============================] - 11s 341us/step - loss: 1.4200 - acc: 0.6869 - val_loss: 1.3740 - val_acc: 0.7003\n",
      "Epoch 55/75\n",
      "33000/33000 [==============================] - 11s 323us/step - loss: 1.4186 - acc: 0.6867 - val_loss: 1.3752 - val_acc: 0.7007\n",
      "Epoch 56/75\n",
      "33000/33000 [==============================] - 11s 344us/step - loss: 1.4198 - acc: 0.6846 - val_loss: 1.3720 - val_acc: 0.6997\n",
      "Epoch 57/75\n",
      "33000/33000 [==============================] - 11s 330us/step - loss: 1.4174 - acc: 0.6848 - val_loss: 1.3984 - val_acc: 0.6970\n",
      "Epoch 58/75\n",
      "33000/33000 [==============================] - 11s 329us/step - loss: 1.4156 - acc: 0.6885 - val_loss: 1.3783 - val_acc: 0.6993\n",
      "Epoch 59/75\n",
      "33000/33000 [==============================] - 10s 302us/step - loss: 1.4124 - acc: 0.6920 - val_loss: 1.3717 - val_acc: 0.6977\n",
      "Epoch 60/75\n",
      "33000/33000 [==============================] - 11s 325us/step - loss: 1.4190 - acc: 0.6905 - val_loss: 1.3706 - val_acc: 0.6993\n",
      "Epoch 61/75\n",
      "33000/33000 [==============================] - 10s 310us/step - loss: 1.4150 - acc: 0.6867 - val_loss: 1.3659 - val_acc: 0.7037\n",
      "Epoch 62/75\n",
      "33000/33000 [==============================] - 11s 340us/step - loss: 1.4163 - acc: 0.6876 - val_loss: 1.3828 - val_acc: 0.7063\n",
      "Epoch 63/75\n",
      "33000/33000 [==============================] - 10s 291us/step - loss: 1.4214 - acc: 0.6856 - val_loss: 1.3855 - val_acc: 0.6930\n",
      "Epoch 64/75\n",
      "33000/33000 [==============================] - 11s 332us/step - loss: 1.4167 - acc: 0.6894 - val_loss: 1.3796 - val_acc: 0.6987\n",
      "Epoch 65/75\n",
      "33000/33000 [==============================] - 11s 344us/step - loss: 1.4150 - acc: 0.6909 - val_loss: 1.3934 - val_acc: 0.6913\n",
      "Epoch 66/75\n",
      "33000/33000 [==============================] - 11s 335us/step - loss: 1.4176 - acc: 0.6908 - val_loss: 1.3896 - val_acc: 0.6907\n",
      "Epoch 67/75\n",
      "33000/33000 [==============================] - 11s 337us/step - loss: 1.4189 - acc: 0.6888 - val_loss: 1.3953 - val_acc: 0.6987\n",
      "Epoch 68/75\n",
      "33000/33000 [==============================] - 11s 336us/step - loss: 1.4169 - acc: 0.6909 - val_loss: 1.3798 - val_acc: 0.6990\n",
      "Epoch 69/75\n",
      "33000/33000 [==============================] - 11s 329us/step - loss: 1.4212 - acc: 0.6888 - val_loss: 1.3884 - val_acc: 0.6947\n",
      "Epoch 70/75\n",
      "33000/33000 [==============================] - 11s 335us/step - loss: 1.4139 - acc: 0.6889 - val_loss: 1.4133 - val_acc: 0.6983\n",
      "Epoch 71/75\n",
      "33000/33000 [==============================] - 11s 333us/step - loss: 1.4155 - acc: 0.6913 - val_loss: 1.3776 - val_acc: 0.6960\n",
      "Epoch 72/75\n",
      "33000/33000 [==============================] - 11s 339us/step - loss: 1.4173 - acc: 0.6927 - val_loss: 1.3770 - val_acc: 0.7007\n",
      "Epoch 73/75\n",
      "33000/33000 [==============================] - 10s 317us/step - loss: 1.4144 - acc: 0.6909 - val_loss: 1.4248 - val_acc: 0.6833\n",
      "Epoch 74/75\n",
      "33000/33000 [==============================] - 10s 289us/step - loss: 1.4177 - acc: 0.6918 - val_loss: 1.3894 - val_acc: 0.6950\n",
      "Epoch 75/75\n",
      "33000/33000 [==============================] - 10s 290us/step - loss: 1.4162 - acc: 0.6916 - val_loss: 1.3944 - val_acc: 0.6977\n"
     ]
    }
   ],
   "source": [
    "#Drop out method with bigger database\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(100,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(19,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_with_drop = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_cat_2 = {'red_dry' : ['tannat', 'nebbiolo','sagrantino','cabernet franc','chianti', 'petit verdot','petite verdot','bordeaux','meritage', 'tempranillo','tempranillo blend','tinto fino','tinta de toro', 'french mourvedre', 'aglianico', 'barbera', 'montepulcaiano','red blend', 'corvina, rondinella, molinara','montepulciano',\n",
    "'sangiovese','carmenere','cabernet france','cabernet sauvignon','cabernet','gamay','menca','mencia','valpolicella','rhone_blend','mourvdre','rhne','beaujolais','burgundy','syrah','trincadeira','pinot noir','pinot nero','dolcetto','garnacha', 'bonarda','amorone della valpolicella','negroamaro','nerello mascalese','supertuscans',\n",
    "'merlot', 'alfrocheiro','alcicante bouschet','shiraz','monastrell','malbec',\"nero d'avola\",'petite sirah','primitivo','zinfandel','grenache','g-s-m','g','touriga nacional'],'red_sweet':['sangiovese grosso','lambrusco','brachetto','port','banyuls','maury'],'white_dry': ['pinot grigio','pinot blanc',\n",
    "'albarino','garganega','dry furmit','gavi','muscadet','melon','muscat','muskat','chablis','grenache blanc','macabeo','vinho verde','grillo','arinto','sauvignon blanc','friulano','fum blanc','fume blanc','sauvignon gris','sauvignon','verdejo','grner veltliner','verdicchio','colombard','vermentino',\n",
    "'turbiana','vernaccia','chenin blanc','torronts','fiano','albario','chardonnay','marsanne','roussanne','semillon','trebbiano','viura','pinot gris','smillon','pinot blanc','pinot bianco','viognier','dry riesling'],'white_sweet':[ 'riesling','johannisberg riesling','chenin blanc','torronts','mller-thurgau','moscato','gewrztraminer','sauternes','tokaji','white port', 'moscatel dessert wine', 'passito wines','vin santo','white blend'], 'rose':\n",
    "['rose','ros','rosato'],'sparkling':['champagne','prosecco','cava','sparkling blend','sparkling wine','champagne blend','glera']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'red_dry': ['tannat',\n",
       "  'nebbiolo',\n",
       "  'sagrantino',\n",
       "  'cabernet franc',\n",
       "  'chianti',\n",
       "  'petit verdot',\n",
       "  'petite verdot',\n",
       "  'bordeaux',\n",
       "  'meritage',\n",
       "  'tempranillo',\n",
       "  'tempranillo blend',\n",
       "  'tinto fino',\n",
       "  'tinta de toro',\n",
       "  'french mourvedre',\n",
       "  'aglianico',\n",
       "  'barbera',\n",
       "  'montepulcaiano',\n",
       "  'red blend',\n",
       "  'corvina, rondinella, molinara',\n",
       "  'montepulciano',\n",
       "  'sangiovese',\n",
       "  'carmenere',\n",
       "  'cabernet france',\n",
       "  'cabernet sauvignon',\n",
       "  'cabernet',\n",
       "  'gamay',\n",
       "  'menca',\n",
       "  'mencia',\n",
       "  'valpolicella',\n",
       "  'rhone_blend',\n",
       "  'mourvdre',\n",
       "  'rhne',\n",
       "  'beaujolais',\n",
       "  'burgundy',\n",
       "  'syrah',\n",
       "  'trincadeira',\n",
       "  'pinot noir',\n",
       "  'pinot nero',\n",
       "  'dolcetto',\n",
       "  'garnacha',\n",
       "  'bonarda',\n",
       "  'amorone della valpolicella',\n",
       "  'negroamaro',\n",
       "  'nerello mascalese',\n",
       "  'supertuscans',\n",
       "  'merlot',\n",
       "  'alfrocheiro',\n",
       "  'alcicante bouschet',\n",
       "  'shiraz',\n",
       "  'monastrell',\n",
       "  'malbec',\n",
       "  \"nero d'avola\",\n",
       "  'petite sirah',\n",
       "  'primitivo',\n",
       "  'zinfandel',\n",
       "  'grenache',\n",
       "  'g-s-m',\n",
       "  'g',\n",
       "  'touriga nacional'],\n",
       " 'red_sweet': ['sangiovese grosso',\n",
       "  'lambrusco',\n",
       "  'brachetto',\n",
       "  'port',\n",
       "  'banyuls',\n",
       "  'maury'],\n",
       " 'white_dry': ['pinot grigio',\n",
       "  'pinot blanc',\n",
       "  'albarino',\n",
       "  'garganega',\n",
       "  'dry furmit',\n",
       "  'gavi',\n",
       "  'muscadet',\n",
       "  'melon',\n",
       "  'muscat',\n",
       "  'muskat',\n",
       "  'chablis',\n",
       "  'grenache blanc',\n",
       "  'macabeo',\n",
       "  'vinho verde',\n",
       "  'grillo',\n",
       "  'arinto',\n",
       "  'sauvignon blanc',\n",
       "  'friulano',\n",
       "  'fum blanc',\n",
       "  'fume blanc',\n",
       "  'sauvignon gris',\n",
       "  'sauvignon',\n",
       "  'verdejo',\n",
       "  'grner veltliner',\n",
       "  'verdicchio',\n",
       "  'colombard',\n",
       "  'vermentino',\n",
       "  'turbiana',\n",
       "  'vernaccia',\n",
       "  'chenin blanc',\n",
       "  'torronts',\n",
       "  'fiano',\n",
       "  'albario',\n",
       "  'chardonnay',\n",
       "  'marsanne',\n",
       "  'roussanne',\n",
       "  'semillon',\n",
       "  'trebbiano',\n",
       "  'viura',\n",
       "  'pinot gris',\n",
       "  'smillon',\n",
       "  'pinot blanc',\n",
       "  'pinot bianco',\n",
       "  'viognier',\n",
       "  'dry riesling'],\n",
       " 'white_sweet': ['riesling',\n",
       "  'johannisberg riesling',\n",
       "  'chenin blanc',\n",
       "  'torronts',\n",
       "  'mller-thurgau',\n",
       "  'moscato',\n",
       "  'gewrztraminer',\n",
       "  'sauternes',\n",
       "  'tokaji',\n",
       "  'white port',\n",
       "  'moscatel dessert wine',\n",
       "  'passito wines',\n",
       "  'vin santo',\n",
       "  'white blend'],\n",
       " 'rose': ['rose', 'ros', 'rosato'],\n",
       " 'sparkling': ['champagne',\n",
       "  'prosecco',\n",
       "  'cava',\n",
       "  'sparkling blend',\n",
       "  'sparkling wine',\n",
       "  'champagne blend',\n",
       "  'glera']}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_cat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict_2 (dict):\n",
    "    update= {}\n",
    "    for i in list(dict.keys()):\n",
    "        for wine in wine_cat_2[i]:\n",
    "            update[wine]=i\n",
    "    return update\n",
    "#creating update dictionary to add new categories\n",
    "update = update_dict_2(wine_cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tannat': 'red_dry',\n",
       " 'nebbiolo': 'red_dry',\n",
       " 'sagrantino': 'red_dry',\n",
       " 'cabernet franc': 'red_dry',\n",
       " 'chianti': 'red_dry',\n",
       " 'petit verdot': 'red_dry',\n",
       " 'petite verdot': 'red_dry',\n",
       " 'bordeaux': 'red_dry',\n",
       " 'meritage': 'red_dry',\n",
       " 'tempranillo': 'red_dry',\n",
       " 'tempranillo blend': 'red_dry',\n",
       " 'tinto fino': 'red_dry',\n",
       " 'tinta de toro': 'red_dry',\n",
       " 'french mourvedre': 'red_dry',\n",
       " 'aglianico': 'red_dry',\n",
       " 'barbera': 'red_dry',\n",
       " 'montepulcaiano': 'red_dry',\n",
       " 'red blend': 'red_dry',\n",
       " 'corvina, rondinella, molinara': 'red_dry',\n",
       " 'montepulciano': 'red_dry',\n",
       " 'sangiovese': 'red_dry',\n",
       " 'carmenere': 'red_dry',\n",
       " 'cabernet france': 'red_dry',\n",
       " 'cabernet sauvignon': 'red_dry',\n",
       " 'cabernet': 'red_dry',\n",
       " 'gamay': 'red_dry',\n",
       " 'menca': 'red_dry',\n",
       " 'mencia': 'red_dry',\n",
       " 'valpolicella': 'red_dry',\n",
       " 'rhone_blend': 'red_dry',\n",
       " 'mourvdre': 'red_dry',\n",
       " 'rhne': 'red_dry',\n",
       " 'beaujolais': 'red_dry',\n",
       " 'burgundy': 'red_dry',\n",
       " 'syrah': 'red_dry',\n",
       " 'trincadeira': 'red_dry',\n",
       " 'pinot noir': 'red_dry',\n",
       " 'pinot nero': 'red_dry',\n",
       " 'dolcetto': 'red_dry',\n",
       " 'garnacha': 'red_dry',\n",
       " 'bonarda': 'red_dry',\n",
       " 'amorone della valpolicella': 'red_dry',\n",
       " 'negroamaro': 'red_dry',\n",
       " 'nerello mascalese': 'red_dry',\n",
       " 'supertuscans': 'red_dry',\n",
       " 'merlot': 'red_dry',\n",
       " 'alfrocheiro': 'red_dry',\n",
       " 'alcicante bouschet': 'red_dry',\n",
       " 'shiraz': 'red_dry',\n",
       " 'monastrell': 'red_dry',\n",
       " 'malbec': 'red_dry',\n",
       " \"nero d'avola\": 'red_dry',\n",
       " 'petite sirah': 'red_dry',\n",
       " 'primitivo': 'red_dry',\n",
       " 'zinfandel': 'red_dry',\n",
       " 'grenache': 'red_dry',\n",
       " 'g-s-m': 'red_dry',\n",
       " 'g': 'red_dry',\n",
       " 'touriga nacional': 'red_dry',\n",
       " 'sangiovese grosso': 'red_sweet',\n",
       " 'lambrusco': 'red_sweet',\n",
       " 'brachetto': 'red_sweet',\n",
       " 'port': 'red_sweet',\n",
       " 'banyuls': 'red_sweet',\n",
       " 'maury': 'red_sweet',\n",
       " 'pinot grigio': 'white_dry',\n",
       " 'pinot blanc': 'white_dry',\n",
       " 'albarino': 'white_dry',\n",
       " 'garganega': 'white_dry',\n",
       " 'dry furmit': 'white_dry',\n",
       " 'gavi': 'white_dry',\n",
       " 'muscadet': 'white_dry',\n",
       " 'melon': 'white_dry',\n",
       " 'muscat': 'white_dry',\n",
       " 'muskat': 'white_dry',\n",
       " 'chablis': 'white_dry',\n",
       " 'grenache blanc': 'white_dry',\n",
       " 'macabeo': 'white_dry',\n",
       " 'vinho verde': 'white_dry',\n",
       " 'grillo': 'white_dry',\n",
       " 'arinto': 'white_dry',\n",
       " 'sauvignon blanc': 'white_dry',\n",
       " 'friulano': 'white_dry',\n",
       " 'fum blanc': 'white_dry',\n",
       " 'fume blanc': 'white_dry',\n",
       " 'sauvignon gris': 'white_dry',\n",
       " 'sauvignon': 'white_dry',\n",
       " 'verdejo': 'white_dry',\n",
       " 'grner veltliner': 'white_dry',\n",
       " 'verdicchio': 'white_dry',\n",
       " 'colombard': 'white_dry',\n",
       " 'vermentino': 'white_dry',\n",
       " 'turbiana': 'white_dry',\n",
       " 'vernaccia': 'white_dry',\n",
       " 'chenin blanc': 'white_sweet',\n",
       " 'torronts': 'white_sweet',\n",
       " 'fiano': 'white_dry',\n",
       " 'albario': 'white_dry',\n",
       " 'chardonnay': 'white_dry',\n",
       " 'marsanne': 'white_dry',\n",
       " 'roussanne': 'white_dry',\n",
       " 'semillon': 'white_dry',\n",
       " 'trebbiano': 'white_dry',\n",
       " 'viura': 'white_dry',\n",
       " 'pinot gris': 'white_dry',\n",
       " 'smillon': 'white_dry',\n",
       " 'pinot bianco': 'white_dry',\n",
       " 'viognier': 'white_dry',\n",
       " 'dry riesling': 'white_dry',\n",
       " 'riesling': 'white_sweet',\n",
       " 'johannisberg riesling': 'white_sweet',\n",
       " 'mller-thurgau': 'white_sweet',\n",
       " 'moscato': 'white_sweet',\n",
       " 'gewrztraminer': 'white_sweet',\n",
       " 'sauternes': 'white_sweet',\n",
       " 'tokaji': 'white_sweet',\n",
       " 'white port': 'white_sweet',\n",
       " 'moscatel dessert wine': 'white_sweet',\n",
       " 'passito wines': 'white_sweet',\n",
       " 'vin santo': 'white_sweet',\n",
       " 'white blend': 'white_sweet',\n",
       " 'rose': 'rose',\n",
       " 'ros': 'rose',\n",
       " 'rosato': 'rose',\n",
       " 'champagne': 'sparkling',\n",
       " 'prosecco': 'sparkling',\n",
       " 'cava': 'sparkling',\n",
       " 'sparkling blend': 'sparkling',\n",
       " 'sparkling wine': 'sparkling',\n",
       " 'champagne blend': 'sparkling',\n",
       " 'glera': 'sparkling'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = wine_cat_df(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_update = mapping (df, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wine_update['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take random sample of 40000\n",
    "\n",
    "wine3 =df_wine.sample(40000)\n",
    "wine3.index = range(40000)\n",
    "product = wine3[\"category\"]\n",
    "complaints =wine3[\"description\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(0)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "33000/33000 [==============================] - 5s 149us/step - loss: 1.1976 - acc: 0.6150 - val_loss: 0.9655 - val_acc: 0.6827\n",
      "Epoch 2/100\n",
      "33000/33000 [==============================] - 4s 126us/step - loss: 0.8207 - acc: 0.7275 - val_loss: 0.9611 - val_acc: 0.6957\n",
      "Epoch 3/100\n",
      "33000/33000 [==============================] - 4s 115us/step - loss: 0.6882 - acc: 0.7674 - val_loss: 0.9808 - val_acc: 0.6937\n",
      "Epoch 4/100\n",
      "33000/33000 [==============================] - 4s 124us/step - loss: 0.5648 - acc: 0.8093 - val_loss: 1.0152 - val_acc: 0.6950\n",
      "Epoch 5/100\n",
      "33000/33000 [==============================] - 4s 127us/step - loss: 0.4439 - acc: 0.8508 - val_loss: 1.0943 - val_acc: 0.6967\n",
      "Epoch 6/100\n",
      "33000/33000 [==============================] - 4s 123us/step - loss: 0.3284 - acc: 0.8954 - val_loss: 1.2209 - val_acc: 0.6953\n",
      "Epoch 7/100\n",
      "33000/33000 [==============================] - 4s 126us/step - loss: 0.2238 - acc: 0.9338 - val_loss: 1.4098 - val_acc: 0.6823\n",
      "Epoch 8/100\n",
      "33000/33000 [==============================] - 4s 129us/step - loss: 0.1464 - acc: 0.9594 - val_loss: 1.6229 - val_acc: 0.6873\n",
      "Epoch 9/100\n",
      "33000/33000 [==============================] - 4s 128us/step - loss: 0.0905 - acc: 0.9773 - val_loss: 1.8160 - val_acc: 0.6757\n",
      "Epoch 10/100\n",
      "33000/33000 [==============================] - 4s 126us/step - loss: 0.0547 - acc: 0.9885 - val_loss: 1.9985 - val_acc: 0.6737\n",
      "Epoch 11/100\n",
      "33000/33000 [==============================] - 4s 126us/step - loss: 0.0359 - acc: 0.9928 - val_loss: 2.1870 - val_acc: 0.6700\n",
      "Epoch 12/100\n",
      "33000/33000 [==============================] - 4s 133us/step - loss: 0.0323 - acc: 0.9920 - val_loss: 2.3530 - val_acc: 0.6680\n",
      "Epoch 13/100\n",
      "33000/33000 [==============================] - 5s 136us/step - loss: 0.0327 - acc: 0.9917 - val_loss: 2.4444 - val_acc: 0.6700\n",
      "Epoch 14/100\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.0270 - acc: 0.9929 - val_loss: 2.5629 - val_acc: 0.6647\n",
      "Epoch 15/100\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.0154 - acc: 0.9970 - val_loss: 2.6402 - val_acc: 0.6757\n",
      "Epoch 16/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0118 - acc: 0.9973 - val_loss: 2.6865 - val_acc: 0.6703\n",
      "Epoch 17/100\n",
      "33000/33000 [==============================] - 4s 135us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 2.8168 - val_acc: 0.6667\n",
      "Epoch 18/100\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.0241 - acc: 0.9927 - val_loss: 2.8783 - val_acc: 0.6600\n",
      "Epoch 19/100\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.0135 - acc: 0.9965 - val_loss: 2.9076 - val_acc: 0.6673\n",
      "Epoch 20/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0102 - acc: 0.9973 - val_loss: 2.9336 - val_acc: 0.6750\n",
      "Epoch 21/100\n",
      "33000/33000 [==============================] - 4s 131us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 3.0098 - val_acc: 0.6763\n",
      "Epoch 22/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 3.1437 - val_acc: 0.6480\n",
      "Epoch 23/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0262 - acc: 0.9914 - val_loss: 3.1129 - val_acc: 0.6593\n",
      "Epoch 24/100\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 3.0238 - val_acc: 0.6700\n",
      "Epoch 25/100\n",
      "33000/33000 [==============================] - 4s 124us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 3.0523 - val_acc: 0.6723\n",
      "Epoch 26/100\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 3.1281 - val_acc: 0.6703\n",
      "Epoch 27/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 3.2615 - val_acc: 0.6717\n",
      "Epoch 28/100\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.0333 - acc: 0.9889 - val_loss: 3.2304 - val_acc: 0.6697\n",
      "Epoch 29/100\n",
      "33000/33000 [==============================] - 4s 129us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 3.3085 - val_acc: 0.6603\n",
      "Epoch 30/100\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 3.2799 - val_acc: 0.6650\n",
      "Epoch 31/100\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 8.7865e-04 - acc: 0.9999 - val_loss: 3.3111 - val_acc: 0.6667\n",
      "Epoch 32/100\n",
      "33000/33000 [==============================] - 4s 133us/step - loss: 1.9160e-04 - acc: 1.0000 - val_loss: 3.3358 - val_acc: 0.6647\n",
      "Epoch 33/100\n",
      "33000/33000 [==============================] - 4s 122us/step - loss: 1.0458e-04 - acc: 1.0000 - val_loss: 3.3500 - val_acc: 0.6667\n",
      "Epoch 34/100\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 7.1739e-05 - acc: 1.0000 - val_loss: 3.3731 - val_acc: 0.6660\n",
      "Epoch 35/100\n",
      "33000/33000 [==============================] - 5s 145us/step - loss: 5.0245e-05 - acc: 1.0000 - val_loss: 3.3969 - val_acc: 0.6680\n",
      "Epoch 36/100\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 3.4465e-05 - acc: 1.0000 - val_loss: 3.4231 - val_acc: 0.6693\n",
      "Epoch 37/100\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 2.3338e-05 - acc: 1.0000 - val_loss: 3.4506 - val_acc: 0.6707\n",
      "Epoch 38/100\n",
      "33000/33000 [==============================] - 4s 134us/step - loss: 1.5593e-05 - acc: 1.0000 - val_loss: 3.4857 - val_acc: 0.6707\n",
      "Epoch 39/100\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 1.0200e-05 - acc: 1.0000 - val_loss: 3.5143 - val_acc: 0.6713\n",
      "Epoch 40/100\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 6.5939e-06 - acc: 1.0000 - val_loss: 3.5447 - val_acc: 0.6720\n",
      "Epoch 41/100\n",
      "33000/33000 [==============================] - 4s 133us/step - loss: 4.1847e-06 - acc: 1.0000 - val_loss: 3.5860 - val_acc: 0.6697\n",
      "Epoch 42/100\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 2.6260e-06 - acc: 1.0000 - val_loss: 3.6209 - val_acc: 0.6683\n",
      "Epoch 43/100\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 1.6606e-06 - acc: 1.0000 - val_loss: 3.6532 - val_acc: 0.6713\n",
      "Epoch 44/100\n",
      "33000/33000 [==============================] - 5s 150us/step - loss: 1.0447e-06 - acc: 1.0000 - val_loss: 3.6919 - val_acc: 0.6693\n",
      "Epoch 45/100\n",
      "33000/33000 [==============================] - 5s 143us/step - loss: 6.7568e-07 - acc: 1.0000 - val_loss: 3.7251 - val_acc: 0.6693\n",
      "Epoch 46/100\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 4.4790e-07 - acc: 1.0000 - val_loss: 3.7563 - val_acc: 0.6700\n",
      "Epoch 47/100\n",
      "33000/33000 [==============================] - 5s 147us/step - loss: 3.0534e-07 - acc: 1.0000 - val_loss: 3.7849 - val_acc: 0.6697\n",
      "Epoch 48/100\n",
      "33000/33000 [==============================] - 5s 144us/step - loss: 2.2332e-07 - acc: 1.0000 - val_loss: 3.8180 - val_acc: 0.6700\n",
      "Epoch 49/100\n",
      "33000/33000 [==============================] - 5s 147us/step - loss: 1.7722e-07 - acc: 1.0000 - val_loss: 3.8471 - val_acc: 0.6683\n",
      "Epoch 50/100\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 1.4926e-07 - acc: 1.0000 - val_loss: 3.8692 - val_acc: 0.6707\n",
      "Epoch 51/100\n",
      "33000/33000 [==============================] - 5s 143us/step - loss: 1.3481e-07 - acc: 1.0000 - val_loss: 3.8901 - val_acc: 0.6700\n",
      "Epoch 52/100\n",
      " 4608/33000 [===>..........................] - ETA: 3s - loss: 1.2696e-07 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-09aed4bf84d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     validation_data=(val, label_val))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Creating model Sequential with reul and no regularization, epoachs 200 batch 30 \n",
    "model = Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 37us/step\n",
      "4000/4000 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the final results on the training and testing sets \n",
    "results_train_big = model.evaluate(train_final, label_train_final)\n",
    "results_test_big = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results without regularization: \n",
      " Loss:1.2596809597887456e-07 \n",
      " Accuracy :1.0\n",
      "Test results without regularization:\n",
      " Loss:3.9679316215515135 \n",
      " Accuracy:0.66525\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results without regularization: \\n Loss:{} \\n Accuracy :{}'.format(results_train_big[0],results_train_big[1],) )\n",
    "print('Test results without regularization:\\n Loss:{} \\n Accuracy:{}'.format(results_test_big[0] ,results_test_big[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/75\n",
      "33000/33000 [==============================] - 5s 148us/step - loss: 1.6826 - acc: 0.4678 - val_loss: 1.1543 - val_acc: 0.6280\n",
      "Epoch 2/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 1.2137 - acc: 0.6051 - val_loss: 1.0207 - val_acc: 0.6643\n",
      "Epoch 3/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 1.0800 - acc: 0.6512 - val_loss: 0.9768 - val_acc: 0.6867\n",
      "Epoch 4/75\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 0.9923 - acc: 0.6804 - val_loss: 0.9633 - val_acc: 0.6927\n",
      "Epoch 5/75\n",
      "33000/33000 [==============================] - 5s 144us/step - loss: 0.9311 - acc: 0.6996 - val_loss: 0.9852 - val_acc: 0.6937\n",
      "Epoch 6/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.8836 - acc: 0.7147 - val_loss: 0.9739 - val_acc: 0.7003\n",
      "Epoch 7/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.8368 - acc: 0.7257 - val_loss: 0.9794 - val_acc: 0.6980\n",
      "Epoch 8/75\n",
      "33000/33000 [==============================] - 5s 143us/step - loss: 0.7996 - acc: 0.7416 - val_loss: 0.9904 - val_acc: 0.6943\n",
      "Epoch 9/75\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 0.7655 - acc: 0.7531 - val_loss: 0.9955 - val_acc: 0.7010\n",
      "Epoch 10/75\n",
      "33000/33000 [==============================] - 5s 144us/step - loss: 0.7354 - acc: 0.7610 - val_loss: 1.0084 - val_acc: 0.6937\n",
      "Epoch 11/75\n",
      "33000/33000 [==============================] - 5s 145us/step - loss: 0.7022 - acc: 0.7732 - val_loss: 1.0375 - val_acc: 0.6970\n",
      "Epoch 12/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.6874 - acc: 0.7759 - val_loss: 1.0673 - val_acc: 0.6927\n",
      "Epoch 13/75\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.6711 - acc: 0.7805 - val_loss: 1.0674 - val_acc: 0.6897\n",
      "Epoch 14/75\n",
      "33000/33000 [==============================] - 5s 144us/step - loss: 0.6418 - acc: 0.7928 - val_loss: 1.0942 - val_acc: 0.6970\n",
      "Epoch 15/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.6241 - acc: 0.7962 - val_loss: 1.1153 - val_acc: 0.6973\n",
      "Epoch 16/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.6003 - acc: 0.8036 - val_loss: 1.1522 - val_acc: 0.6940\n",
      "Epoch 17/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.5869 - acc: 0.8068 - val_loss: 1.1687 - val_acc: 0.7043\n",
      "Epoch 18/75\n",
      "33000/33000 [==============================] - 5s 148us/step - loss: 0.5838 - acc: 0.8081 - val_loss: 1.1540 - val_acc: 0.7073\n",
      "Epoch 19/75\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 0.5611 - acc: 0.8166 - val_loss: 1.2141 - val_acc: 0.6973\n",
      "Epoch 20/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.5574 - acc: 0.8155 - val_loss: 1.2043 - val_acc: 0.6967\n",
      "Epoch 21/75\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.5472 - acc: 0.8178 - val_loss: 1.2558 - val_acc: 0.6997\n",
      "Epoch 22/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.5306 - acc: 0.8238 - val_loss: 1.2644 - val_acc: 0.7027\n",
      "Epoch 23/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.5249 - acc: 0.8244 - val_loss: 1.2658 - val_acc: 0.7043\n",
      "Epoch 24/75\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.5129 - acc: 0.8300 - val_loss: 1.3020 - val_acc: 0.6997\n",
      "Epoch 25/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.5115 - acc: 0.8284 - val_loss: 1.3134 - val_acc: 0.6983\n",
      "Epoch 26/75\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.4977 - acc: 0.8326 - val_loss: 1.3432 - val_acc: 0.7010\n",
      "Epoch 27/75\n",
      "33000/33000 [==============================] - 4s 133us/step - loss: 0.4831 - acc: 0.8404 - val_loss: 1.4033 - val_acc: 0.6960\n",
      "Epoch 28/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4856 - acc: 0.8368 - val_loss: 1.3763 - val_acc: 0.6983\n",
      "Epoch 29/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4800 - acc: 0.8388 - val_loss: 1.4215 - val_acc: 0.6980\n",
      "Epoch 30/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.4619 - acc: 0.8435 - val_loss: 1.4443 - val_acc: 0.6950\n",
      "Epoch 31/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.4622 - acc: 0.8452 - val_loss: 1.4823 - val_acc: 0.6920\n",
      "Epoch 32/75\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.4540 - acc: 0.8478 - val_loss: 1.4990 - val_acc: 0.6947\n",
      "Epoch 33/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4376 - acc: 0.8516 - val_loss: 1.5184 - val_acc: 0.6930\n",
      "Epoch 34/75\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.4508 - acc: 0.8504 - val_loss: 1.5482 - val_acc: 0.6923\n",
      "Epoch 35/75\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 0.4429 - acc: 0.8525 - val_loss: 1.5450 - val_acc: 0.6887\n",
      "Epoch 36/75\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.4447 - acc: 0.8499 - val_loss: 1.5794 - val_acc: 0.6860\n",
      "Epoch 37/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.4282 - acc: 0.8568 - val_loss: 1.5903 - val_acc: 0.6933\n",
      "Epoch 38/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.4221 - acc: 0.8566 - val_loss: 1.6637 - val_acc: 0.6937\n",
      "Epoch 39/75\n",
      "33000/33000 [==============================] - 4s 135us/step - loss: 0.4262 - acc: 0.8561 - val_loss: 1.6641 - val_acc: 0.6880\n",
      "Epoch 40/75\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.4197 - acc: 0.8564 - val_loss: 1.6911 - val_acc: 0.6930\n",
      "Epoch 41/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4153 - acc: 0.8621 - val_loss: 1.6979 - val_acc: 0.6940\n",
      "Epoch 42/75\n",
      "33000/33000 [==============================] - 5s 136us/step - loss: 0.4212 - acc: 0.8561 - val_loss: 1.6817 - val_acc: 0.6960\n",
      "Epoch 43/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.4071 - acc: 0.8638 - val_loss: 1.7224 - val_acc: 0.6867\n",
      "Epoch 44/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4029 - acc: 0.8628 - val_loss: 1.7514 - val_acc: 0.6923\n",
      "Epoch 45/75\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 0.4063 - acc: 0.8636 - val_loss: 1.7682 - val_acc: 0.6897\n",
      "Epoch 46/75\n",
      "33000/33000 [==============================] - 5s 136us/step - loss: 0.3981 - acc: 0.8648 - val_loss: 1.7751 - val_acc: 0.6940\n",
      "Epoch 47/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.4013 - acc: 0.8649 - val_loss: 1.8489 - val_acc: 0.6910\n",
      "Epoch 48/75\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 0.3901 - acc: 0.8672 - val_loss: 1.8228 - val_acc: 0.6950\n",
      "Epoch 49/75\n",
      "33000/33000 [==============================] - 5s 138us/step - loss: 0.3929 - acc: 0.8656 - val_loss: 1.8391 - val_acc: 0.6913\n",
      "Epoch 50/75\n",
      "33000/33000 [==============================] - 5s 137us/step - loss: 0.3902 - acc: 0.8693 - val_loss: 1.8623 - val_acc: 0.6913\n",
      "Epoch 51/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.3885 - acc: 0.8685 - val_loss: 1.8706 - val_acc: 0.6973\n",
      "Epoch 52/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.3861 - acc: 0.8703 - val_loss: 1.8820 - val_acc: 0.6893\n",
      "Epoch 53/75\n",
      "33000/33000 [==============================] - 4s 129us/step - loss: 0.3872 - acc: 0.8669 - val_loss: 1.9047 - val_acc: 0.6880\n",
      "Epoch 54/75\n",
      "33000/33000 [==============================] - 4s 118us/step - loss: 0.3835 - acc: 0.8722 - val_loss: 1.8834 - val_acc: 0.6877\n",
      "Epoch 55/75\n",
      "33000/33000 [==============================] - 5s 140us/step - loss: 0.3850 - acc: 0.8718 - val_loss: 1.9108 - val_acc: 0.6913\n",
      "Epoch 56/75\n",
      "33000/33000 [==============================] - 5s 141us/step - loss: 0.3825 - acc: 0.8701 - val_loss: 1.9482 - val_acc: 0.6923\n",
      "Epoch 57/75\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 0.3733 - acc: 0.8726 - val_loss: 1.9703 - val_acc: 0.6937\n",
      "Epoch 58/75\n",
      "33000/33000 [==============================] - 5s 147us/step - loss: 0.3669 - acc: 0.8762 - val_loss: 1.9859 - val_acc: 0.6913\n",
      "Epoch 59/75\n",
      "33000/33000 [==============================] - 5s 146us/step - loss: 0.3714 - acc: 0.8744 - val_loss: 1.9999 - val_acc: 0.6893\n",
      "Epoch 60/75\n",
      "33000/33000 [==============================] - 5s 142us/step - loss: 0.3670 - acc: 0.8751 - val_loss: 2.0206 - val_acc: 0.6900\n",
      "Epoch 61/75\n",
      "33000/33000 [==============================] - 4s 131us/step - loss: 0.3681 - acc: 0.8763 - val_loss: 2.0338 - val_acc: 0.6917\n",
      "Epoch 62/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.3725 - acc: 0.8742 - val_loss: 2.0112 - val_acc: 0.6877\n",
      "Epoch 63/75\n",
      "33000/33000 [==============================] - 4s 131us/step - loss: 0.3588 - acc: 0.8786 - val_loss: 2.0491 - val_acc: 0.6860\n",
      "Epoch 64/75\n",
      "33000/33000 [==============================] - 4s 130us/step - loss: 0.3782 - acc: 0.8740 - val_loss: 2.0414 - val_acc: 0.6863\n",
      "Epoch 65/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.3679 - acc: 0.8771 - val_loss: 2.0433 - val_acc: 0.6900\n",
      "Epoch 66/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.3470 - acc: 0.8830 - val_loss: 2.1000 - val_acc: 0.6857\n",
      "Epoch 67/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.3657 - acc: 0.8772 - val_loss: 2.0807 - val_acc: 0.6807\n",
      "Epoch 68/75\n",
      "33000/33000 [==============================] - 4s 135us/step - loss: 0.3634 - acc: 0.8775 - val_loss: 2.1032 - val_acc: 0.6833\n",
      "Epoch 69/75\n",
      "33000/33000 [==============================] - 4s 135us/step - loss: 0.3532 - acc: 0.8816 - val_loss: 2.1341 - val_acc: 0.6797\n",
      "Epoch 70/75\n",
      "33000/33000 [==============================] - 4s 132us/step - loss: 0.3558 - acc: 0.8812 - val_loss: 2.1391 - val_acc: 0.6827\n",
      "Epoch 71/75\n",
      "33000/33000 [==============================] - 4s 135us/step - loss: 0.3545 - acc: 0.8815 - val_loss: 2.1188 - val_acc: 0.6853\n",
      "Epoch 72/75\n",
      "33000/33000 [==============================] - 4s 136us/step - loss: 0.3576 - acc: 0.8795 - val_loss: 2.1449 - val_acc: 0.6847\n",
      "Epoch 73/75\n",
      "33000/33000 [==============================] - 4s 133us/step - loss: 0.3558 - acc: 0.8793 - val_loss: 2.1293 - val_acc: 0.6820\n",
      "Epoch 74/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.3488 - acc: 0.8834 - val_loss: 2.1715 - val_acc: 0.6840\n",
      "Epoch 75/75\n",
      "33000/33000 [==============================] - 5s 139us/step - loss: 0.3498 - acc: 0.8808 - val_loss: 2.1613 - val_acc: 0.6847\n"
     ]
    }
   ],
   "source": [
    "#Big with Drop \n",
    "model = Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 40us/step\n",
      "4000/4000 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the final results on the training and testing sets \n",
    "results_train_big_drop = model.evaluate(train_final, label_train_final)\n",
    "results_test_big_drop = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_train_big_drop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-003cbd8bc66c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training results with Drop out at 50%: \\n Loss:{} \\n Accuracy :{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_train_big_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults_train_big_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test results with with Drop out at 50%:\\n Loss:{} \\n Accuracy:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_test_big_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mresults_test_big_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_train_big_drop' is not defined"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results with Drop out at 50%: \\n Loss:{} \\n Accuracy :{}'.format(results_train_big_drop[0],results_train_big_drop[1],) )\n",
    "print('Test results with with Drop out at 50%:\\n Loss:{} \\n Accuracy:{}'.format(results_test_big_drop[0] ,results_test_big_drop[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 3s 335us/step - loss: 0.8822 - acc: 0.7896 - val_loss: 0.5075 - val_acc: 0.8900\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.4976 - acc: 0.9011 - val_loss: 0.4521 - val_acc: 0.9110\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.4449 - acc: 0.9153 - val_loss: 0.4404 - val_acc: 0.9080\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 175us/step - loss: 0.4139 - acc: 0.9201 - val_loss: 0.4403 - val_acc: 0.9060\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.3924 - acc: 0.9207 - val_loss: 0.4229 - val_acc: 0.9100\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.3813 - acc: 0.9295 - val_loss: 0.4274 - val_acc: 0.9110\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.3745 - acc: 0.9253 - val_loss: 0.4177 - val_acc: 0.9160\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 0.3626 - acc: 0.9325 - val_loss: 0.4179 - val_acc: 0.9110\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.3601 - acc: 0.9333 - val_loss: 0.4174 - val_acc: 0.9140\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 2s 218us/step - loss: 0.3520 - acc: 0.9355 - val_loss: 0.4095 - val_acc: 0.9200\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.3514 - acc: 0.9375 - val_loss: 0.4225 - val_acc: 0.9120\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.3451 - acc: 0.9384 - val_loss: 0.4256 - val_acc: 0.9100\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.3360 - acc: 0.9420 - val_loss: 0.4021 - val_acc: 0.9200\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.3287 - acc: 0.9451 - val_loss: 0.4179 - val_acc: 0.9120\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 2s 200us/step - loss: 0.3289 - acc: 0.9424 - val_loss: 0.4022 - val_acc: 0.9270\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.3230 - acc: 0.9464 - val_loss: 0.4049 - val_acc: 0.9180\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.3221 - acc: 0.9488 - val_loss: 0.4143 - val_acc: 0.9170\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.3237 - acc: 0.9471 - val_loss: 0.4020 - val_acc: 0.9260\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.3154 - acc: 0.9493 - val_loss: 0.4195 - val_acc: 0.9100\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.3069 - acc: 0.9553 - val_loss: 0.4099 - val_acc: 0.9210\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.3099 - acc: 0.9516 - val_loss: 0.4140 - val_acc: 0.9210\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.3087 - acc: 0.9513 - val_loss: 0.4011 - val_acc: 0.9200\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.3027 - acc: 0.9555 - val_loss: 0.4016 - val_acc: 0.9210\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.2979 - acc: 0.9560 - val_loss: 0.4021 - val_acc: 0.9290\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2988 - acc: 0.9555 - val_loss: 0.4053 - val_acc: 0.9300\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.3013 - acc: 0.9577 - val_loss: 0.4053 - val_acc: 0.9230\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.3050 - acc: 0.9556 - val_loss: 0.3948 - val_acc: 0.9230\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 0.2870 - acc: 0.9619 - val_loss: 0.4116 - val_acc: 0.9200\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 0.2920 - acc: 0.9584 - val_loss: 0.4024 - val_acc: 0.9300\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 0.2893 - acc: 0.9597 - val_loss: 0.3955 - val_acc: 0.9300\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2788 - acc: 0.9617 - val_loss: 0.4009 - val_acc: 0.9240\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.2887 - acc: 0.9595 - val_loss: 0.4033 - val_acc: 0.9250\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 0.2833 - acc: 0.9635 - val_loss: 0.4180 - val_acc: 0.9230\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 2s 200us/step - loss: 0.2912 - acc: 0.9619 - val_loss: 0.4242 - val_acc: 0.9160\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.2807 - acc: 0.9653 - val_loss: 0.4099 - val_acc: 0.9220\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.2843 - acc: 0.9620 - val_loss: 0.4038 - val_acc: 0.9210\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 0.2687 - acc: 0.9641 - val_loss: 0.4058 - val_acc: 0.9230\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.2678 - acc: 0.9673 - val_loss: 0.4079 - val_acc: 0.9190\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 0.2683 - acc: 0.9647 - val_loss: 0.4090 - val_acc: 0.9220\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 2s 215us/step - loss: 0.2707 - acc: 0.9625 - val_loss: 0.4029 - val_acc: 0.9240\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 2s 234us/step - loss: 0.2773 - acc: 0.9637 - val_loss: 0.3972 - val_acc: 0.9220\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.2719 - acc: 0.9641 - val_loss: 0.3971 - val_acc: 0.9250\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.2714 - acc: 0.9651 - val_loss: 0.4065 - val_acc: 0.9290\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 2s 200us/step - loss: 0.2629 - acc: 0.9647 - val_loss: 0.4126 - val_acc: 0.9210\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 2s 210us/step - loss: 0.2626 - acc: 0.9681 - val_loss: 0.4156 - val_acc: 0.9180\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.2591 - acc: 0.9679 - val_loss: 0.4016 - val_acc: 0.9240\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.2648 - acc: 0.9679 - val_loss: 0.4285 - val_acc: 0.9220\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.2620 - acc: 0.9676 - val_loss: 0.4042 - val_acc: 0.9190\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 2s 209us/step - loss: 0.2582 - acc: 0.9709 - val_loss: 0.3851 - val_acc: 0.9270\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 0.2559 - acc: 0.9669 - val_loss: 0.3970 - val_acc: 0.9210\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.2673 - acc: 0.9691 - val_loss: 0.4005 - val_acc: 0.9230\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 2s 210us/step - loss: 0.2676 - acc: 0.9675 - val_loss: 0.4111 - val_acc: 0.9290\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.2557 - acc: 0.9707 - val_loss: 0.4253 - val_acc: 0.9140\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.2559 - acc: 0.9689 - val_loss: 0.4103 - val_acc: 0.9290\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.2576 - acc: 0.9693 - val_loss: 0.3904 - val_acc: 0.9350\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.2573 - acc: 0.9696 - val_loss: 0.4031 - val_acc: 0.9160\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.2606 - acc: 0.9691 - val_loss: 0.4024 - val_acc: 0.9220\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 0.2415 - acc: 0.9705 - val_loss: 0.4234 - val_acc: 0.9130\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2514 - acc: 0.9687 - val_loss: 0.4288 - val_acc: 0.9240\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.2555 - acc: 0.9716 - val_loss: 0.4069 - val_acc: 0.9260\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.2477 - acc: 0.9720 - val_loss: 0.3950 - val_acc: 0.9250\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 2s 220us/step - loss: 0.2520 - acc: 0.9691 - val_loss: 0.3899 - val_acc: 0.9250\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 2s 220us/step - loss: 0.2563 - acc: 0.9692 - val_loss: 0.4145 - val_acc: 0.9230\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 2s 217us/step - loss: 0.2484 - acc: 0.9707 - val_loss: 0.3959 - val_acc: 0.9270\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.2571 - acc: 0.9667 - val_loss: 0.4033 - val_acc: 0.9240\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.2523 - acc: 0.9692 - val_loss: 0.4182 - val_acc: 0.9210\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 0.2392 - acc: 0.9719 - val_loss: 0.4232 - val_acc: 0.9190\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.2474 - acc: 0.9725 - val_loss: 0.4181 - val_acc: 0.9240\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 0.2345 - acc: 0.9740 - val_loss: 0.4278 - val_acc: 0.9160\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 2s 221us/step - loss: 0.2395 - acc: 0.9717 - val_loss: 0.4040 - val_acc: 0.9250\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.2491 - acc: 0.9687 - val_loss: 0.4356 - val_acc: 0.9210\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.2492 - acc: 0.9712 - val_loss: 0.4082 - val_acc: 0.9250\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 0.2483 - acc: 0.9697 - val_loss: 0.4161 - val_acc: 0.9240\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.2394 - acc: 0.9728 - val_loss: 0.4334 - val_acc: 0.9200\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 0.2412 - acc: 0.9735 - val_loss: 0.4604 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "#drop method.3\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_with_drop_point3 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 99us/step\n",
      "1500/1500 [==============================] - 0s 84us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the final results on the training and testing sets \n",
    "results_train_drop3 = model.evaluate(train_final, label_train_final)\n",
    "results_test_drop3 = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results Drop out .3 : \n",
      " Loss:0.1872738123178482 \n",
      " Accuracy :0.9912000000317891\n",
      "Test results Drop out .30.38969088276227315 \n",
      " Accuracy:0.9293333328564962\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results Drop out .3 : \\n Loss:{} \\n Accuracy :{}'.format(results_train_drop3[0],results_train_drop3[1],) )\n",
    "print('Test results Drop out .3{} \\n Accuracy:{}'.format(results_test_drop3[0] ,results_test_drop3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 3s 348us/step - loss: 1.0321 - acc: 0.7229 - val_loss: 0.5661 - val_acc: 0.8860\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.5969 - acc: 0.8699 - val_loss: 0.4989 - val_acc: 0.8910\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5207 - acc: 0.8816 - val_loss: 0.4735 - val_acc: 0.8920\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.4888 - acc: 0.8888 - val_loss: 0.4594 - val_acc: 0.8930\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.4703 - acc: 0.8977 - val_loss: 0.4598 - val_acc: 0.9010\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.4505 - acc: 0.9035 - val_loss: 0.4541 - val_acc: 0.9010\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 0.4445 - acc: 0.9075 - val_loss: 0.4646 - val_acc: 0.9030\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.4461 - acc: 0.9119 - val_loss: 0.4513 - val_acc: 0.9080\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.4318 - acc: 0.9169 - val_loss: 0.4446 - val_acc: 0.9130\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 2s 209us/step - loss: 0.4249 - acc: 0.9197 - val_loss: 0.4585 - val_acc: 0.9080\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.4116 - acc: 0.9216 - val_loss: 0.4326 - val_acc: 0.9060\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.4200 - acc: 0.9173 - val_loss: 0.4545 - val_acc: 0.9090\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.4153 - acc: 0.9196 - val_loss: 0.4353 - val_acc: 0.9120\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 0.4050 - acc: 0.9264 - val_loss: 0.4537 - val_acc: 0.9050\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 0.4203 - acc: 0.9220 - val_loss: 0.4492 - val_acc: 0.9100\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.3998 - acc: 0.9264 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.4059 - acc: 0.9240 - val_loss: 0.4562 - val_acc: 0.9040\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 2s 212us/step - loss: 0.4048 - acc: 0.9241 - val_loss: 0.4365 - val_acc: 0.9090\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.3978 - acc: 0.9268 - val_loss: 0.4429 - val_acc: 0.9060\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.3931 - acc: 0.9295 - val_loss: 0.4402 - val_acc: 0.9080\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 0.3885 - acc: 0.9253 - val_loss: 0.4440 - val_acc: 0.9070\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 172us/step - loss: 0.3952 - acc: 0.9260 - val_loss: 0.4326 - val_acc: 0.9080\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 175us/step - loss: 0.3922 - acc: 0.9275 - val_loss: 0.4480 - val_acc: 0.9070\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 0.3894 - acc: 0.9299 - val_loss: 0.4319 - val_acc: 0.9170\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 0.3930 - acc: 0.9283 - val_loss: 0.4344 - val_acc: 0.9060\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 0.3854 - acc: 0.9292 - val_loss: 0.4474 - val_acc: 0.9070\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 0.3815 - acc: 0.9329 - val_loss: 0.4286 - val_acc: 0.9120\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 0.3806 - acc: 0.9320 - val_loss: 0.4345 - val_acc: 0.9040\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.3818 - acc: 0.9309 - val_loss: 0.4402 - val_acc: 0.9060\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 0.3783 - acc: 0.9311 - val_loss: 0.4487 - val_acc: 0.9140\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 0.3617 - acc: 0.9357 - val_loss: 0.4389 - val_acc: 0.9130\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 2s 214us/step - loss: 0.3791 - acc: 0.9331 - val_loss: 0.4471 - val_acc: 0.9120\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 2s 216us/step - loss: 0.3774 - acc: 0.9360 - val_loss: 0.4298 - val_acc: 0.9170\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.3676 - acc: 0.9359 - val_loss: 0.4395 - val_acc: 0.9200\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.3710 - acc: 0.9345 - val_loss: 0.4544 - val_acc: 0.9150\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.3763 - acc: 0.9331 - val_loss: 0.4373 - val_acc: 0.9170\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.3702 - acc: 0.9353 - val_loss: 0.4358 - val_acc: 0.9160\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.3686 - acc: 0.9355 - val_loss: 0.4319 - val_acc: 0.9210\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.3685 - acc: 0.9393 - val_loss: 0.4457 - val_acc: 0.9170\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.3691 - acc: 0.9335 - val_loss: 0.4533 - val_acc: 0.9210\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 0.3644 - acc: 0.9389 - val_loss: 0.4558 - val_acc: 0.9180\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 2s 213us/step - loss: 0.3632 - acc: 0.9376 - val_loss: 0.4399 - val_acc: 0.9160\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 2s 209us/step - loss: 0.3620 - acc: 0.9353 - val_loss: 0.4594 - val_acc: 0.9130\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.3689 - acc: 0.9377 - val_loss: 0.4372 - val_acc: 0.9190\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 2s 217us/step - loss: 0.3615 - acc: 0.9420 - val_loss: 0.4527 - val_acc: 0.9130\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 2s 214us/step - loss: 0.3600 - acc: 0.9405 - val_loss: 0.4339 - val_acc: 0.9170\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 0.3585 - acc: 0.9388 - val_loss: 0.4483 - val_acc: 0.9110\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 2s 210us/step - loss: 0.3620 - acc: 0.9383 - val_loss: 0.4559 - val_acc: 0.9060\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 2s 225us/step - loss: 0.3636 - acc: 0.9383 - val_loss: 0.4810 - val_acc: 0.9110\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 2s 245us/step - loss: 0.3578 - acc: 0.9421 - val_loss: 0.4686 - val_acc: 0.9160\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.3589 - acc: 0.9368 - val_loss: 0.4380 - val_acc: 0.9210\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.3674 - acc: 0.9361 - val_loss: 0.4485 - val_acc: 0.9180\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.3587 - acc: 0.9423 - val_loss: 0.4507 - val_acc: 0.9210\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.3479 - acc: 0.9440 - val_loss: 0.4370 - val_acc: 0.9260\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 2s 214us/step - loss: 0.3591 - acc: 0.9407 - val_loss: 0.4645 - val_acc: 0.9170\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.3630 - acc: 0.9401 - val_loss: 0.4824 - val_acc: 0.9180\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 2s 211us/step - loss: 0.3557 - acc: 0.9417 - val_loss: 0.4522 - val_acc: 0.9150\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 2s 211us/step - loss: 0.3591 - acc: 0.9392 - val_loss: 0.4461 - val_acc: 0.9210\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 2s 218us/step - loss: 0.3546 - acc: 0.9412 - val_loss: 0.4523 - val_acc: 0.9210\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.3531 - acc: 0.9428 - val_loss: 0.4467 - val_acc: 0.9240\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.3512 - acc: 0.9416 - val_loss: 0.4517 - val_acc: 0.9140\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.3537 - acc: 0.9393 - val_loss: 0.4420 - val_acc: 0.9210\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.3507 - acc: 0.9436 - val_loss: 0.4541 - val_acc: 0.9210\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.3591 - acc: 0.9419 - val_loss: 0.4713 - val_acc: 0.9140\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 0.3615 - acc: 0.9431 - val_loss: 0.4557 - val_acc: 0.9210\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 0.3621 - acc: 0.9433 - val_loss: 0.4702 - val_acc: 0.9160\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.3534 - acc: 0.9428 - val_loss: 0.4848 - val_acc: 0.9230\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.3589 - acc: 0.9388 - val_loss: 0.4696 - val_acc: 0.9160\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.3528 - acc: 0.9424 - val_loss: 0.5020 - val_acc: 0.9150\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.3490 - acc: 0.9423 - val_loss: 0.4535 - val_acc: 0.9170\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 2s 211us/step - loss: 0.3517 - acc: 0.9427 - val_loss: 0.4565 - val_acc: 0.9160\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.3409 - acc: 0.9425 - val_loss: 0.4533 - val_acc: 0.9180\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 2s 205us/step - loss: 0.3442 - acc: 0.9417 - val_loss: 0.4693 - val_acc: 0.9210\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.3435 - acc: 0.9432 - val_loss: 0.4571 - val_acc: 0.9150\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 0.3375 - acc: 0.9441 - val_loss: 0.4530 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "#drop method.3\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_with_drop_point5 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 99us/step\n",
      "1500/1500 [==============================] - 0s 99us/step\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the final results on the training and testing sets \n",
    "results_train_drop5 = model.evaluate(train_final, label_train_final)\n",
    "results_test_drop5 = model.evaluate(test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results Drop out .5: \n",
      " Loss:0.1872738123178482 \n",
      " Accuracy :0.9912000000317891\n",
      "Test results Drop out .50.38969088276227315 \n",
      " Accuracy:0.9293333328564962\n"
     ]
    }
   ],
   "source": [
    "#print results \n",
    "print('Training results Drop out .5: \\n Loss:{} \\n Accuracy :{}'.format(results_train_drop3[0],results_train_drop3[1],) )\n",
    "print('Test results Drop out .5{} \\n Accuracy:{}'.format(results_test_drop3[0] ,results_test_drop3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 2s 329us/step - loss: 2.8819 - acc: 0.7337 - val_loss: 1.0713 - val_acc: 0.8730\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 0.9356 - acc: 0.8660 - val_loss: 0.8413 - val_acc: 0.8860\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.8117 - acc: 0.8757 - val_loss: 0.7668 - val_acc: 0.8900\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.7581 - acc: 0.8783 - val_loss: 0.7359 - val_acc: 0.8880\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.7335 - acc: 0.8795 - val_loss: 0.7159 - val_acc: 0.8910\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.7167 - acc: 0.8819 - val_loss: 0.7033 - val_acc: 0.8870\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 178us/step - loss: 0.7007 - acc: 0.8836 - val_loss: 0.6839 - val_acc: 0.8910\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 180us/step - loss: 0.6797 - acc: 0.8845 - val_loss: 0.6709 - val_acc: 0.8920\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 0.6632 - acc: 0.8863 - val_loss: 0.6573 - val_acc: 0.8940\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 0.6493 - acc: 0.8880 - val_loss: 0.6519 - val_acc: 0.8920\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 0.6370 - acc: 0.8948 - val_loss: 0.6467 - val_acc: 0.9000\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 175us/step - loss: 0.6279 - acc: 0.9067 - val_loss: 0.6361 - val_acc: 0.9060\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.6189 - acc: 0.9108 - val_loss: 0.6344 - val_acc: 0.9060\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 0.6131 - acc: 0.9112 - val_loss: 0.6319 - val_acc: 0.9100\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 0.6048 - acc: 0.9149 - val_loss: 0.6197 - val_acc: 0.9090\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 180us/step - loss: 0.6026 - acc: 0.9136 - val_loss: 0.6334 - val_acc: 0.9070\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.5986 - acc: 0.9159 - val_loss: 0.6145 - val_acc: 0.9080\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 0.5898 - acc: 0.9185 - val_loss: 0.6178 - val_acc: 0.9100\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5901 - acc: 0.9183 - val_loss: 0.6083 - val_acc: 0.9120\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 0.5862 - acc: 0.9185 - val_loss: 0.6071 - val_acc: 0.9150\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.5870 - acc: 0.9208 - val_loss: 0.6147 - val_acc: 0.9140\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5846 - acc: 0.9189 - val_loss: 0.6118 - val_acc: 0.9160\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 0.5814 - acc: 0.9224 - val_loss: 0.6152 - val_acc: 0.9140\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 0.5762 - acc: 0.9213 - val_loss: 0.6108 - val_acc: 0.9150\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5738 - acc: 0.9237 - val_loss: 0.6052 - val_acc: 0.9170\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5770 - acc: 0.9239 - val_loss: 0.5978 - val_acc: 0.9140\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 0.5724 - acc: 0.9220 - val_loss: 0.6051 - val_acc: 0.9170\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.5672 - acc: 0.9247 - val_loss: 0.5963 - val_acc: 0.9220\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 0.5717 - acc: 0.9240 - val_loss: 0.5980 - val_acc: 0.9170\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.5672 - acc: 0.9236 - val_loss: 0.6050 - val_acc: 0.9140\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 180us/step - loss: 0.5668 - acc: 0.9264 - val_loss: 0.5991 - val_acc: 0.9120\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5623 - acc: 0.9252 - val_loss: 0.5900 - val_acc: 0.9170\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5602 - acc: 0.9263 - val_loss: 0.6031 - val_acc: 0.9160\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 0.5578 - acc: 0.9272 - val_loss: 0.5991 - val_acc: 0.9120\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 175us/step - loss: 0.5592 - acc: 0.9271 - val_loss: 0.5969 - val_acc: 0.9170\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.5606 - acc: 0.9259 - val_loss: 0.5878 - val_acc: 0.9220\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5586 - acc: 0.9276 - val_loss: 0.5904 - val_acc: 0.9210\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5573 - acc: 0.9264 - val_loss: 0.5922 - val_acc: 0.9230\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 0.5538 - acc: 0.9296 - val_loss: 0.5939 - val_acc: 0.9160\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.5558 - acc: 0.9296 - val_loss: 0.5853 - val_acc: 0.9230\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.5513 - acc: 0.9315 - val_loss: 0.5930 - val_acc: 0.9240\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 0.5553 - acc: 0.9248 - val_loss: 0.5961 - val_acc: 0.9150\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5556 - acc: 0.9284 - val_loss: 0.5870 - val_acc: 0.9220\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5551 - acc: 0.9287 - val_loss: 0.5912 - val_acc: 0.9160\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 0.5504 - acc: 0.9295 - val_loss: 0.5901 - val_acc: 0.9210\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5507 - acc: 0.9319 - val_loss: 0.5912 - val_acc: 0.9230\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5488 - acc: 0.9300 - val_loss: 0.5764 - val_acc: 0.9230\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.5548 - acc: 0.9311 - val_loss: 0.6052 - val_acc: 0.9150\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 0.5476 - acc: 0.9296 - val_loss: 0.5785 - val_acc: 0.9230\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 0.5544 - acc: 0.9296 - val_loss: 0.5773 - val_acc: 0.9200\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 2s 215us/step - loss: 0.5472 - acc: 0.9307 - val_loss: 0.5934 - val_acc: 0.9200\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5515 - acc: 0.9287 - val_loss: 0.5845 - val_acc: 0.9230\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.5453 - acc: 0.9327 - val_loss: 0.5985 - val_acc: 0.9190\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.5466 - acc: 0.9323 - val_loss: 0.5847 - val_acc: 0.9240\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.5443 - acc: 0.9321 - val_loss: 0.5939 - val_acc: 0.9190\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.5485 - acc: 0.9304 - val_loss: 0.5765 - val_acc: 0.9220\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.5406 - acc: 0.9324 - val_loss: 0.5927 - val_acc: 0.9280\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.5476 - acc: 0.9327 - val_loss: 0.5897 - val_acc: 0.9190\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 2s 200us/step - loss: 0.5452 - acc: 0.9316 - val_loss: 0.5852 - val_acc: 0.9240\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.5424 - acc: 0.9316 - val_loss: 0.5965 - val_acc: 0.9220\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.5463 - acc: 0.9287 - val_loss: 0.5913 - val_acc: 0.9210\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.5448 - acc: 0.9317 - val_loss: 0.5905 - val_acc: 0.9250\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.5474 - acc: 0.9317 - val_loss: 0.5979 - val_acc: 0.9190\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.5423 - acc: 0.9331 - val_loss: 0.5728 - val_acc: 0.9150\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.5429 - acc: 0.9313 - val_loss: 0.5795 - val_acc: 0.9220\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5414 - acc: 0.9312 - val_loss: 0.5812 - val_acc: 0.9270\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.5430 - acc: 0.9321 - val_loss: 0.5883 - val_acc: 0.9250\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.5422 - acc: 0.9319 - val_loss: 0.5978 - val_acc: 0.9150\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 2s 210us/step - loss: 0.5465 - acc: 0.9327 - val_loss: 0.5816 - val_acc: 0.9260\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.5387 - acc: 0.9329 - val_loss: 0.5885 - val_acc: 0.9270\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.5449 - acc: 0.9327 - val_loss: 0.5750 - val_acc: 0.9250\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.5384 - acc: 0.9312 - val_loss: 0.6065 - val_acc: 0.9170\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.5440 - acc: 0.9335 - val_loss: 0.5926 - val_acc: 0.9260\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.5398 - acc: 0.9321 - val_loss: 0.6015 - val_acc: 0.9150\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.5407 - acc: 0.9337 - val_loss: 0.5861 - val_acc: 0.9130\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l1(0.005),input_shape=(5000,)))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l1(0.005)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L1_model_3 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 81us/step\n",
      "1500/1500 [==============================] - 0s 78us/step \n",
      "Training results Drop out .5: \n",
      " Loss:0.5186026593844096 \n",
      " Accuracy :0.9460000000317892\n",
      "Test results Drop out .50.5826330545743307 \n",
      " Accuracy:0.9253333328564962\n"
     ]
    }
   ],
   "source": [
    "results_train_L1_3 = model.evaluate(train_final, label_train_final)\n",
    "results_test_L1_3 = model.evaluate(test,label_test)\n",
    "#print results \n",
    "print('Training results Drop out .5: \\n Loss:{} \\n Accuracy :{}'.format(results_train_L1_3[0],results_train_L1_3[1],) )\n",
    "print('Test results Drop out .5{} \\n Accuracy:{}'.format(results_test_L1_3[0] ,results_test_L1_3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 3s 353us/step - loss: 0.7512 - acc: 0.8432 - val_loss: 0.4886 - val_acc: 0.9050\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 2s 220us/step - loss: 0.4242 - acc: 0.9193 - val_loss: 0.4522 - val_acc: 0.9070\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 0.3793 - acc: 0.9299 - val_loss: 0.4381 - val_acc: 0.9050\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 0.3546 - acc: 0.9348 - val_loss: 0.4233 - val_acc: 0.9120\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.3396 - acc: 0.9411 - val_loss: 0.4133 - val_acc: 0.9080\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.3200 - acc: 0.9464 - val_loss: 0.4090 - val_acc: 0.9110\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 0.3070 - acc: 0.9480 - val_loss: 0.3977 - val_acc: 0.9150\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.2996 - acc: 0.9497 - val_loss: 0.3975 - val_acc: 0.9130\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2812 - acc: 0.9577 - val_loss: 0.3796 - val_acc: 0.9170\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 2s 243us/step - loss: 0.2751 - acc: 0.9581 - val_loss: 0.3851 - val_acc: 0.9170\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 2s 211us/step - loss: 0.2621 - acc: 0.9599 - val_loss: 0.4011 - val_acc: 0.9070\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2578 - acc: 0.9624 - val_loss: 0.3798 - val_acc: 0.9120\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 0.2522 - acc: 0.9637 - val_loss: 0.3926 - val_acc: 0.9130\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.2459 - acc: 0.9663 - val_loss: 0.3695 - val_acc: 0.9210\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.2335 - acc: 0.9713 - val_loss: 0.3734 - val_acc: 0.9200\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.2333 - acc: 0.9689 - val_loss: 0.3727 - val_acc: 0.9210\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.2241 - acc: 0.9736 - val_loss: 0.3691 - val_acc: 0.9190\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.2190 - acc: 0.9740 - val_loss: 0.3738 - val_acc: 0.9180\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.2144 - acc: 0.9736 - val_loss: 0.3722 - val_acc: 0.9230\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.2116 - acc: 0.9737 - val_loss: 0.3666 - val_acc: 0.9170\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.2049 - acc: 0.9751 - val_loss: 0.3792 - val_acc: 0.9160\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.2026 - acc: 0.9745 - val_loss: 0.3643 - val_acc: 0.9160\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.1948 - acc: 0.9788 - val_loss: 0.3628 - val_acc: 0.9240\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1945 - acc: 0.9792 - val_loss: 0.3681 - val_acc: 0.9200\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1827 - acc: 0.9829 - val_loss: 0.3566 - val_acc: 0.9230\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.1783 - acc: 0.9823 - val_loss: 0.3769 - val_acc: 0.9200\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.1809 - acc: 0.9829 - val_loss: 0.3646 - val_acc: 0.9200\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.1732 - acc: 0.9833 - val_loss: 0.3717 - val_acc: 0.9180\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.1686 - acc: 0.9853 - val_loss: 0.3578 - val_acc: 0.9230\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 0.1691 - acc: 0.9847 - val_loss: 0.3604 - val_acc: 0.9190\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.1607 - acc: 0.9885 - val_loss: 0.3648 - val_acc: 0.9260\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.1604 - acc: 0.9857 - val_loss: 0.3663 - val_acc: 0.9190\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 0.1646 - acc: 0.9855 - val_loss: 0.3666 - val_acc: 0.9200\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.1541 - acc: 0.9877 - val_loss: 0.3603 - val_acc: 0.9150\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.1493 - acc: 0.9903 - val_loss: 0.3596 - val_acc: 0.9210\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1443 - acc: 0.9901 - val_loss: 0.3681 - val_acc: 0.9220\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.1463 - acc: 0.9900 - val_loss: 0.3684 - val_acc: 0.9190\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.1430 - acc: 0.9909 - val_loss: 0.3928 - val_acc: 0.9190\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.1432 - acc: 0.9901 - val_loss: 0.3777 - val_acc: 0.9160\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 0.1403 - acc: 0.9907 - val_loss: 0.3626 - val_acc: 0.9180\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1372 - acc: 0.9908 - val_loss: 0.3673 - val_acc: 0.9130\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.1358 - acc: 0.9915 - val_loss: 0.3634 - val_acc: 0.9230\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 0.1321 - acc: 0.9921 - val_loss: 0.3686 - val_acc: 0.9200\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.1289 - acc: 0.9928 - val_loss: 0.3713 - val_acc: 0.9200\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1309 - acc: 0.9923 - val_loss: 0.3807 - val_acc: 0.9170\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 0.1254 - acc: 0.9933 - val_loss: 0.3696 - val_acc: 0.9200\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.1234 - acc: 0.9932 - val_loss: 0.3570 - val_acc: 0.9200\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.1253 - acc: 0.9931 - val_loss: 0.3693 - val_acc: 0.9170\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 0.1240 - acc: 0.9925 - val_loss: 0.3695 - val_acc: 0.9180\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.1268 - acc: 0.9917 - val_loss: 0.3725 - val_acc: 0.9130\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 2s 200us/step - loss: 0.1231 - acc: 0.9935 - val_loss: 0.3836 - val_acc: 0.9170\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.1181 - acc: 0.9936 - val_loss: 0.3637 - val_acc: 0.9220\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.1169 - acc: 0.9941 - val_loss: 0.3781 - val_acc: 0.9150\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.1166 - acc: 0.9944 - val_loss: 0.3688 - val_acc: 0.9160\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.1133 - acc: 0.9944 - val_loss: 0.3739 - val_acc: 0.9170\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.1169 - acc: 0.9923 - val_loss: 0.3779 - val_acc: 0.9200\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 0.1136 - acc: 0.9940 - val_loss: 0.3795 - val_acc: 0.9160\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.1158 - acc: 0.9936 - val_loss: 0.3812 - val_acc: 0.9160\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.1110 - acc: 0.9943 - val_loss: 0.3750 - val_acc: 0.9190\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.1088 - acc: 0.9953 - val_loss: 0.3791 - val_acc: 0.9200\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.1121 - acc: 0.9936 - val_loss: 0.4042 - val_acc: 0.9130\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.1068 - acc: 0.9944 - val_loss: 0.3851 - val_acc: 0.9140\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.1072 - acc: 0.9949 - val_loss: 0.3720 - val_acc: 0.9140\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 0.1074 - acc: 0.9933 - val_loss: 0.3899 - val_acc: 0.9150\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 0.1053 - acc: 0.9963 - val_loss: 0.3790 - val_acc: 0.9190\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.1037 - acc: 0.9949 - val_loss: 0.3716 - val_acc: 0.9180\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 0.1030 - acc: 0.9953 - val_loss: 0.3787 - val_acc: 0.9170\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 0.1054 - acc: 0.9935 - val_loss: 0.3794 - val_acc: 0.9210\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 190us/step - loss: 0.1012 - acc: 0.9960 - val_loss: 0.3777 - val_acc: 0.9190\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.1018 - acc: 0.9949 - val_loss: 0.4078 - val_acc: 0.9240\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 0.1066 - acc: 0.9932 - val_loss: 0.3877 - val_acc: 0.9120\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 0.1019 - acc: 0.9949 - val_loss: 0.3822 - val_acc: 0.9150\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 191us/step - loss: 0.1010 - acc: 0.9941 - val_loss: 0.3768 - val_acc: 0.9170\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 0.1006 - acc: 0.9951 - val_loss: 0.3812 - val_acc: 0.9200\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 198us/step - loss: 0.0961 - acc: 0.9955 - val_loss: 0.3700 - val_acc: 0.9100\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005),input_shape=(5000,)))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_model_3 = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 93us/step\n",
      "1500/1500 [==============================] - 0s 78us/step\n",
      "Training results Drop out .5: \n",
      " Loss:0.08896201866467794 \n",
      " Accuracy :0.998\n",
      "Test results Drop out .50.3183498136997223 \n",
      " Accuracy:0.9326666661898295\n"
     ]
    }
   ],
   "source": [
    "results_train_L2 = model.evaluate(train_final, label_train_final)\n",
    "results_test_L2= model.evaluate(test,label_test)\n",
    "#print results \n",
    "print('Training results Drop out .5: \\n Loss:{} \\n Accuracy :{}'.format(results_train_L2[0],results_train_L2[1],) )\n",
    "print('Test results Drop out .5{} \\n Accuracy:{}'.format(results_test_L2[0] ,results_test_L2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 3s 342us/step - loss: 0.7529 - acc: 0.8565 - val_loss: 0.5063 - val_acc: 0.9090\n",
      "Epoch 2/75\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 0.4342 - acc: 0.9204 - val_loss: 0.4612 - val_acc: 0.9080\n",
      "Epoch 3/75\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.3886 - acc: 0.9317 - val_loss: 0.4489 - val_acc: 0.9090\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.3574 - acc: 0.9383 - val_loss: 0.4236 - val_acc: 0.9140\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.3314 - acc: 0.9440 - val_loss: 0.4194 - val_acc: 0.9110\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 0.3178 - acc: 0.9485 - val_loss: 0.4086 - val_acc: 0.9210\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.3121 - acc: 0.9491 - val_loss: 0.4031 - val_acc: 0.9150\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 0.2928 - acc: 0.9548 - val_loss: 0.3997 - val_acc: 0.9190\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 2s 212us/step - loss: 0.2918 - acc: 0.9515 - val_loss: 0.3938 - val_acc: 0.9170\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 2s 212us/step - loss: 0.2772 - acc: 0.9601 - val_loss: 0.3955 - val_acc: 0.9140\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 0.2751 - acc: 0.9591 - val_loss: 0.4000 - val_acc: 0.9160\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 0.2601 - acc: 0.9651 - val_loss: 0.3883 - val_acc: 0.9160\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 0.2526 - acc: 0.9660 - val_loss: 0.3817 - val_acc: 0.9180\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.2479 - acc: 0.9675 - val_loss: 0.3852 - val_acc: 0.9180\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.2409 - acc: 0.9679 - val_loss: 0.3662 - val_acc: 0.9240\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.2364 - acc: 0.9717 - val_loss: 0.3773 - val_acc: 0.9260\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.2271 - acc: 0.9712 - val_loss: 0.3651 - val_acc: 0.9230\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.2220 - acc: 0.9737 - val_loss: 0.3781 - val_acc: 0.9190\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.2180 - acc: 0.9749 - val_loss: 0.3637 - val_acc: 0.9230\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.2121 - acc: 0.9747 - val_loss: 0.3820 - val_acc: 0.9200\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.2092 - acc: 0.9757 - val_loss: 0.3633 - val_acc: 0.9250\n",
      "Epoch 22/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.2033 - acc: 0.9789 - val_loss: 0.3795 - val_acc: 0.9180\n",
      "Epoch 23/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1984 - acc: 0.9787 - val_loss: 0.3794 - val_acc: 0.9220\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1988 - acc: 0.9780 - val_loss: 0.3758 - val_acc: 0.9220\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1955 - acc: 0.9792 - val_loss: 0.3679 - val_acc: 0.9240\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1851 - acc: 0.9835 - val_loss: 0.3791 - val_acc: 0.9220\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1812 - acc: 0.9848 - val_loss: 0.3756 - val_acc: 0.9210\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.1816 - acc: 0.9816 - val_loss: 0.3807 - val_acc: 0.9150\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.1757 - acc: 0.9852 - val_loss: 0.3750 - val_acc: 0.9220\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.1721 - acc: 0.9849 - val_loss: 0.3684 - val_acc: 0.9220\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.1730 - acc: 0.9843 - val_loss: 0.3791 - val_acc: 0.9220\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.1731 - acc: 0.9851 - val_loss: 0.3730 - val_acc: 0.9180\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1659 - acc: 0.9871 - val_loss: 0.3722 - val_acc: 0.9220\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.1586 - acc: 0.9880 - val_loss: 0.3807 - val_acc: 0.9220\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1557 - acc: 0.9872 - val_loss: 0.3705 - val_acc: 0.9180\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1571 - acc: 0.9877 - val_loss: 0.3691 - val_acc: 0.9210\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1560 - acc: 0.9875 - val_loss: 0.3647 - val_acc: 0.9220\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1507 - acc: 0.9889 - val_loss: 0.3775 - val_acc: 0.9110\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1470 - acc: 0.9896 - val_loss: 0.3745 - val_acc: 0.9250\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1459 - acc: 0.9907 - val_loss: 0.3871 - val_acc: 0.9170\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1455 - acc: 0.9887 - val_loss: 0.3715 - val_acc: 0.9190\n",
      "Epoch 42/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1448 - acc: 0.9900 - val_loss: 0.3775 - val_acc: 0.9150\n",
      "Epoch 43/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1409 - acc: 0.9900 - val_loss: 0.3695 - val_acc: 0.9210\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1360 - acc: 0.9917 - val_loss: 0.3753 - val_acc: 0.9210\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1344 - acc: 0.9911 - val_loss: 0.3651 - val_acc: 0.9210\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1395 - acc: 0.9896 - val_loss: 0.3711 - val_acc: 0.9180\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1327 - acc: 0.9904 - val_loss: 0.3765 - val_acc: 0.9160\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1303 - acc: 0.9917 - val_loss: 0.3754 - val_acc: 0.9250\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1261 - acc: 0.9927 - val_loss: 0.3699 - val_acc: 0.9200\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1320 - acc: 0.9901 - val_loss: 0.3881 - val_acc: 0.9100\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1333 - acc: 0.9912 - val_loss: 0.3803 - val_acc: 0.9180\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1241 - acc: 0.9919 - val_loss: 0.3753 - val_acc: 0.9180\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1205 - acc: 0.9941 - val_loss: 0.3690 - val_acc: 0.9180\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1228 - acc: 0.9917 - val_loss: 0.3767 - val_acc: 0.9230\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1198 - acc: 0.9931 - val_loss: 0.3809 - val_acc: 0.9140\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1223 - acc: 0.9919 - val_loss: 0.3746 - val_acc: 0.9200\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1221 - acc: 0.9917 - val_loss: 0.3702 - val_acc: 0.9180\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1200 - acc: 0.9935 - val_loss: 0.3924 - val_acc: 0.9190\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.1170 - acc: 0.9937 - val_loss: 0.3710 - val_acc: 0.9170\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1166 - acc: 0.9932 - val_loss: 0.3668 - val_acc: 0.9140\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1136 - acc: 0.9943 - val_loss: 0.3858 - val_acc: 0.9140\n",
      "Epoch 62/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1147 - acc: 0.9925 - val_loss: 0.3797 - val_acc: 0.9210\n",
      "Epoch 63/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1081 - acc: 0.9959 - val_loss: 0.4021 - val_acc: 0.9160\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1111 - acc: 0.9929 - val_loss: 0.3640 - val_acc: 0.9190\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1166 - acc: 0.9933 - val_loss: 0.3839 - val_acc: 0.9190\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1118 - acc: 0.9928 - val_loss: 0.3882 - val_acc: 0.9210\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1092 - acc: 0.9944 - val_loss: 0.3992 - val_acc: 0.9150\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1086 - acc: 0.9944 - val_loss: 0.3836 - val_acc: 0.9190\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1077 - acc: 0.9947 - val_loss: 0.3951 - val_acc: 0.9150\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 0.1066 - acc: 0.9944 - val_loss: 0.3832 - val_acc: 0.9160\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1084 - acc: 0.9928 - val_loss: 0.4018 - val_acc: 0.9170\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.1085 - acc: 0.9943 - val_loss: 0.3974 - val_acc: 0.9210\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.1066 - acc: 0.9936 - val_loss: 0.3819 - val_acc: 0.9150\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1068 - acc: 0.9944 - val_loss: 0.4134 - val_acc: 0.9170\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.1091 - acc: 0.9939 - val_loss: 0.3963 - val_acc: 0.9140\n"
     ]
    }
   ],
   "source": [
    "#drop method.3\n",
    "from keras.layers import Dropout\n",
    "model= Sequential()\n",
    "model.add(Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(5000,)))\n",
    "model.add(Dense(25,activation='relu',kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "L2_reg = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=75,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 84us/step\n",
      "1500/1500 [==============================] - 0s 87us/step\n",
      "Training results l2 .5: \n",
      " Loss:0.09828941461245219 \n",
      " Accuracy :0.9973333333333333\n",
      "Test results l2 .50.3259114747842153 \n",
      " Accuracy:0.9280000003178914\n"
     ]
    }
   ],
   "source": [
    "results_train_L2 = model.evaluate(train_final, label_train_final)\n",
    "results_test_L2= model.evaluate(test,label_test)\n",
    "#print results \n",
    "print('Training results l2 .5: \\n Loss:{} \\n Accuracy :{}'.format(results_train_L2[0],results_train_L2[1],) )\n",
    "print('Test results l2 .5{} \\n Accuracy:{}'.format(results_test_L2[0] ,results_test_L2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53123x19056 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1271026 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dropout\n",
    "# model= Sequential()\n",
    "# model.add(Dense(100,activation='relu',input_shape=(2000,))) \n",
    "# model.add(Dropout(.5))\n",
    "# model.add(Dense(50,activation='relu'))\n",
    "# model.add(Dropout(.5))\n",
    "# model.add(Dense(50,activation='relu'))\n",
    "# model.add(Dropout(.5))\n",
    "# model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# drop_model2 = model.fit(train_final,\n",
    "#                     label_train_final,\n",
    "#                     epochs=200,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(val, label_val))\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten,Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d46c2f1d67f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,activation='relu',input_shape=(2000,)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(32,activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D(62,activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "train_final,\n",
    "                    label_train_final\n",
    "    \n",
    "    validation_data=(val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_7_input to have 4 dimensions, but got array with shape (7500, 2000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-cca4ea362a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=(val, label_val))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_7_input to have 4 dimensions, but got array with shape (7500, 2000)"
     ]
    }
   ],
   "source": [
    "cnn_history = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>state</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>state_label</th>\n",
       "      <th>new_v2</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot gris</td>\n",
       "      <td>white_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "      <td>10</td>\n",
       "      <td>riesling</td>\n",
       "      <td>white_off_dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>US</td>\n",
       "      <td>Soft, supple plum envelopes an oaky structure ...</td>\n",
       "      <td>87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Kirkland Signature 2011 Mountain Cuve Caberne...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Kirkland Signature</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>US</td>\n",
       "      <td>Slightly reduced, this wine offers a chalky, t...</td>\n",
       "      <td>87</td>\n",
       "      <td>34.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Alexander Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Louis M. Martini 2012 Cabernet Sauvignon (Alex...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Louis M. Martini</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>Building on 150 years and six generations of w...</td>\n",
       "      <td>87</td>\n",
       "      <td>12.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Mirassou 2012 Chardonnay (Central Coast)</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Mirassou</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>US</td>\n",
       "      <td>Red fruit aromas pervade on the nose, with cig...</td>\n",
       "      <td>87</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Quivremont 2012 Meritage (Virginia)</td>\n",
       "      <td>Meritage</td>\n",
       "      <td>Quivremont</td>\n",
       "      <td>23</td>\n",
       "      <td>meritage</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>US</td>\n",
       "      <td>Ripe aromas of dark berries mingle with ample ...</td>\n",
       "      <td>87</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Quivremont 2012 Vin de Maison Red (Virginia)</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Quivremont</td>\n",
       "      <td>23</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>US</td>\n",
       "      <td>A sleek mix of tart berry, stem and herb, alon...</td>\n",
       "      <td>87</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Oregon Other</td>\n",
       "      <td>Acrobat 2013 Pinot Noir (Oregon)</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Acrobat</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>US</td>\n",
       "      <td>This wine from the Geneseo district offers aro...</td>\n",
       "      <td>87</td>\n",
       "      <td>22.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Bianchi 2011 Signature Selection Merlot (Paso ...</td>\n",
       "      <td>Merlot</td>\n",
       "      <td>Bianchi</td>\n",
       "      <td>1</td>\n",
       "      <td>merlot</td>\n",
       "      <td>red_dry_spice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>US</td>\n",
       "      <td>Oak and earth intermingle around robust aromas...</td>\n",
       "      <td>87</td>\n",
       "      <td>69.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma Coast</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Castello di Amorosa 2011 King Ridge Vineyard P...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Castello di Amorosa</td>\n",
       "      <td>1</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>US</td>\n",
       "      <td>Clarksburg is becoming a haven for Chenin Blan...</td>\n",
       "      <td>86</td>\n",
       "      <td>16.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Clarksburg</td>\n",
       "      <td>Central Valley</td>\n",
       "      <td>Clarksburg Wine Company 2010 Chenin Blanc (Cla...</td>\n",
       "      <td>Chenin Blanc</td>\n",
       "      <td>Clarksburg Wine Company</td>\n",
       "      <td>1</td>\n",
       "      <td>chenin blanc</td>\n",
       "      <td>white_off_dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>US</td>\n",
       "      <td>Rustic and dry, this has flavors of berries, c...</td>\n",
       "      <td>86</td>\n",
       "      <td>50.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Dry Creek Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Envolve 2010 Puma Springs Vineyard Red (Dry Cr...</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Envolve</td>\n",
       "      <td>1</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>US</td>\n",
       "      <td>This shows a tart, green gooseberry flavor tha...</td>\n",
       "      <td>86</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Envolve 2011 Sauvignon Blanc (Sonoma Valley)</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Envolve</td>\n",
       "      <td>1</td>\n",
       "      <td>sauvignon blanc</td>\n",
       "      <td>white_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>US</td>\n",
       "      <td>As with many of the Erath 2010 vineyard design...</td>\n",
       "      <td>86</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>McMinnville</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Erath 2010 Hyland Pinot Noir (McMinnville)</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Erath</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>US</td>\n",
       "      <td>A stiff, tannic wine, this slowly opens and br...</td>\n",
       "      <td>86</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Hawkins Cellars 2009 Pinot Noir (Willamette Va...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Hawkins Cellars</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>US</td>\n",
       "      <td>The clean, brisk mouthfeel gives this slightly...</td>\n",
       "      <td>86</td>\n",
       "      <td>14.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Robert Hall 2011 Sauvignon Blanc (Paso Robles)</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Robert Hall</td>\n",
       "      <td>1</td>\n",
       "      <td>sauvignon blanc</td>\n",
       "      <td>white_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>US</td>\n",
       "      <td>Right out of the starting blocks this is an oa...</td>\n",
       "      <td>86</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Tarara 2010 #SocialSecret Red (Virginia)</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Tarara</td>\n",
       "      <td>23</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>US</td>\n",
       "      <td>This is a sweet wine with flavors of white sug...</td>\n",
       "      <td>86</td>\n",
       "      <td>13.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Lake County</td>\n",
       "      <td>Lake County</td>\n",
       "      <td>The White Knight 2011 Riesling (Lake County)</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>The White Knight</td>\n",
       "      <td>1</td>\n",
       "      <td>riesling</td>\n",
       "      <td>white_off_dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>US</td>\n",
       "      <td>This bottling resembles the New Zealand paradi...</td>\n",
       "      <td>86</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Monticello</td>\n",
       "      <td>Monticello</td>\n",
       "      <td>Trump 2011 Sauvignon Blanc (Monticello)</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Trump</td>\n",
       "      <td>23</td>\n",
       "      <td>sauvignon blanc</td>\n",
       "      <td>white_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>US</td>\n",
       "      <td>This shows jelly-like flavors of orange and pe...</td>\n",
       "      <td>85</td>\n",
       "      <td>30.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>RustRidge 2010 Estate Bottled Chardonnay (Napa...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>RustRidge</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>US</td>\n",
       "      <td>This is weighty, creamy and medium to full in ...</td>\n",
       "      <td>85</td>\n",
       "      <td>14.0</td>\n",
       "      <td>California</td>\n",
       "      <td>North Coast</td>\n",
       "      <td>North Coast</td>\n",
       "      <td>Souverain 2010 Chardonnay (North Coast)</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Souverain</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>US</td>\n",
       "      <td>Aromas of cranberry, barrel spice and herb are...</td>\n",
       "      <td>86</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Mellisoni 2014 Malbec (Columbia Valley (WA))</td>\n",
       "      <td>Malbec</td>\n",
       "      <td>Mellisoni</td>\n",
       "      <td>24</td>\n",
       "      <td>malbec</td>\n",
       "      <td>red_dry_fruit_vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>US</td>\n",
       "      <td>Syrupy and dense, this wine is jammy in plum a...</td>\n",
       "      <td>86</td>\n",
       "      <td>100.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Okapi 2013 Estate Cabernet Sauvignon (Napa Val...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Okapi</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>US</td>\n",
       "      <td>The aromas are brooding, with notes of barrel ...</td>\n",
       "      <td>86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Ram 2014 Alder Ridge Vineyard Cabernet Franc (...</td>\n",
       "      <td>Cabernet Franc</td>\n",
       "      <td>Ram</td>\n",
       "      <td>24</td>\n",
       "      <td>cabernet franc</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>US</td>\n",
       "      <td>There are intriguing touches to the nose of th...</td>\n",
       "      <td>86</td>\n",
       "      <td>26.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Santa Ynez Valley</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Sevtap 2015 Golden Horn Sauvignon Blanc (Santa...</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Sevtap</td>\n",
       "      <td>1</td>\n",
       "      <td>sauvignon blanc</td>\n",
       "      <td>white_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>US</td>\n",
       "      <td>A blend of Merlot and Cabernet Franc, this win...</td>\n",
       "      <td>86</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Basel Cellars 2013 Inspired Red (Columbia Vall...</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>Basel Cellars</td>\n",
       "      <td>24</td>\n",
       "      <td>bordeaux</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>US</td>\n",
       "      <td>Very deep in color and spicy-smoky in flavor, ...</td>\n",
       "      <td>86</td>\n",
       "      <td>12.0</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>California Other</td>\n",
       "      <td>Cocobon 2014 Red (California)</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Cocobon</td>\n",
       "      <td>1</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>US</td>\n",
       "      <td>Aromas of vanilla, char and toast lead to ligh...</td>\n",
       "      <td>86</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Drumheller 2014 Chardonnay (Columbia Valley (WA))</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Drumheller</td>\n",
       "      <td>24</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>US</td>\n",
       "      <td>Big oak defines this robustly dense and extrac...</td>\n",
       "      <td>86</td>\n",
       "      <td>40.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Alexander Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Eco Terreno 2013 Old Vine Cabernet Sauvignon (...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Eco Terreno</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129880</th>\n",
       "      <td>129880</td>\n",
       "      <td>US</td>\n",
       "      <td>Apple blossom intrigues on the nose of this li...</td>\n",
       "      <td>90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Russian River Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Martin Ray 2015 Chardonnay (Russian River Valley)</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Martin Ray</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129882</th>\n",
       "      <td>129882</td>\n",
       "      <td>US</td>\n",
       "      <td>This wine is mostly Cabernet Sauvignon (78%), ...</td>\n",
       "      <td>90</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Matthews 2012 Reserve Red (Columbia Valley (WA))</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>Matthews</td>\n",
       "      <td>24</td>\n",
       "      <td>bordeaux</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129885</th>\n",
       "      <td>129885</td>\n",
       "      <td>US</td>\n",
       "      <td>Lemon-lime soda aromas meet with orange blosso...</td>\n",
       "      <td>91</td>\n",
       "      <td>25.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Santa Ynez Valley</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Zaca Mesa 2013 Roussanne (Santa Ynez Valley)</td>\n",
       "      <td>Roussanne</td>\n",
       "      <td>Zaca Mesa</td>\n",
       "      <td>1</td>\n",
       "      <td>roussanne</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129888</th>\n",
       "      <td>129888</td>\n",
       "      <td>US</td>\n",
       "      <td>Round red cherry, deep plum and ripe cranberry...</td>\n",
       "      <td>91</td>\n",
       "      <td>34.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Santa Lucia Highlands</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>August West 2015 Pinot Noir (Santa Lucia Highl...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>August West</td>\n",
       "      <td>1</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129891</th>\n",
       "      <td>129891</td>\n",
       "      <td>US</td>\n",
       "      <td>Chocolate, mocha and coconut notes highlight t...</td>\n",
       "      <td>91</td>\n",
       "      <td>100.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Calistoga</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Jax 2013 Estate Block 3 Cabernet Sauvignon (Ca...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Jax</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129895</th>\n",
       "      <td>129895</td>\n",
       "      <td>US</td>\n",
       "      <td>A blend of vineyards including the winery's Mi...</td>\n",
       "      <td>91</td>\n",
       "      <td>30.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Green Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Martin Ray 2015 Limited Release Chardonnay (Gr...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Martin Ray</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129896</th>\n",
       "      <td>129896</td>\n",
       "      <td>US</td>\n",
       "      <td>Smoothly seductive in texture and body, this m...</td>\n",
       "      <td>91</td>\n",
       "      <td>36.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Alexander Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Mazzocco 2014 Stuhlmuller Vineyard Reserve Cha...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Mazzocco</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129897</th>\n",
       "      <td>129897</td>\n",
       "      <td>US</td>\n",
       "      <td>This hearty, rich bottling needs time to breat...</td>\n",
       "      <td>91</td>\n",
       "      <td>48.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>MCV 2014 Petite Sirah (Paso Robles)</td>\n",
       "      <td>Petite Sirah</td>\n",
       "      <td>MCV</td>\n",
       "      <td>1</td>\n",
       "      <td>petite sirah</td>\n",
       "      <td>red_dry_fruit_vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129898</th>\n",
       "      <td>129898</td>\n",
       "      <td>US</td>\n",
       "      <td>Plum and cherry fruit show on the nose of this...</td>\n",
       "      <td>91</td>\n",
       "      <td>52.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Midnight 2012 Mare Nectaris Reserve Red (Paso ...</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>1</td>\n",
       "      <td>bordeaux</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129899</th>\n",
       "      <td>129899</td>\n",
       "      <td>US</td>\n",
       "      <td>Aged entirely in stainless steel, this combine...</td>\n",
       "      <td>91</td>\n",
       "      <td>30.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Paraduxx 2015 Proprietary White (Napa Valley)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Paraduxx</td>\n",
       "      <td>1</td>\n",
       "      <td>white blend</td>\n",
       "      <td>white_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129904</th>\n",
       "      <td>129904</td>\n",
       "      <td>US</td>\n",
       "      <td>A blend of Sauvignon Blanc, Sauvignon Gris and...</td>\n",
       "      <td>91</td>\n",
       "      <td>33.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Chalk Hill</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Chalk Hill 2015 Estate Bottled Sauvignon Blanc...</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Chalk Hill</td>\n",
       "      <td>1</td>\n",
       "      <td>sauvignon blanc</td>\n",
       "      <td>white_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129906</th>\n",
       "      <td>129906</td>\n",
       "      <td>US</td>\n",
       "      <td>Pencil shaving and graphite notes lend a class...</td>\n",
       "      <td>91</td>\n",
       "      <td>35.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Conn Creek 2013 Cabernet Sauvignon (Napa Valley)</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Conn Creek</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129909</th>\n",
       "      <td>129909</td>\n",
       "      <td>US</td>\n",
       "      <td>Hailing from an extreme coastal vineyard, this...</td>\n",
       "      <td>91</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>San Luis Obispo County</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Derby 2015 Derbyshire Vineyard Pinot Gris (San...</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Derby</td>\n",
       "      <td>1</td>\n",
       "      <td>pinot gris</td>\n",
       "      <td>white_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129912</th>\n",
       "      <td>129912</td>\n",
       "      <td>US</td>\n",
       "      <td>From the Ranch House block of the famous viney...</td>\n",
       "      <td>91</td>\n",
       "      <td>55.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma Coast</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Dunstan 2014 Durell Vineyard Pinot Noir (Sonom...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Dunstan</td>\n",
       "      <td>1</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129913</th>\n",
       "      <td>129913</td>\n",
       "      <td>US</td>\n",
       "      <td>This is taut and dense, and requires time and ...</td>\n",
       "      <td>92</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Woodward Canyon 2005 Artist Series #14 Caberne...</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Woodward Canyon</td>\n",
       "      <td>24</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129914</th>\n",
       "      <td>129914</td>\n",
       "      <td>US</td>\n",
       "      <td>Fritz has made tremendous progress with Cab ov...</td>\n",
       "      <td>91</td>\n",
       "      <td>35.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Dry Creek Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Fritz 2005 Cabernet Sauvignon (Dry Creek Valley)</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Fritz</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129915</th>\n",
       "      <td>129915</td>\n",
       "      <td>US</td>\n",
       "      <td>The third vintage of Mith is the best yet, com...</td>\n",
       "      <td>91</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Balboa 2005 Mith Red Wine Red (Columbia Valley...</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Balboa</td>\n",
       "      <td>24</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129919</th>\n",
       "      <td>129919</td>\n",
       "      <td>US</td>\n",
       "      <td>This ripe, rich, almost decadently thick wine ...</td>\n",
       "      <td>91</td>\n",
       "      <td>105.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Walla Walla Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Nicholas Cole Cellars 2004 Reserve Red (Walla ...</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>Nicholas Cole Cellars</td>\n",
       "      <td>24</td>\n",
       "      <td>red blend</td>\n",
       "      <td>red_blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129920</th>\n",
       "      <td>129920</td>\n",
       "      <td>US</td>\n",
       "      <td>Shows the clean, citrus acid juiciness and sil...</td>\n",
       "      <td>91</td>\n",
       "      <td>48.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sta. Rita Hills</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Pali 2006 Fiddlestix Vineyard Pinot Noir (Sta....</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Pali</td>\n",
       "      <td>1</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129926</th>\n",
       "      <td>129926</td>\n",
       "      <td>US</td>\n",
       "      <td>This pure Syrah from Reininger's estate vineya...</td>\n",
       "      <td>91</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Walla Walla Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Reininger 2005 Ash Hollow Vineyard Syrah (Wall...</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Reininger</td>\n",
       "      <td>24</td>\n",
       "      <td>syrah</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129927</th>\n",
       "      <td>129927</td>\n",
       "      <td>US</td>\n",
       "      <td>It's 100% Cabernet Franc, a delightful, bright...</td>\n",
       "      <td>91</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Tamarack Cellars 2006 Cabernet Franc (Columbia...</td>\n",
       "      <td>Cabernet Franc</td>\n",
       "      <td>Tamarack Cellars</td>\n",
       "      <td>24</td>\n",
       "      <td>cabernet franc</td>\n",
       "      <td>red_bone_dry_savory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129935</th>\n",
       "      <td>129935</td>\n",
       "      <td>US</td>\n",
       "      <td>This has lovely raspberry and cranberry fruit,...</td>\n",
       "      <td>91</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley-Walla Walla Valley</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Va Piano 2006 Syrah (Columbia Valley-Walla Wal...</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Va Piano</td>\n",
       "      <td>24</td>\n",
       "      <td>syrah</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129940</th>\n",
       "      <td>129940</td>\n",
       "      <td>US</td>\n",
       "      <td>This is the winery's bells and whistles Chardo...</td>\n",
       "      <td>91</td>\n",
       "      <td>36.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>Mendocino</td>\n",
       "      <td>Standish 2006 Watson Vineyard Chardonnay (Mend...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Standish</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129941</th>\n",
       "      <td>129941</td>\n",
       "      <td>US</td>\n",
       "      <td>A Chardonnay with an unusual companion, 8% Sm...</td>\n",
       "      <td>90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Mendocino County</td>\n",
       "      <td>Mendocino County</td>\n",
       "      <td>Apriori 2013 Chardonnay (Mendocino County)</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Apriori</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129942</th>\n",
       "      <td>129942</td>\n",
       "      <td>US</td>\n",
       "      <td>This is classic in herbaceous aromas and flavo...</td>\n",
       "      <td>90</td>\n",
       "      <td>35.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Sonoma County</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Arrowood 2010 Cabernet Sauvignon (Sonoma County)</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Arrowood</td>\n",
       "      <td>1</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "      <td>red_dry_herb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129945</th>\n",
       "      <td>129945</td>\n",
       "      <td>US</td>\n",
       "      <td>Hailing from one of the more popular vineyards...</td>\n",
       "      <td>90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Santa Ynez Valley</td>\n",
       "      <td>Central Coast</td>\n",
       "      <td>Birichino 2013 Jurassic Park Vineyard Old Vine...</td>\n",
       "      <td>Chenin Blanc</td>\n",
       "      <td>Birichino</td>\n",
       "      <td>1</td>\n",
       "      <td>chenin blanc</td>\n",
       "      <td>white_off_dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129949</th>\n",
       "      <td>129949</td>\n",
       "      <td>US</td>\n",
       "      <td>There's no bones about the use of oak in this ...</td>\n",
       "      <td>90</td>\n",
       "      <td>35.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Flora Springs 2013 Barrel Fermented Chardonnay...</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Flora Springs</td>\n",
       "      <td>1</td>\n",
       "      <td>chardonnay</td>\n",
       "      <td>white_dry_sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129950</th>\n",
       "      <td>129950</td>\n",
       "      <td>US</td>\n",
       "      <td>This opens with herbaceous dollops of thyme an...</td>\n",
       "      <td>90</td>\n",
       "      <td>35.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Hendry 2012 Blocks 7 &amp; 22 Zinfandel (Napa Valley)</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>Hendry</td>\n",
       "      <td>1</td>\n",
       "      <td>zinfandel</td>\n",
       "      <td>red_dry_fruit_vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129952</th>\n",
       "      <td>129952</td>\n",
       "      <td>US</td>\n",
       "      <td>This Zinfandel from the eastern section of Nap...</td>\n",
       "      <td>90</td>\n",
       "      <td>22.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Chiles Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Houdini 2011 Zinfandel (Chiles Valley)</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>Houdini</td>\n",
       "      <td>1</td>\n",
       "      <td>zinfandel</td>\n",
       "      <td>red_dry_fruit_vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129967</th>\n",
       "      <td>129967</td>\n",
       "      <td>US</td>\n",
       "      <td>Citation is given as much as a decade of bottl...</td>\n",
       "      <td>90</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Oregon Other</td>\n",
       "      <td>Citation 2004 Pinot Noir (Oregon)</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Citation</td>\n",
       "      <td>18</td>\n",
       "      <td>pinot noir</td>\n",
       "      <td>red_dry_floral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53989 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 country                                        description  \\\n",
       "2                2      US  Tart and snappy, the flavors of lime flesh and...   \n",
       "3                3      US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4                4      US  Much like the regular bottling from 2012, this...   \n",
       "10              10      US  Soft, supple plum envelopes an oaky structure ...   \n",
       "12              12      US  Slightly reduced, this wine offers a chalky, t...   \n",
       "14              14      US  Building on 150 years and six generations of w...   \n",
       "19              19      US  Red fruit aromas pervade on the nose, with cig...   \n",
       "20              20      US  Ripe aromas of dark berries mingle with ample ...   \n",
       "21              21      US  A sleek mix of tart berry, stem and herb, alon...   \n",
       "23              23      US  This wine from the Geneseo district offers aro...   \n",
       "25              25      US  Oak and earth intermingle around robust aromas...   \n",
       "29              29      US  Clarksburg is becoming a haven for Chenin Blan...   \n",
       "33              33      US  Rustic and dry, this has flavors of berries, c...   \n",
       "34              34      US  This shows a tart, green gooseberry flavor tha...   \n",
       "35              35      US  As with many of the Erath 2010 vineyard design...   \n",
       "41              41      US  A stiff, tannic wine, this slowly opens and br...   \n",
       "43              43      US  The clean, brisk mouthfeel gives this slightly...   \n",
       "45              45      US  Right out of the starting blocks this is an oa...   \n",
       "47              47      US  This is a sweet wine with flavors of white sug...   \n",
       "48              48      US  This bottling resembles the New Zealand paradi...   \n",
       "55              55      US  This shows jelly-like flavors of orange and pe...   \n",
       "56              56      US  This is weighty, creamy and medium to full in ...   \n",
       "59              59      US  Aromas of cranberry, barrel spice and herb are...   \n",
       "60              60      US  Syrupy and dense, this wine is jammy in plum a...   \n",
       "62              62      US  The aromas are brooding, with notes of barrel ...   \n",
       "64              64      US  There are intriguing touches to the nose of th...   \n",
       "67              67      US  A blend of Merlot and Cabernet Franc, this win...   \n",
       "68              68      US  Very deep in color and spicy-smoky in flavor, ...   \n",
       "70              70      US  Aromas of vanilla, char and toast lead to ligh...   \n",
       "71              71      US  Big oak defines this robustly dense and extrac...   \n",
       "...            ...     ...                                                ...   \n",
       "129880      129880      US  Apple blossom intrigues on the nose of this li...   \n",
       "129882      129882      US  This wine is mostly Cabernet Sauvignon (78%), ...   \n",
       "129885      129885      US  Lemon-lime soda aromas meet with orange blosso...   \n",
       "129888      129888      US  Round red cherry, deep plum and ripe cranberry...   \n",
       "129891      129891      US  Chocolate, mocha and coconut notes highlight t...   \n",
       "129895      129895      US  A blend of vineyards including the winery's Mi...   \n",
       "129896      129896      US  Smoothly seductive in texture and body, this m...   \n",
       "129897      129897      US  This hearty, rich bottling needs time to breat...   \n",
       "129898      129898      US  Plum and cherry fruit show on the nose of this...   \n",
       "129899      129899      US  Aged entirely in stainless steel, this combine...   \n",
       "129904      129904      US  A blend of Sauvignon Blanc, Sauvignon Gris and...   \n",
       "129906      129906      US  Pencil shaving and graphite notes lend a class...   \n",
       "129909      129909      US  Hailing from an extreme coastal vineyard, this...   \n",
       "129912      129912      US  From the Ranch House block of the famous viney...   \n",
       "129913      129913      US  This is taut and dense, and requires time and ...   \n",
       "129914      129914      US  Fritz has made tremendous progress with Cab ov...   \n",
       "129915      129915      US  The third vintage of Mith is the best yet, com...   \n",
       "129919      129919      US  This ripe, rich, almost decadently thick wine ...   \n",
       "129920      129920      US  Shows the clean, citrus acid juiciness and sil...   \n",
       "129926      129926      US  This pure Syrah from Reininger's estate vineya...   \n",
       "129927      129927      US  It's 100% Cabernet Franc, a delightful, bright...   \n",
       "129935      129935      US  This has lovely raspberry and cranberry fruit,...   \n",
       "129940      129940      US  This is the winery's bells and whistles Chardo...   \n",
       "129941      129941      US  A Chardonnay with an unusual companion, 8% Sm...   \n",
       "129942      129942      US  This is classic in herbaceous aromas and flavo...   \n",
       "129945      129945      US  Hailing from one of the more popular vineyards...   \n",
       "129949      129949      US  There's no bones about the use of oak in this ...   \n",
       "129950      129950      US  This opens with herbaceous dollops of thyme an...   \n",
       "129952      129952      US  This Zinfandel from the eastern section of Nap...   \n",
       "129967      129967      US  Citation is given as much as a decade of bottl...   \n",
       "\n",
       "        points  price       state                            region_1  \\\n",
       "2           87   14.0      Oregon                   Willamette Valley   \n",
       "3           87   13.0    Michigan                 Lake Michigan Shore   \n",
       "4           87   65.0      Oregon                   Willamette Valley   \n",
       "10          87   19.0  California                         Napa Valley   \n",
       "12          87   34.0  California                    Alexander Valley   \n",
       "14          87   12.0  California                       Central Coast   \n",
       "19          87   32.0    Virginia                            Virginia   \n",
       "20          87   23.0    Virginia                            Virginia   \n",
       "21          87   20.0      Oregon                              Oregon   \n",
       "23          87   22.0  California                         Paso Robles   \n",
       "25          87   69.0  California                        Sonoma Coast   \n",
       "29          86   16.0  California                          Clarksburg   \n",
       "33          86   50.0  California                    Dry Creek Valley   \n",
       "34          86   20.0  California                       Sonoma Valley   \n",
       "35          86   50.0      Oregon                         McMinnville   \n",
       "41          86   22.0      Oregon                   Willamette Valley   \n",
       "43          86   14.0  California                         Paso Robles   \n",
       "45          86   40.0    Virginia                            Virginia   \n",
       "47          86   13.0  California                         Lake County   \n",
       "48          86   16.0    Virginia                          Monticello   \n",
       "55          85   30.0  California                         Napa Valley   \n",
       "56          85   14.0  California                         North Coast   \n",
       "59          86   55.0  Washington                Columbia Valley (WA)   \n",
       "60          86  100.0  California                         Napa Valley   \n",
       "62          86   25.0  Washington                Columbia Valley (WA)   \n",
       "64          86   26.0  California                   Santa Ynez Valley   \n",
       "67          86   46.0  Washington                Columbia Valley (WA)   \n",
       "68          86   12.0  California                          California   \n",
       "70          86   12.0  Washington                Columbia Valley (WA)   \n",
       "71          86   40.0  California                    Alexander Valley   \n",
       "...        ...    ...         ...                                 ...   \n",
       "129880      90   20.0  California                Russian River Valley   \n",
       "129882      90   60.0  Washington                Columbia Valley (WA)   \n",
       "129885      91   25.0  California                   Santa Ynez Valley   \n",
       "129888      91   34.0  California               Santa Lucia Highlands   \n",
       "129891      91  100.0  California                           Calistoga   \n",
       "129895      91   30.0  California                        Green Valley   \n",
       "129896      91   36.0  California                    Alexander Valley   \n",
       "129897      91   48.0  California                         Paso Robles   \n",
       "129898      91   52.0  California                         Paso Robles   \n",
       "129899      91   30.0  California                         Napa Valley   \n",
       "129904      91   33.0  California                          Chalk Hill   \n",
       "129906      91   35.0  California                         Napa Valley   \n",
       "129909      91   20.0  California              San Luis Obispo County   \n",
       "129912      91   55.0  California                        Sonoma Coast   \n",
       "129913      92   44.0  Washington                Columbia Valley (WA)   \n",
       "129914      91   35.0  California                    Dry Creek Valley   \n",
       "129915      91   40.0  Washington                Columbia Valley (WA)   \n",
       "129919      91  105.0  Washington             Walla Walla Valley (WA)   \n",
       "129920      91   48.0  California                     Sta. Rita Hills   \n",
       "129926      91   41.0  Washington             Walla Walla Valley (WA)   \n",
       "129927      91   28.0  Washington                Columbia Valley (WA)   \n",
       "129935      91   38.0  Washington  Columbia Valley-Walla Walla Valley   \n",
       "129940      91   36.0  California                           Mendocino   \n",
       "129941      90   20.0  California                    Mendocino County   \n",
       "129942      90   35.0  California                       Sonoma County   \n",
       "129945      90   20.0  California                   Santa Ynez Valley   \n",
       "129949      90   35.0  California                         Napa Valley   \n",
       "129950      90   35.0  California                         Napa Valley   \n",
       "129952      90   22.0  California                       Chiles Valley   \n",
       "129967      90   75.0      Oregon                              Oregon   \n",
       "\n",
       "                   region_2  \\\n",
       "2         Willamette Valley   \n",
       "3       Lake Michigan Shore   \n",
       "4         Willamette Valley   \n",
       "10                     Napa   \n",
       "12                   Sonoma   \n",
       "14            Central Coast   \n",
       "19                 Virginia   \n",
       "20                 Virginia   \n",
       "21             Oregon Other   \n",
       "23            Central Coast   \n",
       "25                   Sonoma   \n",
       "29           Central Valley   \n",
       "33                   Sonoma   \n",
       "34                   Sonoma   \n",
       "35        Willamette Valley   \n",
       "41        Willamette Valley   \n",
       "43            Central Coast   \n",
       "45                 Virginia   \n",
       "47              Lake County   \n",
       "48               Monticello   \n",
       "55                     Napa   \n",
       "56              North Coast   \n",
       "59          Columbia Valley   \n",
       "60                     Napa   \n",
       "62          Columbia Valley   \n",
       "64            Central Coast   \n",
       "67          Columbia Valley   \n",
       "68         California Other   \n",
       "70          Columbia Valley   \n",
       "71                   Sonoma   \n",
       "...                     ...   \n",
       "129880               Sonoma   \n",
       "129882      Columbia Valley   \n",
       "129885        Central Coast   \n",
       "129888        Central Coast   \n",
       "129891                 Napa   \n",
       "129895               Sonoma   \n",
       "129896               Sonoma   \n",
       "129897        Central Coast   \n",
       "129898        Central Coast   \n",
       "129899                 Napa   \n",
       "129904               Sonoma   \n",
       "129906                 Napa   \n",
       "129909        Central Coast   \n",
       "129912               Sonoma   \n",
       "129913      Columbia Valley   \n",
       "129914               Sonoma   \n",
       "129915      Columbia Valley   \n",
       "129919      Columbia Valley   \n",
       "129920        Central Coast   \n",
       "129926      Columbia Valley   \n",
       "129927      Columbia Valley   \n",
       "129935      Columbia Valley   \n",
       "129940            Mendocino   \n",
       "129941     Mendocino County   \n",
       "129942               Sonoma   \n",
       "129945        Central Coast   \n",
       "129949                 Napa   \n",
       "129950                 Napa   \n",
       "129952                 Napa   \n",
       "129967         Oregon Other   \n",
       "\n",
       "                                                    title  \\\n",
       "2           Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3       St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4       Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "10      Kirkland Signature 2011 Mountain Cuve Caberne...   \n",
       "12      Louis M. Martini 2012 Cabernet Sauvignon (Alex...   \n",
       "14               Mirassou 2012 Chardonnay (Central Coast)   \n",
       "19                   Quivremont 2012 Meritage (Virginia)   \n",
       "20          Quivremont 2012 Vin de Maison Red (Virginia)   \n",
       "21                       Acrobat 2013 Pinot Noir (Oregon)   \n",
       "23      Bianchi 2011 Signature Selection Merlot (Paso ...   \n",
       "25      Castello di Amorosa 2011 King Ridge Vineyard P...   \n",
       "29      Clarksburg Wine Company 2010 Chenin Blanc (Cla...   \n",
       "33      Envolve 2010 Puma Springs Vineyard Red (Dry Cr...   \n",
       "34           Envolve 2011 Sauvignon Blanc (Sonoma Valley)   \n",
       "35             Erath 2010 Hyland Pinot Noir (McMinnville)   \n",
       "41      Hawkins Cellars 2009 Pinot Noir (Willamette Va...   \n",
       "43         Robert Hall 2011 Sauvignon Blanc (Paso Robles)   \n",
       "45               Tarara 2010 #SocialSecret Red (Virginia)   \n",
       "47           The White Knight 2011 Riesling (Lake County)   \n",
       "48                Trump 2011 Sauvignon Blanc (Monticello)   \n",
       "55      RustRidge 2010 Estate Bottled Chardonnay (Napa...   \n",
       "56                Souverain 2010 Chardonnay (North Coast)   \n",
       "59           Mellisoni 2014 Malbec (Columbia Valley (WA))   \n",
       "60      Okapi 2013 Estate Cabernet Sauvignon (Napa Val...   \n",
       "62      Ram 2014 Alder Ridge Vineyard Cabernet Franc (...   \n",
       "64      Sevtap 2015 Golden Horn Sauvignon Blanc (Santa...   \n",
       "67      Basel Cellars 2013 Inspired Red (Columbia Vall...   \n",
       "68                          Cocobon 2014 Red (California)   \n",
       "70      Drumheller 2014 Chardonnay (Columbia Valley (WA))   \n",
       "71      Eco Terreno 2013 Old Vine Cabernet Sauvignon (...   \n",
       "...                                                   ...   \n",
       "129880  Martin Ray 2015 Chardonnay (Russian River Valley)   \n",
       "129882   Matthews 2012 Reserve Red (Columbia Valley (WA))   \n",
       "129885       Zaca Mesa 2013 Roussanne (Santa Ynez Valley)   \n",
       "129888  August West 2015 Pinot Noir (Santa Lucia Highl...   \n",
       "129891  Jax 2013 Estate Block 3 Cabernet Sauvignon (Ca...   \n",
       "129895  Martin Ray 2015 Limited Release Chardonnay (Gr...   \n",
       "129896  Mazzocco 2014 Stuhlmuller Vineyard Reserve Cha...   \n",
       "129897                MCV 2014 Petite Sirah (Paso Robles)   \n",
       "129898  Midnight 2012 Mare Nectaris Reserve Red (Paso ...   \n",
       "129899      Paraduxx 2015 Proprietary White (Napa Valley)   \n",
       "129904  Chalk Hill 2015 Estate Bottled Sauvignon Blanc...   \n",
       "129906   Conn Creek 2013 Cabernet Sauvignon (Napa Valley)   \n",
       "129909  Derby 2015 Derbyshire Vineyard Pinot Gris (San...   \n",
       "129912  Dunstan 2014 Durell Vineyard Pinot Noir (Sonom...   \n",
       "129913  Woodward Canyon 2005 Artist Series #14 Caberne...   \n",
       "129914   Fritz 2005 Cabernet Sauvignon (Dry Creek Valley)   \n",
       "129915  Balboa 2005 Mith Red Wine Red (Columbia Valley...   \n",
       "129919  Nicholas Cole Cellars 2004 Reserve Red (Walla ...   \n",
       "129920  Pali 2006 Fiddlestix Vineyard Pinot Noir (Sta....   \n",
       "129926  Reininger 2005 Ash Hollow Vineyard Syrah (Wall...   \n",
       "129927  Tamarack Cellars 2006 Cabernet Franc (Columbia...   \n",
       "129935  Va Piano 2006 Syrah (Columbia Valley-Walla Wal...   \n",
       "129940  Standish 2006 Watson Vineyard Chardonnay (Mend...   \n",
       "129941         Apriori 2013 Chardonnay (Mendocino County)   \n",
       "129942   Arrowood 2010 Cabernet Sauvignon (Sonoma County)   \n",
       "129945  Birichino 2013 Jurassic Park Vineyard Old Vine...   \n",
       "129949  Flora Springs 2013 Barrel Fermented Chardonnay...   \n",
       "129950  Hendry 2012 Blocks 7 & 22 Zinfandel (Napa Valley)   \n",
       "129952             Houdini 2011 Zinfandel (Chiles Valley)   \n",
       "129967                  Citation 2004 Pinot Noir (Oregon)   \n",
       "\n",
       "                         variety                   winery  state_label  \\\n",
       "2                     Pinot Gris                Rainstorm           18   \n",
       "3                       Riesling               St. Julian           10   \n",
       "4                     Pinot Noir             Sweet Cheeks           18   \n",
       "10            Cabernet Sauvignon       Kirkland Signature            1   \n",
       "12            Cabernet Sauvignon         Louis M. Martini            1   \n",
       "14                    Chardonnay                 Mirassou            1   \n",
       "19                      Meritage              Quivremont           23   \n",
       "20                     Red Blend              Quivremont           23   \n",
       "21                    Pinot Noir                  Acrobat           18   \n",
       "23                        Merlot                  Bianchi            1   \n",
       "25                    Pinot Noir      Castello di Amorosa            1   \n",
       "29                  Chenin Blanc  Clarksburg Wine Company            1   \n",
       "33                     Red Blend                  Envolve            1   \n",
       "34               Sauvignon Blanc                  Envolve            1   \n",
       "35                    Pinot Noir                    Erath           18   \n",
       "41                    Pinot Noir          Hawkins Cellars           18   \n",
       "43               Sauvignon Blanc              Robert Hall            1   \n",
       "45                     Red Blend                   Tarara           23   \n",
       "47                      Riesling         The White Knight            1   \n",
       "48               Sauvignon Blanc                    Trump           23   \n",
       "55                    Chardonnay                RustRidge            1   \n",
       "56                    Chardonnay                Souverain            1   \n",
       "59                        Malbec                Mellisoni           24   \n",
       "60            Cabernet Sauvignon                    Okapi            1   \n",
       "62                Cabernet Franc                      Ram           24   \n",
       "64               Sauvignon Blanc                   Sevtap            1   \n",
       "67      Bordeaux-style Red Blend            Basel Cellars           24   \n",
       "68                     Red Blend                  Cocobon            1   \n",
       "70                    Chardonnay               Drumheller           24   \n",
       "71            Cabernet Sauvignon              Eco Terreno            1   \n",
       "...                          ...                      ...          ...   \n",
       "129880                Chardonnay               Martin Ray            1   \n",
       "129882  Bordeaux-style Red Blend                 Matthews           24   \n",
       "129885                 Roussanne                Zaca Mesa            1   \n",
       "129888                Pinot Noir              August West            1   \n",
       "129891        Cabernet Sauvignon                      Jax            1   \n",
       "129895                Chardonnay               Martin Ray            1   \n",
       "129896                Chardonnay                 Mazzocco            1   \n",
       "129897              Petite Sirah                      MCV            1   \n",
       "129898  Bordeaux-style Red Blend                 Midnight            1   \n",
       "129899               White Blend                 Paraduxx            1   \n",
       "129904           Sauvignon Blanc               Chalk Hill            1   \n",
       "129906        Cabernet Sauvignon               Conn Creek            1   \n",
       "129909                Pinot Gris                    Derby            1   \n",
       "129912                Pinot Noir                  Dunstan            1   \n",
       "129913        Cabernet Sauvignon          Woodward Canyon           24   \n",
       "129914        Cabernet Sauvignon                    Fritz            1   \n",
       "129915                 Red Blend                   Balboa           24   \n",
       "129919                 Red Blend    Nicholas Cole Cellars           24   \n",
       "129920                Pinot Noir                     Pali            1   \n",
       "129926                     Syrah                Reininger           24   \n",
       "129927            Cabernet Franc         Tamarack Cellars           24   \n",
       "129935                     Syrah                 Va Piano           24   \n",
       "129940                Chardonnay                 Standish            1   \n",
       "129941                Chardonnay                  Apriori            1   \n",
       "129942        Cabernet Sauvignon                 Arrowood            1   \n",
       "129945              Chenin Blanc                Birichino            1   \n",
       "129949                Chardonnay            Flora Springs            1   \n",
       "129950                 Zinfandel                   Hendry            1   \n",
       "129952                 Zinfandel                  Houdini            1   \n",
       "129967                Pinot Noir                 Citation           18   \n",
       "\n",
       "                    new_v2               category  \n",
       "2               pinot gris       white_dry_floral  \n",
       "3                 riesling          white_off_dry  \n",
       "4               pinot noir         red_dry_floral  \n",
       "10      cabernet sauvignon           red_dry_herb  \n",
       "12      cabernet sauvignon           red_dry_herb  \n",
       "14              chardonnay        white_dry_sweet  \n",
       "19                meritage    red_bone_dry_savory  \n",
       "20               red blend              red_blend  \n",
       "21              pinot noir         red_dry_floral  \n",
       "23                  merlot          red_dry_spice  \n",
       "25              pinot noir         red_dry_floral  \n",
       "29            chenin blanc          white_off_dry  \n",
       "33               red blend              red_blend  \n",
       "34         sauvignon blanc         white_dry_herb  \n",
       "35              pinot noir         red_dry_floral  \n",
       "41              pinot noir         red_dry_floral  \n",
       "43         sauvignon blanc         white_dry_herb  \n",
       "45               red blend              red_blend  \n",
       "47                riesling          white_off_dry  \n",
       "48         sauvignon blanc         white_dry_herb  \n",
       "55              chardonnay        white_dry_sweet  \n",
       "56              chardonnay        white_dry_sweet  \n",
       "59                  malbec  red_dry_fruit_vanilla  \n",
       "60      cabernet sauvignon           red_dry_herb  \n",
       "62          cabernet franc    red_bone_dry_savory  \n",
       "64         sauvignon blanc         white_dry_herb  \n",
       "67                bordeaux    red_bone_dry_savory  \n",
       "68               red blend              red_blend  \n",
       "70              chardonnay        white_dry_sweet  \n",
       "71      cabernet sauvignon           red_dry_herb  \n",
       "...                    ...                    ...  \n",
       "129880          chardonnay        white_dry_sweet  \n",
       "129882            bordeaux    red_bone_dry_savory  \n",
       "129885           roussanne        white_dry_sweet  \n",
       "129888          pinot noir         red_dry_floral  \n",
       "129891  cabernet sauvignon           red_dry_herb  \n",
       "129895          chardonnay        white_dry_sweet  \n",
       "129896          chardonnay        white_dry_sweet  \n",
       "129897        petite sirah  red_dry_fruit_vanilla  \n",
       "129898            bordeaux    red_bone_dry_savory  \n",
       "129899         white blend            white_blend  \n",
       "129904     sauvignon blanc         white_dry_herb  \n",
       "129906  cabernet sauvignon           red_dry_herb  \n",
       "129909          pinot gris       white_dry_floral  \n",
       "129912          pinot noir         red_dry_floral  \n",
       "129913  cabernet sauvignon           red_dry_herb  \n",
       "129914  cabernet sauvignon           red_dry_herb  \n",
       "129915           red blend              red_blend  \n",
       "129919           red blend              red_blend  \n",
       "129920          pinot noir         red_dry_floral  \n",
       "129926               syrah         red_dry_floral  \n",
       "129927      cabernet franc    red_bone_dry_savory  \n",
       "129935               syrah         red_dry_floral  \n",
       "129940          chardonnay        white_dry_sweet  \n",
       "129941          chardonnay        white_dry_sweet  \n",
       "129942  cabernet sauvignon           red_dry_herb  \n",
       "129945        chenin blanc          white_off_dry  \n",
       "129949          chardonnay        white_dry_sweet  \n",
       "129950           zinfandel  red_dry_fruit_vanilla  \n",
       "129952           zinfandel  red_dry_fruit_vanilla  \n",
       "129967          pinot noir         red_dry_floral  \n",
       "\n",
       "[53989 rows x 14 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
